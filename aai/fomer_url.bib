@inproceedings{pub12923,
    abstract = {Collecting large-scale medical datasets with fully annotated samples for training of deep networks is prohibitively expensive, especially for 3D volume data. Recent breakthroughs in self-supervised learning (SSL) offer the ability to overcome the lack of labeled training samples by learning feature representations from unlabeled data. However, most current SSL techniques in the medical field have been designed for either 2D images or 3D volumes. In practice, this restricts the capability to fully leverage unlabeled data from numerous sources, which may include both 2D and 3D data. Additionally, the use of these pre-trained networks is constrained to downstream tasks with compatible data dimensions. In this paper, we propose a novel framework for unsupervised joint learning on 2D and 3D data modalities. Given a set of 2D images or 2D slices extracted from 3D volumes, we construct an SSL task based on a 2D contrastive clustering problem for distinct classes. The 3D volumes are exploited by computing vectored embedding at each slice and then assembling a holistic feature through deformable self-attention mechanisms in Transformer, allowing incorporating long-range dependencies between slices inside 3D volumes. These holistic features are further utilized to define a novel 3D clustering agreement-based SSL task and masking embedding prediction inspired by pre-trained language models. Experiments on downstream tasks, such as 3D brain segmentation, lung nodule detection, 3D heart structures segmentation, and abnormal chest X-ray detection, demonstrate the effectiveness of our joint 2D and 3D SSL approach. We improve plain 2D Deep-ClusterV2 and SwAV by a significant margin and also surpass various modern 2D and 3D SSL approaches.},
    month = {2},
    year = {2023},
    title = {Joint Self-Supervised Image-Volume Representation Learning with Intra-Inter Contrastive Clustering},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI-2023). AAAI Conference on Artificial Intelligence (AAAI), February 7-14, Washington,, DC, USA},
    publisher = {AAAI Press},
    author = {Ho Minh Duy Nguyen and Hoang Nguyen and Mai T. N. Truong and Tri Cao and Binh T. Nguyen and Nhat Ho and Paul Swoboda and Shadi Albarqouni and Pengtao Xie and Daniel Sonntag},
    url = {https://arxiv.org/pdf/2212.01893.pdf https://www.dfki.de/fileadmin/user_upload/import/12923_Joint_Self-Supervised_Image-Volume_Representation_Learning.pdf}
}

@article{pub12997,
    abstract = {The fast advancement of technology and developers’ utilization of data centers have dramatically increased energy usage in today’s society. Thermal control is a key issue in hyper-scale cloud data centers. Hotspots form when the temperature of the host rises, increasing cooling costs and affecting dependability. Precise estimation of host temperatures is critical for optimal resource management. Thermal changes in the data center make estimating temperature a difficult challenge. Existing temperature estimating algorithms are ineffective due to their processing complexity as well as lack of accuracy. Regarding that data-driven approaches seem promising for temperature prediction, this research offers a unique efficient temperature prediction model. The model uses a combination of convolutional neural networks (CNN) and stacking multi-layer bi-directional long-term short memory (BiLSTM) for thermal prediction. The findings of the experiments reveal that the model successfully anticipates the temperature with the highest value of 97.15% and the lowest error rate of RMSE value of 0.2892, and an RMAE of 0.5003, which decreases the projection error as opposed to the other method.},
    month = {12},
    year = {2023},
    title = {Thermal prediction for energy management of clouds using a hybrid model based on CNN and stacking multi-layer bi-directional LSTM},
    journal = {Energy Reports},
    volume = {9},
    pages = {2253-2268},
    publisher = {Elsevier},
    doi = {https://doi.org/10.1016/j.egyr.2023.01.032},
    author = {Hamed Tabrizchi and Jafar Razmara and Amirhosein Mosavi},
    keywords = {Deep learning, Energy management, Cloud computing, Thermal predictio},
    url = {https://www.researchgate.net/publication/367260355_Thermal_prediction_for_energy_management_of_clouds_using_a_hybrid_model_based_on_CNN_and_stacking_multi-layer_bi-directional_LSTM https://www.dfki.de/fileadmin/user_upload/import/12997_Thermal_prediction_for_energy_management_of_clouds_using_a_hybrid.pdf}
}

@article{pub13025,
    abstract = {A hybrid machine learning method is proposed for wildfire susceptibility mapping. For modeling a geographical information system (GIS) database including 11 influencing factors and 262 fire locations from 2013 to 2018 is used for developing an integrated multivariate adaptive regression splines (MARS). The cat swarm optimization (CSO) algorithm tunes the parameters of the MARS in order to generate accurate susceptibility maps. From the Pearson correlation results, it is observed that land use, temperature, and slope angle have strong correlation with the fire severity. The results demonstrate that the prediction capability of the MARS-CSO model outperforms model tree, reduced error pruning tree and MARS. The resulting wildfire risk map using MARS-CSO reveals that 20% of the study areas is categorized in the very low wildfire risk class, whereas 40% is under the very high class of fire hazard.},
    number = {1, 2167005},
    year = {2023},
    title = {An integrated GIS-based multivariate adaptive regression splines-cat swarm optimization for improving the accuracy of wildfire susceptibility mapping},
    journal = {Geocarto International},
    volume = {38},
    pages = {1-25},
    publisher = {Taylor & Francis},
    doi = {https://doi.org/10.1080/10106049.2023.2167005},
    author = {Tao Hai and Biju Theruvil Sayed and Ali Majdi and Jincheng Zhou and Rafid Sagban and Shahab S. Band and Amirhosein Mosavi},
    keywords = {Wildfire susceptibility, geospatial, machine learning, cat swarm optimization, artificial intelligence, natural hazard},
    url = {https://www.tandfonline.com/doi/full/10.1080/10106049.2023.2167005 https://www.dfki.de/fileadmin/user_upload/import/13025_An_integrated_GIS-based_multivariate_adaptive_regressionsplines-cat_swarm_optimization_for_improving_theaccuracy_of_wildfire_susceptibility_mapping.pdf}
}

@article{pub13154,
    abstract = {Prediction of the longitudinal dispersion coefficient (LDC) is essential for the river and water resources engineering and environmental management. This study proposes ensemble models for predicting LDC based on multilayer perceptron (MULP) methods and optimization algorithms. The honey badger optimization algorithm (HBOA), salp swarm algorithm (SASA), firefly algorithm (FIFA), and particle swarm optimization algorithm (PASOA) are used to adjust the MULP parameters. Then, the outputs of the MULP-HBOA, MULP-SASA, MULP-PASOA, MULP-FIFA, and MULP models were incorporated into an inclusive multiple model (IMM). For IMM at the testing level, the mean absolute error (MEAE) was 15, whereas it was 17, 18, 23, 24, and 25 for the MULP-HBOA, MULP-SASA, MULP-FIFA, MULP-PASOA, and MULP models. The study also modified the structure of MULP models using a goodness factor which decreased the CPU time. Removing redundant neurons reduces CPU time. Thus, the modified ANN model and the suggested IMM model can decrease the computational time and further improve the performance of models},
    month = {4},
    year = {2023},
    title = {Predicting longitudinal dispersion coefficient using ensemble models and optimized multi-layer perceptron models},
    journal = {Ain Shams Engineering Journal (ASEJ)},
    volume = {10},
    pages = {2253-2277},
    publisher = {Elsevier},
    doi = {https://doi.org/10.1016/j.asej.2023.102223},
    author = {Mahsa Gholami and Elham Ghanbari-Adivi and Mohammad Ehteram and Vijay P. Singh and Ali Najah Ahmed and Amirhosein Mosavi and Ahmed El-Shafie},
    keywords = {Longitudinal dispersion coefficient, Multilayer perceptron, Optimization, Artificial intelligence, Machine learning, Deep learning, Big data},
    url = {https://www.sciencedirect.com/science/article/pii/S2090447923001120 https://www.dfki.de/fileadmin/user_upload/import/13154_Predicting_longitudinal_dispersion_coefficient_using_ensemble_models.pdf}
}

@article{pub13155,
    abstract = {The present study models the effect of climate change on the distribution of Persian oak (Quercus brantii Lindl.) in the Zagros forests, located in the west of Iran. The modeling is conducted under the current and future climatic conditions by fitting the machine learning method of the Bayesian additive regression tree (BART). For the anticipation of the potential habitats for the Persian oak, two general circulation models (GCMs) of CCSM4 and HADGEM2-ES under the representative concentration pathways (RCPs) of 2.6 and 8.5 for 2050 and 2070 are used. The mean temperature (MT) of the wettest quarter (bio8), solar radiation, slope and precipitation of the wettest month (bio13) are respectively reported as the most important variables in the modeling. The results indicate that the suitable habitat of Persian oak will significantly decrease in the future under both climate change scenarios as much as 75.06% by 2070. The proposed study brings insight into the current condition and further projects the future conditions of the local forests for proper management and protection of endangered ecosystems.},
    number = {3},
    year = {2023},
    title = {Modeling Climate Change Effects on the Distribution of Oak Forests with Machine Learning},
    journal = {Forests},
    volume = {14},
    pages = {13220-13233},
    publisher = {MDPI},
    doi = {https://doi.org/10.3390/f14030469},
    author = {Hengameh Mirhashemi and Mehdi Heydari and Omid Karami and Kourosh Ahmadi and Amirhosein Mosavi},
    keywords = {species distribution; climate change; Bayesian; machine learning; artificial intelligence; deep learning; mathematics; forest; big data; data science},
    url = {https://www.mdpi.com/1999-4907/14/3/469 https://www.dfki.de/fileadmin/user_upload/import/13155_Modeling_Climate_Change_Effects_on_the_Distribution_of_Oak_Forests_with_Machine_Learning.pdf}
}

@article{pub13195,
    abstract = {Abstract Background New methods are constantly being developed to adapt cognitive load measurement to different contexts. However, research on middle childhood students' cognitive load measurement is rare. Research indicates that the three cognitive load dimensions (intrinsic, extraneous, and germane) can be measured well in adults and teenagers using differentiated subjective rating instruments. Moreover, digital ink recorded by smartpens could serve as an indicator for cognitive load in adults. Aims With the present research, we aimed at investigating the relation between subjective cognitive load ratings, velocity and pressure measures recorded with a smartpen, and performance in standardized sketching tasks in middle childhood students. Sample Thirty-six children (age 7–12) participated at the university's laboratory. Methods The children performed two standardized sketching tasks, each in two versions. The induced intrinsic cognitive load or the extraneous cognitive load was varied between the versions. Digital ink was recorded while the children drew with a smartpen on real paper and after each task, they were asked to report their perceived intrinsic and extraneous cognitive load using a newly developed 5-item scale. Results Results indicated that cognitive load ratings as well as velocity and pressure measures were substantially related to the induced cognitive load and to performance in both sketching tasks. However, cognitive load ratings and smartpen measures were not substantially related. Conclusions Both subjective rating and digital ink hold potential for cognitive load and performance measurement. However, it is questionable whether they measure the exact same constructs.},
    year = {2023},
    title = {Digital ink and differentiated subjective ratings for cognitive load measurement in middle childhood},
    note = {e12595 BJEP.22.0172},
    journal = {British Journal of Educational Psychology},
    volume = {n/a},
    pages = {18},
    publisher = {John Wiley & Sons, Ltd},
    doi = {https://doi.org/10.1111/bjep.12595},
    author = {Kristin Altmeyer and Michael Barz and Luisa Lauer and Markus Peschel and Daniel Sonntag and Roland Brünken and Sarah Malone},
    keywords = {assessment, cognitive load measurement, extraneous load, intrinsic load, primary school, smartpen},
    url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bjep.12595 https://www.dfki.de/fileadmin/user_upload/import/13195_Brit_J_of_Edu_Psychol_-_2023_-_Altmeyer_-_Digital_ink_and_differentiated_subjective_ratings_for_cognitive_load_measurement.pdf}
}

@inproceedings{pub13196,
    series = {IUI '23 Companion},
    abstract = {Mobile eye tracking is an important tool in psychology and human-centred interaction design for understanding how people process visual scenes and user interfaces. However, analysing recordings from mobile eye trackers, which typically include an egocentric video of the scene and a gaze signal, is a time-consuming and largely manual process. To address this challenge, we propose a web-based annotation tool that leverages few-shot image classification and interactive machine learning (IML) to accelerate the annotation process. The tool allows users to efficiently map fixations to areas of interest (AOI) in a video-editing-style interface. It includes an IML component that generates suggestions and learns from user feedback using a few-shot image classification model initialised with a small number of images per AOI. Our goal is to improve the efficiency and accuracy of fixation-to-AOI mapping in mobile eye tracking.},
    year = {2023},
    title = {Interactive Fixation-to-AOI Mapping for Mobile Eye Tracking Data Based on Few-Shot Image Classification},
    booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces. International Conference on Intelligent User Interfaces (IUI-2023), Sydney,, NSW, Australia},
    pages = {175-178},
    isbn = {9798400701078},
    publisher = {Association for Computing Machinery},
    doi = {https://doi.org/10.1145/3581754.3584179},
    author = {Michael Barz and Omair Shahzad Bhatti and Hasan Md Tusfiqur Alam and Ho Minh Duy Nguyen and Daniel Sonntag},
    keywords = {visual attention, interactive machine learning, eye tracking, area of interest, fixation to AOI mapping, mobile eye tracking, eye tracking data analysis},
    url = {https://doi.org/10.1145/3581754.3584179 https://www.dfki.de/fileadmin/user_upload/import/13196_3581754.3584179.pdf}
}

@inproceedings{pub13200,
    series = {IUI'23 Companion},
    abstract = {Explainable Artificial Intelligence (XAI) is an emerging subdiscipline of Machine Learning (ML) and human-computer interaction. Discriminative models need to be understood. An explanation of such ML models is vital when an AI system makes decisions that have significant consequences, such as in healthcare or finance. By providing an input-specific explanation, users can gain confidence in an AI system’s decisions and be more willing to trust and rely on it. One problem is that interpreting example-based explanations for discriminative models, such as saliency maps, can be difficult because it is not always clear how the highlighted features contribute to the model’s overall prediction or decisions. Moreover, saliency maps, which are state-of-the-art visual explanation methods, do not provide concrete information on the influence of particular features. We propose an interactive visualisation tool called EMILE-UI that allows users to evaluate the provided explanations of an image-based classification task, specifically those provided by saliency maps. This tool allows users to evaluate the accuracy of a saliency map by reflecting the true attention or focus of the corresponding model. It visualises the relationship between the ML model and its explanation of input images, making it easier to interpret saliency maps and understand how the ML model actually predicts. Our tool supports a wide range of deep learning image classification models and image data as inputs.},
    month = {3},
    year = {2023},
    title = {A User Interface for Explaining Machine Learning Model Explanations},
    booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces. International Conference on Intelligent User Interfaces (IUI-2023), March 27-31, Sydney,, NSW, Australia},
    volume = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
    pages = {59-63},
    isbn = {9798400701078},
    publisher = {Association for Computing Machinery, New York, NY, United States},
    doi = {https://doi.org/10.1145/3581754.3584131},
    author = {Md Abdul Kadir and Abdulrahman Mohamed Selim and Michael Barz and Daniel Sonntag},
    keywords = {Transparency, Explainability, ML, AI, Trustworthiness, Interpretability},
    url = {https://doi.org/10.1145/3581754.3584131 https://www.dfki.de/fileadmin/user_upload/import/13200_iui23companion.pdf}
}

@inproceedings{pub13201,
    series = {IUI '23 Companion},
    abstract = {Mobile eye tracking studies involve analyzing areas of interest (AOIs) and visual attention to these AOIs to understand how people process visual information. However, accurately annotating the data collected for user studies can be a challenging and time-consuming task. Current approaches for automatically or semi-automatically analyzing head-mounted eye tracking data in mobile eye tracking studies have limitations, including a lack of annotation flexibility or the inability to adapt to specific target domains. To address this problem, we present IMETA, an architecture for semi-automatic fixation-to-AOI mapping. When an annotator assigns an AOI label to a sequence of frames based on the respective fixation points, an interactive video object segmentation method is used to estimate the mask proposal of the AOI. Then, we use the 3D reconstruction of the visual scene created from the eye tracking video to map these AOI masks to 3D. The resulting 3D segmentation of the AOI can be used to suggest labels for the rest of the video, with the suggestions becoming increasingly accurate as more samples are provided by an annotator using interactive machine learning (IML). IMETA has the potential to reduce the annotation workload and speed up the evaluation of mobile eye tracking studies.},
    year = {2023},
    title = {IMETA: An Interactive Mobile Eye Tracking Annotation Method for Semi-Automatic Fixation-to-AOI Mapping},
    booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces. International Conference on Intelligent User Interfaces (IUI-2023), March 27-31, Sydney,, NSW, Australia},
    pages = {33-36},
    isbn = {9798400701078},
    publisher = {Association for Computing Machinery},
    doi = {https://doi.org/10.1145/3581754.3584125},
    author = {László Kopácsi and Michael Barz and Omair Shahzad Bhatti and Daniel Sonntag},
    keywords = {3D reconstruction, fixation to aoi mapping, areas of interest, video object segmentation, annotation, interactive machine learning, mobile eye tracking},
    url = {https://doi.org/10.1145/3581754.3584125 https://www.dfki.de/fileadmin/user_upload/import/13201_3581754.3584125.pdf}
}

@inproceedings{pub13356,
    abstract = {Biodiversity loss is taking place at accelerated rates globally, and a business-as-usual trajectory will lead to missing internationally established conservation goals. Biosphere reserves are sites designed to be of global significance in terms of both the biodiversity within them and their potential for sustainable development, and are therefore ideal places for the development of local solutions to global challenges. While the protection of biodiversity is a primary goal of biosphere reserves, adequate information on the state and trends of biodiversity remains a critical gap for adaptive management in biosphere reserves. Passive acoustic monitoring (PAM) is an increasingly popular method for continued, reproducible, scalable, and cost-effective monitoring of animal wildlife. PAM adoption is on the rise, but its data management and analysis requirements pose a barrier for adoption for most agencies tasked with monitoring biodiversity. As an interdisciplinary team of machine learning scientists and ecologists experienced with PAM and working at biosphere reserves in marine and terrestrial ecosystems on three different continents, we report on the co-development of interactive machine learning tools for semi-automated assessment of animal wildlife.},
    year = {2023},
    title = {Interactive Machine Learning Solutions for Acoustic Monitoring of Animal Wildlife in Biosphere Reserves},
    booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. International Joint Conference on Artificial Intelligence (IJCAI-2023), located at IJCAI, August 19-25, Macao, Macao},
    publisher = {International Joint Conferences on Artificial Intelligence},
    author = {Thiago Gouvea and Hannes Kath and Ilira Troshani and Bengt Lüers and Patrícia P. Serafini and Ivan B. Campos and André S. Afonso and Sérgio M. F. M. Leandro and Lourens Swanepoel and Nicholas Theron and Anthony M. Swemmer and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/13356_IJCAI_ProjectProposal_PAM_in_Biosphere_Reserves.pdf https://www.ijcai.org/proceedings/2023/711}
}

@inproceedings{pub13357,
    abstract = {With artificial intelligence (AI) systems entering our working and leisure environments with increasing adaptation and learning capabilities, new opportunities arise for developing hybrid (human-AI) intelligence (HI) systems, comprising new ways of collaboration. However, there is not yet a structured way of specifying design solutions of collaboration for hybrid intelligence (HI) systems and there is a lack of best practices shared across application domains. We address this gap by investigating the generalization of specific design solutions into design patterns that can be shared and applied in different contexts. We present a human-centered bottom-up approach for the specification of design solutions and their abstraction into team design patterns. We apply the proposed approach for 4 concrete HI use cases and show the successful extraction of team design patterns that are generalizable, providing re-usable design components across various domains. This work advances previous research on team design patterns and designing applications of HI systems.},
    year = {2023},
    title = {Developing Team Design Patterns for Hybrid Intelligence Systems},
    booktitle = {Frontiers in Artificial Intelligence and Applications. International Conference on Hybrid Human-Artificial Intelligence (HHAI-2023), June 26-30, Munich, Germany},
    publisher = {IOS Press},
    doi = {https://doi.org/10.3233/FAIA230071},
    author = {Emma van Zoelen and Tina Mioch and Mani Tajaddini and Christian Fleiner and Stefani Tsaneva and Pietro Camin and Thiago Gouvea and Kim Baraka and Maaike H.T. de Boer and Mark A. Neerincx},
    keywords = {Hybrid Intelligence, Team Design Patterns, Use-case based research, Human-centered AI, Co-evolution, Interdependence},
    url = {https://ebooks.iospress.nl/doi/10.3233/FAIA230071 https://www.dfki.de/fileadmin/user_upload/import/13357_Van_Zoelen_et_al_2023_Developing_Team_Design_Patterns_for_Hybrid_Intelligence_Systems.pdf}
}

@inproceedings{pub13395,
    abstract = {Deep learning methods are well suited for data analysis in several domains, but application is often limited by technical entry barriers and the availability of large annotated datasets. We present an interactive machine learning tool for annotating passive acoustic monitoring datasets created for wildlife monitoring, which are time-consuming and costly to annotate manually. The tool, designed as a web application, consists of an interactive user interface implementing a human-in-the-loop workflow. Class label annotations provided manually as bounding boxes drawn over a spectrogram are consumed by a deep generative model (DGM) that learns a low-dimensional representation of the input data, as well as the available class labels. The learned low-dimensional representation is displayed as an interactive interface element, where new bounding boxes can be efficiently generated by the user with lasso-selection; alternatively, the DGM can propose new, automatically generated bounding boxes on demand. The user can accept, edit, or reject annotations suggested by the model, thus owning final judgement. Generated annotations can be used to fine-tune the underlying model, thus closing the loop. Investigations of the prediction accuracy and first empirical experiments show promising results on an artificial data set, laying the ground for application to a real life scenario.},
    year = {2023},
    title = {A Human-in-the-Loop Tool for Annotating Passive Acoustic Monitoring Datasets},
    booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. International Joint Conference on Artificial Intelligence (IJCAI-2023), located at IJCAI, August 19-25, Macao, China},
    publisher = {International Joint Conferences on Artificial Intelligence},
    author = {Hannes Kath and Thiago Gouvea and Daniel Sonntag},
    url = {https://www.ijcai.org/proceedings/2023/835 https://www.dfki.de/fileadmin/user_upload/import/13395_Kath_et_al_2023_A_Human-in-the-Loop_Tool_for_Annotating_Passive_Acoustic_Monitoring_Datasets.pdf}
}

@inproceedings{pub13402,
    abstract = {nformation extraction from clinical text has the potential to facilitate clinical research and personalized clinical care, but annotating large amounts of data for each set of target tasks is prohibitive. We present a German medical Named Entity Recognition (NER) system capable of cross-domain knowledge transferring. The system builds on a pre-trained German language model and a token-level binary classifier, employing semantic types sourced from the Unified Medical Language System (UMLS) as entity labels to identify corresponding entity spans within the input text. To enhance the system’s performance and robustness, we pre-train it using a medical literature corpus that incorporates UMLS semantic term annotations. We evaluate the system’s effectiveness on two German annotated datasets obtained from different clinics in zero- and few-shot settings. The results show that our approach outperforms task-specific Condition Random Fields (CRF) classifiers in terms of accuracy. Our work contributes to developing robust and transparent German medical NER models that can support the extraction of information from various clinical texts.},
    year = {2023},
    title = {Cross-domain German Medical Named Entity Recognition using a Pre-Trained Language Model and Unified Medical Semantic Types},
    booktitle = {Association for Computational Linguistics. Clinical Natural Language Processing Workshop (ClinicalNLP-2023), July 9-14, Toronto,, ON, Canada},
    publisher = {ACL},
    author = {Siting Liang and Mareike Hartmann and Daniel Sonntag},
    url = {https://aclanthology.org/2023.clinicalnlp-1.31/ https://www.dfki.de/fileadmin/user_upload/import/13402_2023.clinicalnlp-1.31.pdf}
}

@inproceedings{pub13420,
    abstract = {Interactive machine learning (IML) is a beneficial learning paradigm in cases of limited data availability, as human feedback is incrementally integrated into the training process. In this paper, we present an IML pipeline for image captioning which allows us to incrementally adapt a pre-trained image captioning model to a new data distribution based on user input.  In order to incorporate user input into the model, we explore the use of a combination of simple data augmentation methods to obtain larger data batches for each newly annotated data instance and implement continual learning methods to prevent catastrophic forgetting from repeated updates. For our experiments, we split a domain-specific image captioning dataset, namely VizWiz, into non-overlapping parts to simulate an incremental input flow for continually adapting the model to new data. We find that, while data augmentation worsens results, even when relatively small amounts of data are available, episodic memory is an effective strategy to retain knowledge from previously seen clusters.},
    month = {7},
    year = {2023},
    title = {Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory},
    booktitle = {Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP). ACL Workshop on Simple and Efficient Natural Language Processing (SustaiNLP-2023), located at Annual Meeting of the Association for Computational Linguistics 2023, July 13, Toronto, Canada},
    publisher = {Association for Computational Linguistics},
    author = {Aliki Anagnostopoulou and Mareike Hartmann and Daniel Sonntag},
    url = {https://aclanthology.org/2023.sustainlp-1.19/ https://www.dfki.de/fileadmin/user_upload/import/13420__SustaiNLP__Interactive_Image_Captioning.pdf}
}

@inproceedings{pub13988,
    abstract = {AI models that can recognize and understand the semantics of graphical user interfaces (GUIs) enable a variety of use cases ranging from accessibility to automation. Recent efforts in this domain have pursued the development of a set of foundation models: generic GUI understanding models that can be used off-the-shelf to solve a variety of GUI-related tasks, including ones that they were not trained on. In order to develop such foundation models, meaningful downstream tasks and baselines for GUI-related use cases will be required. In this paper, we present interactive link prediction as a downstream task for GUI understanding models and provide baselines as well as testing tools to effectively and efficiently evaluate predictive GUI understanding models. In interactive link prediction, the task is to predict whether tapping on an element on one screen of a mobile application (source element) navigates the user to a second screen (target screen). If this task is solved sufficiently, it can demonstrate an understanding of the relationship between elements and components across screens and enable various applications in GUI design automation and assistance. To encourage and support research on interactive link prediction, this paper contributes (1) a pre-processed large-scale dataset of links in mobile applications (18,830 links from 5,362 applications) derived from the popular RICO dataset, (2) performance baselines from five heuristic-based and two learning-based GUI understanding models, (3) a small-scale dataset of links in mobile GUI prototypes including ratings from an online study with 36 end-users for out-of-sample testing, and (4) a Figma plugin that can leverage link prediction models to automate and assist mobile GUI prototyping.},
    month = {9},
    year = {2023},
    title = {Interactive Link Prediction as a Downstream Task for Foundational GUI Understanding Models},
    booktitle = {KI 2023: Advances in Artificial Intelligence. German Conference on Artificial Intelligence (KI-2023), Berlin, Germany},
    editor = {Dietmar Seipel and Alexander Steen},
    pages = {75-89},
    isbn = {978-3-031-42608-7},
    publisher = {Springer Nature Switzerland, Cham},
    doi = {https://doi.org/10.1007/978-3-031-42608-7_7},
    author = {Christoph Albert Johns and Michael Barz and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/13988_KI__23___Link_Prediction_as_a_Downstream_Task_in_GUI_Understanding_(2).pdf https://link.springer.com/chapter/10.1007/978-3-031-42608-7_7}
}

@inproceedings{pub14164,
    series = {ICMI '23 Companion},
    abstract = {Contract cheating, i.e., when a student employs another person to participate in an exam, appears to become a growing problem in academia. Cases of paid test takers are repeatedly reported in the media, but the number of unreported cases is unclear. Proctoring systems as a countermeasure are typically not appreciated by students and teachers because they may violate the students' privacy and can be imprecise and nontransparent. In this work, we propose to use automatic handwriting analysis based on digital ballpoint pens to identify individuals during exams unobtrusively. We implement a system that enables continuous authentication of the user during exams. We use a deep neural network architecture to model a user's handwriting style. An evaluation based on the large Deepwriting dataset shows that our system can successfully differentiate between the handwriting styles of different authors and hence detect simulated cases of contract cheating. In addition, we conducted a small validation study using digital ballpoint pens to assess the system's reliability in a more realistic environment.},
    year = {2023},
    title = {Detection of Contract Cheating in Pen-and-Paper Exams through the Analysis of Handwriting Style},
    booktitle = {Companion Publication of the 25th International Conference on Multimodal Interaction. ACM International Conference on Multimodal Interaction (ICMI-2023), October 9-13, Paris, France},
    pages = {26-30},
    isbn = {9798400703218},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {https://doi.org/10.1145/3610661.3617162},
    author = {Konstantin Kuznetsov and Michael Barz and Daniel Sonntag},
    keywords = {Digital Pens, Contract Cheating, Writer Identification, Proctoring Solutions},
    url = {https://doi.org/10.1145/3610661.3617162 https://www.dfki.de/fileadmin/user_upload/import/14164_ICMI23_contract_cheating.pdf}
}

@inproceedings{pub14309,
    abstract = {Obtaining large pre-trained models that can be fine-tuned to new tasks with limited annotated samples has remained an open challenge for medical imaging data. While pre-trained deep networks on ImageNet and vision-language foundation models trained on web-scale data are prevailing approaches, their effectiveness on medical tasks is limited due to the significant domain shift between natural and medical im- ages. To bridge this gap, we introduce LVM-Med, the first family of deep networks trained on large-scale medical datasets. We have collected approximately 1.3 million medical images from 55 publicly available datasets, covering a large number of organs and modalities such as CT, MRI, X-ray, and Ultrasound. We benchmark several state-of-the-art self-supervised algorithms on this dataset and propose a novel self-supervised contrastive learning algorithm using a graph-matching formulation. The proposed approach makes three contributions: (i) it integrates prior pair-wise image similarity metrics based on local and global information; (ii) it captures the structural constraints of feature embeddings through a loss function constructed via a combinatorial graph-matching objective; and (iii) it can be trained efficiently end-to-end using modern gradient-estimation techniques for black-box solvers. We thoroughly evaluate the proposed LVM-Med on 15 downstream medical tasks ranging from segmentation and classification to object detection, and both for the in and out-of-distribution settings. LVM-Med empirically outperforms a number of state-of-the-art supervised, self-supervised, and foundation models. For challenging tasks such as Brain Tumor Classification or Diabetic Retinopathy Grading, LVM-Med improves previous vision-language models trained on 1 billion masks by 6-7% while using only a ResNet-50.},
    month = {12},
    year = {2023},
    title = {LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching},
    booktitle = {The Thirty-Seventh Annual Conference on Neural Information Processing Systems (NeurIPS 2023). Neural Information Processing Systems (NeurIPS), December 10-16, USA},
    publisher = {Advances in Neural Information Processing Systems},
    author = {Ho Minh Duy Nguyen and Hoang Nguyen and Nghiem T. Diep and Tan Pham and Tri Cao and Binh T. Nguyen and Paul Swoboda and Nhat Ho and Shadi Albarqouni and Pengtao Xie and Daniel Sonntag and Mathias Niepert},
    url = {https://www.dfki.de/fileadmin/user_upload/import/14309_LVM-Med_Camera_Version_2.pdf https://arxiv.org/abs/2306.11925}
}

@inproceedings{pub14499,
    abstract = {Constructing a robust model that can effectively generalize to test samples under distribution shifts remains a significant challenge in the field of medical imaging. The foundational models for vision and language, pre-trained on extensive
sets of natural image and text data, have emerged as a promising approach. It showcases impressive learning abilities across different tasks with the need for only a limited amount of annotated samples. While numerous techniques have
focused on developing better fine-tuning strategies to adapt these models for specific domains, we instead examine their robustness to domain shifts in the medical image segmentation task. To this end, we compare the generalization performance to unseen domains of various pre-trained models after being fine-tuned on the same in-distribution dataset and show that foundation-based models enjoy better robustness than other architectures. From here, we further developed a new Bayesian uncertainty estimation for frozen models and used them as an indicator to characterize the model’s performance on out-of-distribution (OOD) data, proving particularly beneficial for real-world applications. Our experiments not only reveal the limitations of current indicators like accuracy on the line or agreement on the line commonly used in natural image applications but also emphasize the promise of the introduced Bayesian uncertainty. Specifically, lower uncertainty predictions usually tend to higher out-of-distribution (OOD) performance.},
    month = {12},
    year = {2023},
    title = {On the Out of Distribution Robustness of Foundation Models in Medical Image Segmentation},
    booktitle = {The Thirty-Seventh Annual Conference on Neural Information Processing Systems (NeurIPS 2023). Neural Information Processing Systems (NeurIPS), Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models, December 10-16},
    publisher = {Advances in Neural Information Processing Systems},
    author = {Ho Minh Duy Nguyen and Tan Pham and Nghiem Tuong Diep and Nghi Phan and Quang Pham and Vinh Tong and Binh T. Nguyen and Ngan Hoang Le and Nhat Ho and Pengtao Xie and Daniel Sonntag and Mathias Niepert},
    keywords = {Foundation Models, Uncertainty Estimation, Robustness},
    url = {https://www.dfki.de/fileadmin/user_upload/import/14499_52_on_the_out_of_distribution_rob.pdf https://arxiv.org/abs/2311.11096}
}

@inproceedings{pub12121,
    abstract = {Real-time visualization of electrical circuit scematics in accordance to the components’ semantic connection• Use of the toolkit may faciliate the acquisition of representational competencies (concerning the matching of components and symbols and the matching of circuits and circuit schematics)• Usable with either handheld AR-devices or head-mounted AR-devices},
    year = {2022},
    title = {Encountering Students' Learning Difficulties in Electrics - Didactical Concept and Prototype of Augmented Reality-Toolkit},
    booktitle = {Fostering scientific citizenship in an uncertain world - ESERA 2021 e-Proceedings. European Science Education Research Association Conference (ESERA-2021), Workshop: Digital Resources for Science Teaching and Learning, August 30 - September 3},
    publisher = {University of Minho},
    author = {Luisa Lauer and Hamraz Javaheri and Kristin Altmeyer and Sarah Malone and Agnes Grünerbl and Michael Barz and Markus Peschel and Roland Brünken and Paul Lukowicz},
    organization = {University of Minho},
    url = {https://www.markus-peschel.de/files_neu/publikationen/Workshop_ESERA2021-komprimiert.pdf https://www.dfki.de/fileadmin/user_upload/import/12121_2022_Encountering_Students'_Learning_Difficulties_in_Electrics_-_Didactical_Concept_and_Prototype_of_Augmented_Reality-Toolkit.pdf}
}

@article{pub12165,
    abstract = {Eye movements were shown to be an effective source of implicit relevance feedback in constrained search and decision-making tasks. Recent research suggests that gaze-based features, extracted from scanpaths over short news articles (g-REL), can reveal the perceived relevance of read text with respect to a previously shown trigger question. In this work, we aim to confirm this finding and we investigate whether it generalizes to multi-paragraph documents from Wikipedia (Google Natural Questions) that require readers to scroll down to read the whole text. We conduct a user study (n=24) in which participants read single- and multi-paragraph articles and rate their relevance at the paragraph level with respect to a trigger question. We model the perceived document relevance using machine learning and features from the literature as input. Our results confirm that eye movements can be used to effectively model the relevance of short news articles, in particular if we exclude difficult cases: documents which are on topic of the trigger questions but irrelevant. However, our results do not clearly show that the modeling approach generalizes to multi-paragraph document settings. We publish our dataset and our code for feature extraction under an open source license to enable future research in the field of gaze-based implicit relevance feedback.},
    month = {1},
    year = {2022},
    title = {Implicit Estimation of Paragraph Relevance from Eye Movements},
    journal = {Frontiers in Computer Science},
    volume = {3},
    pages = {13},
    publisher = {Frontiers Media S.A.},
    doi = {https://doi.org/10.3389/fcomp.2021.808507},
    author = {Michael Barz and Omair Shahzad Bhatti and Daniel Sonntag},
    keywords = {Implicit relevance feedback, Reading analysis, machine learning, eye tracking, Perceived paragraph relevance, Eye movements and reading},
    url = {https://www.frontiersin.org/articles/10.3389/fcomp.2021.808507 https://www.dfki.de/fileadmin/user_upload/import/12165_fcomp-03-808507.pdf}
}

@misc{pub12167,
    abstract = {We propose an approach for interactive learning for an image captioning model. As human feedback is expensive and modern neural network based approaches often require large amounts of supervised data to be trained, we envision a system that exploits human feedback as good as possible by multiplying the feedback using data augmentation methods, and integrating the resulting training examples into the model in a smart way. This approach has three key components, for which we need to find suitable practical implementations: feedback collection, data augmentation, and model update. We outline our idea and review different possibilities to address these tasks.},
    month = {2},
    year = {2022},
    title = {Interactive Machine Learning for Image Captioning},
    howpublished = {The AAAI-22 Workshop on Interactive Machine Learning},
    author = {Mareike Hartmann and Aliki Anagnostopoulou and Daniel Sonntag},
    keywords = {learning from feedback, image captioning, data augmentation},
    url = {https://www.dfki.de/fileadmin/user_upload/import/12167_interactive_learning_for_image_captioning.pdf}
}

@article{pub12216,
    abstract = {Existing skin attributes detection methods usually initialize with a pre-trained Imagenet network and then fine-tune on a medical target task. However, we argue that such approaches are suboptimal because medical datasets are largely different from ImageNet and often contain limited training samples. In this work, we propose Task Agnostic Transfer Learning (TATL), a novel framework motivated by dermatologists' behaviors in the skincare context. TATL learns an attribute-agnostic segmenter that detects lesion skin regions and then transfers this knowledge to a set of attribute-specific classifiers to detect each particular attribute. Since TATL's attribute-agnostic segmenter only detects skin attribute regions, it enjoys ample data from all attributes, allows transferring knowledge among features, and compensates for the lack of training data from rare attributes.  We conduct extensive experiments to evaluate the proposed TATL transfer learning mechanism with various neural network architectures on two popular skin attributes detection benchmarks. The empirical results show that TATL not only works well with multiple architectures but also can achieve state-of-the-art performances, while enjoying minimal model and computational complexities. We also provide theoretical insights and explanations for why our transfer learning framework performs well in practice.},
    year = {2022},
    title = {TATL: Task Agnostic Transfer Learning for Skin Attributes Detection},
    journal = {Medical Image Analysis},
    volume = {01},
    pages = {1-27},
    publisher = {Elsevier},
    author = {Ho Minh Duy Nguyen and Thu T. Nguyen and Huong Vu and Quang Pham and Manh-Duy Nguyen and Binh T. Nguyen and Daniel Sonntag},
    url = {https://arxiv.org/pdf/2104.01641.pdf https://www.dfki.de/fileadmin/user_upload/import/12216_TATL_Task_Agnostic_Transfer_Learning_for_Skin_Attributes_Detection.pdf}
}

@article{pub12243,
    abstract = {Rational decision makers aim to maximize their gains, but humans and other animals often fail to do so, exhibiting biases and distortions in their choice behavior. In a recent study of economic decisions, humans, mice, and rats were reported to succumb to the sunk cost fallacy, making decisions based on irrecoverable past investments to the detriment of expected future returns. We challenge this interpretation because it is subject to a statistical fallacy, a form of attrition bias, and the observed behavior can be explained without invoking a sunk cost–dependent mechanism. Using a computational model, we illustrate how a rational decision maker with a reward-maximizing decision strategy reproduces the reported behavioral pattern and propose an improved task design to dissociate sunk costs from fluctuations in decision valuation. Similar statistical confounds may be common in analyses of cognitive behaviors, highlighting the need to use causal statistical inference and generative models for interpretation.},
    number = {6},
    month = {2},
    year = {2022},
    title = {Apparent sunk cost effect in rational agents},
    journal = {Science Advances},
    volume = {8},
    pages = {1-10},
    publisher = {American Association for the Advancement of Science},
    doi = {https://doi.org/10.1126/sciadv.abi7004},
    author = {Torben Ott and Paul Masset and Thiago Gouvea and Adam Kepecs},
    url = {https://www.science.org/doi/10.1126/sciadv.abi7004 https://www.dfki.de/fileadmin/user_upload/import/12243_Apparent_sunk_cost_effect_in_rational_agents.pdf}
}

@inproceedings{pub12286,
    abstract = {Multi-Camera Multi-Object Tracking is currently drawing attention in the computer vision field due to its superior performance in real-world applications such as video surveillance with crowded scenes or in vast space. In this work, we propose a mathematically elegant multi-camera multiple object tracking approach based on a spatial-temporal lifted multicut formulation. Our model utilizes state-of-the-art tracklets produced by single-camera trackers as proposals. As these tracklets may contain ID-Switch errors, we refine them through a novel pre-clustering obtained from 3D geometry projections. As a result, we derive a better tracking graph without ID switches and more precise affinity costs for the data association phase. Tracklets are then matched to multi-camera trajectories by solving a global lifted multicut formulation that incorporates short and long-range temporal interactions on tracklets located in the same camera as well as inter-camera ones. Experimental results on the WildTrack dataset yield near-perfect result, outperforming state-of-the-art trackers on Campus while being on par on the PETS-09 dataset. We will make our implementations available upon acceptance of the paper.},
    year = {2022},
    title = {LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking},
    booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR) 2022. International Conference on Computer Vision and Pattern Recognition (CVPR), June 21-24},
    publisher = {IEEE/CVF},
    author = {Ho Minh Duy Nguyen and Roberto Henschel and Bodo Rosenhahn and Daniel Sonntag and Paul Swoboda},
    url = {https://arxiv.org/pdf/2111.11892.pdf https://www.dfki.de/fileadmin/user_upload/import/12286_LMGP_Lifted_Multicut_Meets_Geometry_Projections_for_Multi-Camera.pdf}
}

@inproceedings{pub12287,
    abstract = {Eye movements were shown to be an effective source of implicit relevance feedback in information retrieval tasks. They can be used to, e.g., estimate the relevance of read documents and expand search queries using machine learning. In this paper, we present the Reading Model Assessment tool (ReMA), an interactive tool for assessing gaze-based relevance estimation models. Our tool allows experimenters to easily browse recorded trials, compare the model output to a ground truth, and visualize gaze-based features at the token- and paragraph-level that serve as model input. Our goal is to facilitate the understanding of the relation between eye movements and the human relevance estimation process, to understand the strengths and weaknesses of a model at hand, and, eventually, to enable researchers to build more effective models.},
    month = {3},
    year = {2022},
    title = {Interactive Assessment Tool for Gaze-based Machine Learning Models in Information Retrieval},
    booktitle = {ACM SIGIR Conference on Human Information Interaction and Retrieval. ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR-2022), March 14-18, Regensburg, Germany},
    isbn = {9781450391863},
    publisher = {Association for Computing Machinery},
    doi = {https://doi.org/10.1145/3498366.3505834},
    author = {Pablo Valdunciel and Omair Shahzad Bhatti and Michael Barz and Daniel Sonntag},
    keywords = {eye tracking, relevance estimation, information retrieval, reading, data visualization, interactive model assessment},
    url = {https://www.dfki.de/fileadmin/user_upload/import/12287_3498366.3505834.pdf https://www.researchgate.net/publication/359219435_Interactive_Assessment_Tool_for_Gaze-based_Machine_Learning_Models_in_Information_Retrieval}
}

@inproceedings{pub12428,
    series = {Frontiers of AI},
    abstract = {Professionals of all domains of expertise expect to take part in the benefits of the machine learning (ML) revolution, but realisation is often slowed down by lack of training in ML concepts and tools, as well as low availability of annotated data for supervised methods. Inspired by the problem of assessing the impact of human-generated activity on marine ecosystems through passive acoustic monitoring (PAM), we are developing Seadash, an interactive tool for event detection and classification in multivariate time series.},
    year = {2022},
    title = {Annotating sound events through interactive design of interpretable features},
    booktitle = {Proceedings of the First International Conference on Hybrid Human-Machine Intelligence. International Conference on Hybrid Human-Artificial Intelligence (HHAI-2022), June 13-17, Amsterdam, Netherlands},
    publisher = {IOS Press},
    author = {Thiago Gouvea and Ilira Troshani and Marc Herrlich and Daniel Sonntag},
    keywords = {data analysis, interactive machine learning, data visualization, data mining, representation learning},
    url = {https://www.dfki.de/fileadmin/user_upload/import/12428_Gouvea_et_al_2022_Annotating_Sound_Events_Through_Interactive_Design_of_Interpretable_Features.pdf https://ebooks.iospress.nl/doi/10.3233/FAIA220225}
}

@inproceedings{pub12429,
    abstract = {Machine learning (ML) is increasingly used in different application domains. However, to reach its full potential it is important that experts without extensive ML training be able to create and effectively apply models in their domain. This requires forms of co-learning that need to be facilitated by effective interfaces and interaction paradigms. Inspired by the problem of detecting and classifying sound events in marine soundscapes, we are developing Seadash. Through a rapid, iterative data exploration workflow, the user designs and curates features that capture meaningful structure in the data, and uses these to efficiently annotate the dataset. While the tool is still in early stages, we present the concept and discuss future directions.},
    year = {2022},
    title = {Interactive design of interpretable features for marine soundscape data annotation},
    booktitle = {Workshop on Human-centered Design of Symbiotic Hybrid Intelligence. Workshop on Human-centered Design of Symbiotic Hybrid Intelligence (HCSHI-2022), located at HHAI, June 14, Amsterdam, Netherlands},
    publisher = {HHAI},
    author = {Thiago S. Gouvêa and Ilira Troshani and Marc Herrlich and Daniel Sonntag},
    keywords = {interactive machine learning, marine research, passive acoustic monitoring, sound classification, data programming},
    url = {https://ii.tudelft.nl/humancenteredsymbioticHI/node/9 https://www.dfki.de/fileadmin/user_upload/import/12429_Gouvea_et_al_2022_Interactive_design_of_interpretable_features_for_marine_soundscape_data.pdf}
}

@misc{pub12516,
    abstract = {Image Captioning (IC) models can highly benefit from human feedback in the training process, especially in cases where data is limited. We present work-in-progress on adapting an IC system to integrate human feedback, with the goal to make it easily adaptable to user-specific data. Our approach builds on a base IC model pre-trained on the MS COCO dataset, which generates captions for unseen images. The user will then be able to offer feedback on the image and the generated/predicted caption, which will be augmented to create additional training instances for the adaptation of the model. The additional instances are integrated into the model using step-wise updates, and a sparse memory replay component is used to avoid catastrophic forgetting. We hope that this approach, while leading to improved results, will also result in customizable IC models.},
    month = {7},
    year = {2022},
    title = {Putting Humans in the Image Captioning Loop},
    howpublished = {Bridging Human-Computer Interaction and Natural Language Processing (NAACL 2022)},
    author = {Aliki Anagnostopoulou and Mareike Hartmann and Daniel Sonntag},
    url = {https://drive.google.com/file/d/1WT1Emfc76Myv_PujMXaWI4ucqF9eegqC/view https://www.dfki.de/fileadmin/user_upload/import/12516_5.pdf}
}

@inproceedings{pub12519,
    abstract = {Training a model with access to human explanations can improve data efficiency and model performance on in- and out-of-domain data. Adding to these empirical findings, similarity with the process of human learning makes learning from explanations a promising way to establish a fruitful human-machine interaction. Several methods have been proposed for improving natural language processing (NLP) models with human explanations, that rely on different explanation types and mechanism for integrating these explanations into the learning process. These methods are rarely compared with each other, making it hard for practitioners to choose the best combination of explanation type and integration mechanism for a specific use-case. In this paper, we give an overview of different methods for learning from human explanations, and discuss different factors that can inform the decision of which method to choose for a specific use-case.},
    year = {2022},
    title = {A survey on improving NLP models with human explanations},
    booktitle = {Proceedings of the First Workshop on Learning with Natural Language Supervision. Workshop on Learning with Natural Language Supervision, located at ACL 2022, May 26, Dublin, Ireland},
    publisher = {Association for Computational Linguistics},
    author = {Mareike Hartmann and Daniel Sonntag},
    keywords = {Human explanations; NLP},
    url = {https://aclanthology.org/2022.lnls-1.5.pdf https://www.dfki.de/fileadmin/user_upload/import/12519_A_survey_on_improving_NLP_models_with_human_explanations.pdf}
}

@inproceedings{pub12535,
    abstract = {The Player Experience Inventory (PXI), initially developed by Abeele et al. (2020), measures player experiences among English-speaking players. However, empirically validated translations of the PXI are sparse, limiting the use of the scale among non-English speaking players. In this paper, we address this issue by providing a translated version of the scale in German, the most widely spoken first language in the European Union. After translating the original items, we conducted a confirmatory factor analysis (N=506) to validate the German version of the PXI. Our results confirmed a 10-factor model - which the original authors of the instrument suggested - and show that the German PXI has valid psychometric properties. While model fit, internal consistency and convergent validity were acceptable, there was room for improvement regarding discriminant validity. Based on our results, we advocate for the German PXI as a valid and reliable instrument for assessing player experiences in German-speaking samples.},
    year = {2022},
    title = {Development and Validation of a German Version of the Player Experience Inventory (PXI)},
    booktitle = {Proceedings of the Mensch und Computer Conference. Mensch und Computer (MuC-2022), September 4-7, Darmstadt, Germany},
    publisher = {ACM},
    doi = {https://doi.org/10.1145/3543758.3543763},
    author = {Linda Graf and Maximilian Altmeyer and Katharina Emmerich and Marc Herrlich and Andrey Krekhov and Katta Spiel},
    keywords = {games user research;PXI;validation;games},
    url = {https://www.dfki.de/fileadmin/user_upload/import/12535_MuC22__German_PXI_Version.pdf https://dl.acm.org/doi/10.1145/3543758.3543763}
}

@inproceedings{pub12550,
    abstract = {Smartwatches and fitness trackers integrate different sensors from inertial measurement units to heart rate sensors in a very compact and affordable form factor. This makes them interesting and relevant research tools. One potential application domain is virtual reality, e.g., for health related applications such as exergames or simulation approaches. However, commercial devices complicate and limit the collection of raw and real-time data, suffer from privacy issues and are not tailored to using them with VR tracking systems. We address these issues with an open source design to facilitate the construction of VR-enabled wearables for conducting scientific experiments. Our work is motivated by research in mixed realities in pervasive computing environments. We introduce our system and present a proof-of-concept study with 17 participants. Our results show that the wearable reliably measures high-quality data comparable to commercially available fitness trackers and that it does not impede movements or interfere with VR tracking.},
    month = {4},
    year = {2022},
    title = {SpiderClip: Towards an Open Source System for Wearable Device Simulation in Virtual Reality},
    booktitle = {CHI EA '22: Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems. International Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA-2022)},
    publisher = {Association for Computing Machinery},
    doi = {https://doi.org/10.1145/3491101.3519758},
    author = {Dirk Queck and Iannis Albert and Nicole Burkard and Philipp Zimmer and Georg Volkmar and Bastian Dänekas and Rainer Malaka and Marc Herrlich},
    url = {https://dl.acm.org/doi/abs/10.1145/3491101.3519758#sec-supp https://www.dfki.de/fileadmin/user_upload/import/12550_SpiderClip__Towards_an_Open_Source_System_for_Wearable.pdf}
}

@inproceedings{pub12619,
    abstract = {Silent speech Brain-Computer Interfaces (BCIs) try to decode imagined speech from brain activity. Those BCIs require a tremendous amount of training data usually collected during mentally and physically exhausting sessions in which participants silently repeat words presented on a screen for several hours. Within this work we present an approach to overcome those exhausting sessions by training a silent speech classifier on data recorded while speaking certain words and transferring this classifier to EEG data recorded during silent repetition of the same words. This approach does not only allow for a less mentally and physically exhausting training procedure but also for a more productive one as the overt speech output can be used for interaction while the classifier for silent speech is trained simultaneously.
We evaluated our approach in a study in which 15 participants navigated a virtual robot on a screen in a game like scenario through a maze once with 5 overtly spoken and once with the same 5 silently spoken command words. In an offline analysis we trained a classifier on overt speech data and let it predict silent speech data. Our classification results do not only show successful results for the transfer (61.78%) significantly above chance level but also comparable results to a standard silents speech classifier (71.48%) trained and tested on the same data. These results illustrate the potential of the method to replace the currently tedious training procedures for silent speech BCIs with a more comfortable, engaging and productive approach by a transfer from overt to silent speech.},
    year = {2022},
    title = {Improving Silent Speech BCI Training Procedures through Transfer from Overt to Silent Speech},
    booktitle = {Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics. IEEE International Conference on Systems, Man, and Cybernetics (SMC-2022)},
    publisher = {IEEE},
    author = {Maurice Rekrut and Abdulrahman Mohamed Selim and Antonio Krüger}
}

@inproceedings{pub12621,
    series = {Frontiers in Artificial Intelligence and Applications},
    abstract = {Despite the current dominance of typed text, writing by hand remains the most natural mean of written communication and information keeping. Still, digital pen input provides limited user experience and lacks flexibility, as most of the manipulations are performed on a digitalized version of the text. In this paper, we present our prototype that enables spellchecking for handwritten text: it allows users to interactively correct misspellings directly in a handwritten script. We plan to study the usability of the proposed user interface and its acceptance by users. Also, we aim to investigate how user feedback can be used to incrementally improve the underlying recognition models.},
    year = {2022},
    title = {SpellInk: Interactive correction of spelling mistakes in handwritten text},
    booktitle = {Proceedings of the First International Conference on Hybrid Human-Machine Intelligence. International Conference on Hybrid Human-Artificial Intelligence (HHAI-2022), the 1st International Conference on Hybrid Human-Artificial Intelligence, June 13-17, Amsterdam, Netherlands},
    volume = {354},
    pages = {278-280},
    isbn = {978-1-64368-308-9},
    address = {De Boelelaan 1105, 1081 HV Amsterdam, Netherlands},
    publisher = {IOS Press},
    doi = {10.3233/FAIA220216},
    author = {Konstantin Kuznetsov and Michael Barz and Daniel Sonntag},
    keywords = {digital pen, handwriting recognition, handwriting generation},
    organization = {Vrije Universiteit Amsterdam},
    url = {https://www.hhai-conference.org/demos/pd_paper_5349/ https://www.hhai-conference.org/wp-content/uploads/2022/06/hhai2022-pd_paper_5349.pdf https://www.dfki.de/fileadmin/user_upload/import/12621_hhai22_demo_spellink.pdf}
}

@inproceedings{pub12622,
    series = {Frontiers in Artificial Intelligence and Applications},
    abstract = {Many features have been proposed for encoding the input signal from digital pens and touch-based interaction. They are widely used for analyzing and classifying handwritten texts, sketches, or gestures. Although they are well defined mathematically, many features are non-trivial and therefore difficult to understand for a human. In this paper, we present an application that visualizes a subset from 114 digital pen features in real-time while drawing. It provides an easy-to-use interface that allows application developers and machine learning practitioners to learn how digital pen features encode their inputs, helps in the feature selection process, and enables rapid prototyping of sketch and gesture classifiers.},
    year = {2022},
    title = {pEncode: A Tool for Visualizing Pen Signal Encodings in Real-time},
    booktitle = {Proceedings of the First International Conference on Hybrid Human-Machine Intelligence. International Conference on Hybrid Human-Artificial Intelligence (HHAI-2022), 1st International Conference on Hybrid Human-Artificial Intelligence, June 13-17, Amsterdam, Netherlands},
    volume = {354},
    pages = {281-284},
    isbn = {978-1-64368-308-9},
    address = {De Boelelaan 1105, 1081 HV Amsterdam, Netherlands},
    publisher = {IOS Press},
    doi = {10.3233/FAIA220217},
    author = {Felix Céard-Falkenberg and Konstantin Kuznetsov and Alexander Prange and Michael Barz and Daniel Sonntag},
    keywords = {digital pen, gesture recognition, digital pen features, machine learning},
    organization = {Vrije Universiteit Amsterdam},
    url = {https://www.hhai-conference.org/demos/pd_paper_6439/ https://www.youtube.com/watch?v=t80aa2E5jKo https://www.dfki.de/fileadmin/user_upload/import/12622_hhai22_demo_pencode.pdf}
}

@inproceedings{pub12633,
    abstract = {Interactive Machine Learning (IML) systems incorporate humans into the learning process to enable iterative and continuous model improvements. The interactive process can be designed to leverage the expertise of domain experts with no background in machine learning, for instance, through repeated user feedback requests. However, excessive requests can be perceived as annoying and cumbersome and could reduce user trust. Hence, it is mandatory to establish an efficient dialog between a user and a machine learning system. We aim to detect when a domain expert disagrees with the output of a machine learning system by observing its eye movements and facial expressions. In this paper, we describe our approach for modelling user disagreement and discuss how such a model could be used for triggering user feedback requests in the context of interactive machine learning.},
    year = {2022},
    title = {Leveraging Implicit Gaze-Based User Feedback for Interactive Machine Learning},
    booktitle = {KI 2022: Advances in Artificial Intelligence. German Conference on Artificial Intelligence (KI), Cham},
    editor = {Ralph Bergmann and Lukas Malburg and Stephanie C. Rodermund and Ingo J. Timm},
    pages = {9-16},
    isbn = {978-3-031-15791-2},
    publisher = {Springer International Publishing},
    author = {Omair Shahzad Bhatti and Michael Barz and Daniel Sonntag},
    url = {https://doi.org/10.1007/978-3-031-15791-2 https://www.dfki.de/fileadmin/user_upload/import/12633_Leveraging_implicit_gaze_based_user_feedback_for_interactive_machine_learning__KI_22__Accepted__(6).pdf}
}

@inproceedings{pub12809,
    abstract = {Writing the conclusion section of radiology reports is essential for communicating the radiology findings and its assessment to physicians in a condensed form. In this work, we employ a transformer-based Seq2Seq model for generating the conclusion section of German radiology reports. The model is initialized with the pre-trained parameters of a German BERT model and fine-tuned in our downstream task on our domain data. We proposed two strategies to improve the factual correctness of the model. In the first method, next to the abstractive learning objective, we introduce an extraction learning objective to train the decoder in the model to both generate one summary sequence and extract the key findings from the source input. The second approach is to integrate the pointer mechanism into the transformer-based Seq2Seq model. The pointer network helps the Seq2Seq model to choose between generating tokens from the vocabulary or copying parts from the source input during generation. The results of the automatic and human evaluations show that the enhanced Seq2Seq model is capable of generating human-like radiology conclusions and that the improved models effectively reduce the factual errors in the generations despite the small amount of training data.},
    month = {7},
    year = {2022},
    title = {Fine-tuning BERT Models for Summarizing German Radiology Findings},
    booktitle = {Proceedings of the 4th Clinical Natural Language Processing Workshop. Clinical Natural Language Processing Workshop (ClinicalNLP-2022), located at NAACL 2022, July 14, Seattle,, WA, USA},
    editor = {Tristan Naumann and Steven Bethard and Kirk Roberts and Anna Rumshisky},
    publisher = {Association for Computational Linguistics},
    author = {Siting Liang and Klaus Kades and Matthias A. Fink and Peter M. Full and Tim F. Weber and Jens Kleesiek and Michael Strube and Klaus Maier-Hein},
    url = {https://aclanthology.org/2022.clinicalnlp-1.4.pdf https://www.dfki.de/fileadmin/user_upload/import/12809_2022.clinicalnlp-1.4.pdf}
}

@inproceedings{pub12839,
    abstract = {This paper presents our project proposal for extracting biomedical information from German clinical narratives with limited amounts of annotations. We first describe the applied strategies in transfer learning and active learning for solving our problem. After that, we discuss the design of the user interface for both supplying model inspection and obtaining user annotations in the interactive environment.},
    month = {7},
    year = {2022},
    title = {Cross-lingual German Biomedical Information Extraction: from Zero-shot to Human-in-the-Loop},
    booktitle = {Second Workshop on Bridging Human-Computer Interaction and Natural Language Processing - Proceedings of the Workshop. Workshop on Bridging Human-Computer Interaction and Natural Language Processing (HCI+NLP-2022), located at NAACL 2022, July 15, Seattle, Washington, USA},
    isbn = {978-1-955917-90-2},
    publisher = {Association for Computational Linguistics},
    author = {Siting Liang and Mareike Hartmann and Daniel Sonntag},
    organization = {NAACL},
    url = {https://www.dfki.de/fileadmin/user_upload/import/12839_2022_HCI+NLP.3.1.pdf https://arxiv.org/abs/2301.09908}
}

@article{pub12840,
    abstract = {The use of procedural content generation (PCG) in the context of video games has increased over the years as it provides an economical way to generate game content whilst enhancing their variety and replayability. For city-building games, this approach is often utilized to predefine map layouts, terrains, or cityscapes for the player. One core aspect of facilitating enjoyment in these games comes from creative expressivity. PCG, in this context, may support creativity by lowering the technical complexity for content creation, or it may hinder creativity by taking away control and freedom from the user. To examine these potential effects, this paper investigates if PCG has an impact on players' creativity in the context of VR city-building games. We present a VR prototype that provides varying degrees of procedural content: No PCG, terrain generation, city generation, and full (city + terrain) generation. In a remote user study, these conditions were compared regarding their capability to support creativity. Statistical tests for equivalence revealed that the presence of PCG did not affect creativity in any way. Our work suggests that PCG can be a useful integration into city-building games without notably decreasing players' ability to express themselves creatively.},
    year = {2022},
    title = {Effects of PCG on Creativity in Playful City-Building Environments in VR},
    journal = {Proceedings of the ACM on Human-Computer Interaction (PACMHCI)},
    volume = {6},
    pages = {1-20},
    publisher = {Association for Computing Machinery},
    author = {Georg Volkmar and Dmitry Alexandrovsky and Asmus Eike Eilks and Dirk Queck and Marc Herrlich and Rainer Malaka}
}

@article{pub12841,
    abstract = {Physical inactivity and an increasingly sedentary lifestyle constitute a significant public health concern. Exergames try to tackle this problem by combining exercising with motivational gameplay. Another approach in sports science is the use of auditory-motor synchronization, the entrainment of movements to the rhythm of music. There are already commercially successful games making use of the combination of both, such as the popular VR rhythm game BeatSaber. However, unlike traditional exercise settings often relying on periodic movements that can be easily entrained to a rhythmic pulse, exergames typically offer an additional cognitive challenge through their gameplay and might be based more on reaction or memorization. That poses the question as to what extent the effects of auditory-motor synchronization can be transferred to exergames, and if the synchronization of music and gameplay facilitates the playing experience. We conducted a user study (N = 54) to investigate the effects of different degrees of synchronization between music and gameplay using the VR rhythm game BeatSaber. Results show significant effects on performance, perceived workload, and player experience between the synchronized and non-synchronized conditions, but the results seem to be strongly mediated by the ability of the participants to consciously perceive the synchronization differences.},
    year = {2022},
    title = {The Effect of Auditory-Motor Synchronization in Exergames on the Example of the VR Rhythm Game BeatSaber},
    journal = {Proceedings of the ACM on Human-Computer Interaction (PACMHCI)},
    volume = {6},
    pages = {1-26},
    publisher = {Association for Computing Machinery},
    author = {Iannis Albert and Nicole Burkard and Dirk Queck and Marc Herrlich}
}

@misc{pub12900,
    abstract = {Semantic 3D maps are highly useful for human-robot collaboration and joint task planning. We build upon an existing real-time 3D semantic reconstruction pipeline and extend it with semantic matching across human and robot viewpoints, which is required if class labels differ or are missing due to different perspectives during collaborative reconstruction. We use deep recognition networks, which usually perform well from higher (human) viewpoints but are inferior from ground robot viewpoints. Therefore, we propose several approaches for acquiring semantic labels for unusual perspectives. We group the pixels from the lower viewpoint, project voxel class labels of the upper perspective to the lower perspective and apply majority voting to obtain labels for the robot. The quality of the reconstruction is evaluated in the Habitat simulator and in a real environment using a robot car equipped with an RGBD camera. The proposed approach can provide high-quality semantic segmentation from the robot perspective with accuracy similar to the human perspective. Furthermore, as computations are close to real time, the approach enables interactive applications.},
    month = {10},
    year = {2022},
    title = {3D Semantic Label Transfer and Matching in Human-Robot Collaboration},
    publisher = {Learning to Generate 3D Shapes and Scenes, ECCV 2022 Workshop},
    author = {Szilvia Szeier and Benjámin Baffy and Gábor Baranyi and Joul Skaf and László Kopácsi and Daniel Sonntag and Gábor Sörös and András Lőrincz},
    url = {https://learn3dg.github.io/ https://www.dfki.de/fileadmin/user_upload/import/12900_0003_paper.pdf}
}

@article{pub12980,
    abstract = {The DC-Link capacitor is defined as the essential electronics element which sources or sinks the respective currents. The reliability of DC-link capacitor-banks (CBs) encounters many challenges due to their usage in electric vehicles. Heavy shocks may damage the internal capacitors without shutting down the CB. The fundamental development obstacles of CBs are: lack of considering capacitor degradation in reliability assessment, the impact of unforeseen sudden internal capacitor faults in forecasting CB lifetime, and the faults consequence on CB degradation. The sudden faults change the CB capacitance, which leads to reliability change. To more accurately estimate the reliability, the type of the fault needs to be detected for predicting the correct post-fault capacitance. To address these practical problems, a new CB model and reliability assessment formula covering all fault types are first presented, then, a new analog fault-detection method is presented, and a combination of online-learning long short-term memory (LSTM) and fault-detection method is subsequently performed, which adapt the sudden internal CB faults with the LSTM to correctly predict the CB degradation. To confirm the correct LSTM operation, four capacitors degradation is practically recorded for 2000-hours, and the off-line faultless degradation values predicted by the LSTM are compared with the actual data. The experimental findings validate the applicability of the proposed method. The codes and data are provided.},
    month = {12},
    year = {2022},
    title = {Adaptation of A Real-Time Deep Learning Approach with An Analog Fault Detection Technique for Reliability Forecasting of Capacitor Banks Used in Mobile Vehicles},
    journal = {IEEE Access (IEEE)},
    volume = {10},
    pages = {132271-132287},
    publisher = {IEEE},
    doi = {https://doi.org/1109/ACCESS.2022.3228916},
    author = {Mohammad Amin Rezaei and Arman Fathollahi and Sajad Rezaei and Jiefeng Hu and Meysam Gheisarnejad and Ali Reza Teimouri and Rituraj Rituraj and Amirhosein Mosavi and Mohammad-Hassan Khooban},
    url = {https://www.researchgate.net/publication/366230330_Adaptation_of_A_Real-Time_Deep_Learning_Approach_with_An_Analog_Fault_Detection_Technique_for_Reliability_Forecasting_of_Capacitor_Banks_Used_in_Mobile_Vehicles https://www.dfki.de/fileadmin/user_upload/import/12980_Adaptation_of_a_Real-Time_Deep_Learning.pdf}
}

@article{pub12981,
    abstract = {The disorder that directly impacts the heart and the blood vessels inside the body is cardiovascular disease
(CVD). According to the World Health Organization reports, CVDs are the leading cause of mortality
worldwide, claiming the human life of nearly 23.6 million people annually. The categorization of diseases in
CVD includes coronary heart disease, strokes, and transient ischemic attacks (TIA), peripheral arterial disease,
aortic disease. Most CVD fatalities are caused by strokes and heart attacks, with an estimated one-third of these
deaths currently happening before 60. The standard medical organization "New York Heart Association"
(NYHA) categorize the various stages of heart failure as Class I (with no symptoms), Class II (mild symptoms),
Class III (comfortable only when in resting position), Class IV (severe condition or patient is bed-bound), and
Class V (unable to determine the class). Machine learning-based methods play an essential role in clinical data
analysis. This research presents the importance of various essential attributes related to heart disease based on
a hybrid machine learning model. The proposed hybrid model SVM-GA is based on a support vector machine
and the genetic algorithm. This research analyzed an online dataset obtainable at the UCI Machine Learning
Repository with the medical data of 299 patients who suffered from heart failures and are classified as Class
III or IV as per the standard NYHA. This dataset was collected through patients' available follow-up and
checkup duration and involved thirteen clinical characteristics. The proposed machine learning models were
used to calculate feature importance in this research. The proposed model and existing well-known machine
learning based-models, i.e., Bayesian generalized linear model, ANN, Bagged CART, Bag Earth, and SVM,
are implemented using Python and various performance measuring parameters, i.e., accuracy, processing time,
precision, recall, F-measures are calculated. Experimental analysis shows the proposed SVM-GA model
strengthens in terms of better accuracy, processing time, precision, recall, F-measures over existing methods.},
    month = {12},
    year = {2022},
    title = {Predicting the Risk of Heart Failure Based on Clinical Data},
    journal = {Human-centric Computing and Information Sciences (HCIS)},
    volume = {12},
    pages = {1322-1355},
    publisher = {Kora Information Processing Soc (KIPS-CSWRG))},
    doi = {https://doi.org/10.22967/HCIS.2022.12.057},
    author = {Jasminder Kaur Sandhu and Umesh Kumar Lilhore and Poongodi M and Navpreet Kaur and Shahab S. Band and Mounir Hamdi and Celestine Iwendi and Sarita Simaiya and M.M. Kamruzzaman and Amirhosein Mosavi},
    keywords = {Heart Failure, Machine Learning, Computing, Healthcare, Biomedical Diagnosis},
    url = {https://www.researchgate.net/publication/366297985_Predicting_the_Risk_of_Heart_Failure_Based_on_Clinical_Data/link/639b3250e42faa7e75c57942/download?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6InB1YmxpY2F0aW9uIiwicGFnZSI6InB1YmxpY2F0aW9uIn19 https://www.dfki.de/fileadmin/user_upload/import/12981_Predicting_the_Risk_of_Heart_Failure_Based_on_Clinical_Data.pdf}
}

@article{pub12990,
    abstract = {The combination of an offshore wind turbine and a wave energy converter on an integrated platform is an economical solution for the electrical power demand in coastal countries. Due to the expensive installation cost, a prediction should be used to investigate whether the location is suitable for these sites. For this purpose, this research presents the feasibility of installing a combined hybrid site in the desired coastal location by predicting the net produced power due to the environmental parameters. For combining these two systems, an optimized array includes ten turbines and ten wave energy converters. The mathematical equations of the net force on the two introduced systems and the produced power of the wind turbines are proposed. The turbines’ maximum forces are 4 kN, and for the wave energy converters are 6 kN, respectively. Furthermore, the comparison is conducted in order to find the optimum system. The comparison shows that the most effective system of desired environmental condition is introduced. A number of machine learning and deep learning methods are used to predict key parameters after collecting the dataset. Moreover, a comparative analysis is conducted to find a suitable model. The models’ performance has been well studied through generating the confusion matrix and the receiver operating characteristic (ROC) curve of the hybrid site. The deep learning model outperformed other models, with an approximate accuracy of 0.96.},
    number = {24},
    month = {12},
    year = {2022},
    title = {Deep Learning for Modeling an Offshore Hybrid Wind–Wave Energy System},
    journal = {Energies},
    volume = {15},
    pages = {9484-9494},
    publisher = {MDPI},
    doi = {https://doi.org/10.3390/en15249484},
    author = {Mahsa Manshadi and Milad Mousavi and M. Soltani and Amirhosein Mosavi and Levente Kovacs},
    keywords = {renewable energy; artificial intelligence; machine learning; comparative analysis; wind turbine; energy; deep learning; big data; wave energy; wave power; offshore},
    url = {https://www.mdpi.com/1996-1073/15/24/9484 https://www.dfki.de/fileadmin/user_upload/import/12990_"Deep_Learning_for_Modeling_an_Offshore_Hybrid_Wind_8211}
}

@article{pub12991,
    abstract = {In this study, a neural network-based approach is designed for mid-term load forecasting (MTLF). The structure and hyperparameters are tuned to obtain the best forecasting accuracy one year ahead. The suggested approach is practically applied to a region in Iran by the use of real-world data sets of 10 years. The influential factors such as economic, weather, and social factors are investigated, and their impact on accuracy is numerically analyzed. The bad data are detected by a suggested effective method. In addition to load peak, the 24-hours load pattern is also predicted, which helps for better mid-term planning. The simulations show that the suggested approach is practical, and the accuracy is more than 95%, even when there are drastic weather changes.},
    month = {11},
    year = {2022},
    title = {An Experimental Machine Learning Approach for Mid-Term Energy Demand Forecasting},
    journal = {IEEE Access (IEEE)},
    volume = {10},
    pages = {118926-118940},
    publisher = {IEEE},
    doi = {https://doi.org/10.1109/ACCESS.2022.3221454},
    author = {Shu-Rong Yan and Manwen Tian and Khalid A. Alattas and Ardashir Mohamadzadeh and Mohammad Hosein Sabzalian and Amirhosein Mosavi},
    url = {https://ieeexplore.ieee.org/document/9945969 https://www.dfki.de/fileadmin/user_upload/import/12991_An_Experimental_Machine_Learning_Approach_for_Mid-Term_Energy_Demand_Forecasting.pdf}
}

@article{pub13116,
    abstract = {Artificial Intelligence (AI) systems are increasingly pervasive: Internet of Things, in-car intelligent devices, robots, and virtual assistants, and their large-scale adoption makes it necessary to explain their behaviour, for example to their users who are impacted by their decisions, or to their developers who need to ensure their functionality. This requires, on the one hand, to obtain an accurate representation of the chain of events that caused the system to behave in a certain way (e.g., to make a specific decision). On the other hand, this causal chain needs to be communicated to the users depending on their needs and expectations. In this phase of explanation delivery, allowing interaction between user and model has the potential to improve both model quality and user experience. The XAINES project investigates the explanation of AI systems through narratives targeted to the needs of a specific audience, focusing on two important aspects that are crucial for enabling successful explanation: generating and selecting appropriate explanation content, i.e. the information to be contained in the explanation, and delivering this information to the user in an appropriate way. In this article, we present the project’s roadmap towards enabling the explanation of AI with narratives.},
    month = {12},
    year = {2022},
    title = {XAINES: Explaining AI with Narratives},
    editor = {Ute Schmid and Britta Wrede},
    journal = {KI - Künstliche Intelligenz, German Journal on Artificial Intelligence - Organ des Fachbereiches "Künstliche Intelligenz" der Gesellschaft für Informatik e.V. (KI)},
    volume = {36},
    pages = {287-296},
    publisher = {Springer},
    doi = {10.1007/s13218-022-00780-8},
    author = {Mareike Hartmann and Han Du and Nils Feldhus and Ivana Kruijff-Korbayová and Daniel Sonntag},
    keywords = {xai, explainable ai, interaction, explanations},
    url = {https://doi.org/10.1007/s13218-022-00780-8 https://www.dfki.de/fileadmin/user_upload/import/13116_s13218-022-00780-8.pdf}
}

@inproceedings{pub11369,
    abstract = {Besides principal polymerase chain reaction (PCR) tests, automatically identifying positive samples based on computed tomography (CT) scans can present a promising option in the early diagnosis of COVID-19. Recently, there have been increasing efforts to utilize deep networks for COVID-19 diagnosis based on CT scans. While these approaches mostly focus on introducing novel architectures, transfer learning techniques or construction of large scale data, we propose a novel strategy to improve several performance baselines by leveraging multiple useful information sources relevant to doctors' judgments.  Specifically, infected regions and heat-map features extracted from learned networks are integrated with the global image via an attention mechanism during the learning process. This procedure makes our system more robust to noise and guides the network focusing on local lesion areas.  Extensive experiments illustrate the superior performance of our approach compared to recent baselines. Furthermore, our learned network guidance presents an explainable feature to doctors to understand the connection between input and output in a grey-box model.},
    year = {2021},
    title = {An Attention Mechanism using Multiple Knowledge Sources for COVID-19 Detection from CT Images},
    booktitle = {The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21). AAAI Conference on Artificial Intelligence (AAAI), Workshop on Trustworthy AI for Healthcare, February 2-9, Vancouver,, BC, Canada},
    note = {Virtual Conference},
    publisher = {AAAI},
    author = {Ho Minh Duy Nguyen and Duy M. Nguyen and Huong Vu and Binh T. Nguyen and Fabrizio Nunnari and Daniel Sonntag},
    keywords = {Explainable AI, Covid-19, Medical Imaging},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11369_AAAI_Workshop_TrustworthyHealthcare_v3.pdf https://arxiv.org/abs/2009.11008}
}

@inproceedings{pub11432,
    abstract = {The Trail Making Test (TMT) is a frequently used neuropsychological test for assessing cognitive performance. The subject connects a sequence of numbered nodes by using a pen on normal paper. We present an automatic cognitive assessment tool that analyzes samples of the TMT which we record using a digital pen. This enables us to analyze digital pen features that are difficult or impossible to evaluate manually. Our system automatically measures several pen features, including the completion time which is the main performance indicator used by clinicians to score the TMT in practice. In addition, our system provides a structured report of the analysis of the test, for example indicating missed or erroneously connected nodes, thereby offering more objective, transparent and explainable results to the clinician. We evaluate our system with 40 elderly subjects from a geriatrics daycare clinic of a large hospital.},
    year = {2021},
    title = {Explainable Automatic Evaluation of the Trail Making Test for Dementia Screening},
    booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM International Conference on Human Factors in Computing Systems (CHI-2021), May 8-13, Yokohama, Japan},
    note = {Virtual Conference},
    isbn = {978-1-4503-8096-6/21/05},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    author = {Alexander Prange and Michael Barz and Anika Heimann-Steinert and Daniel Sonntag},
    url = {https://doi.org/10.1145/3411764.3445046 https://www.dfki.de/fileadmin/user_upload/import/11432_Explainable_Automatic_Evaluation_of_the_Trail_Making_Test_for_Dementia_Screening.pdf}
}

@incollection{pub11522,
    series = {Lecture Notes in Electrical Engineering},
    abstract = {We implement a method for re-ranking top-10 results of a state-of-the-art question answering (QA) system. The goal of our re-ranking approach is to improve the answer selection given the user question and the top-10 candidates. We focus on improving deployed QA systems that do not allow re-training or when re-training comes at a high cost. Our re-ranking approach learns a similarity function using n-gram based features using the query, the answer and the initial system confidence as input. Our contributions are: (1) we generate a QA training corpus starting from 877 answers from the customer care domain of T-Mobile Austria, (2) we implement a state-of-the-art QA pipeline using neural sentence embeddings that encode queries in the same space than the answer index, and (3) we evaluate the QA pipeline and our re-ranking approach using a separately provided test set. The test set can be considered to be available after deployment of the system, e.g., based on feedback of users. Our results show that the system performance, in terms of top-n accuracy and the mean reciprocal rank, benefits from re-ranking using gradient boosted regression trees. On average, the mean reciprocal rank improves by 9.15%9.15%9.15textbackslash%.},
    year = {2021},
    title = {Incremental Improvement of a Question Answering System by Re-ranking Answer Candidates Using Machine Learning},
    booktitle = {Increasing Naturalness and Flexibility in Spoken Dialogue Interaction: 10th International Workshop on Spoken Dialogue Systems},
    editor = {Erik Marchi and Sabato Marco Siniscalchi and Sandro Cumani and Valerio Mario Salerno and Haizhou Li},
    pages = {367-379},
    isbn = {9789811593239},
    publisher = {Springer, Singapore},
    doi = {https://doi.org/10.1007/978-981-15-9323-9_34},
    author = {Michael Barz and Daniel Sonntag},
    url = {https://doi.org/10.1007/978-981-15-9323-9_34 https://www.dfki.de/fileadmin/user_upload/import/11522_2019_Incremental_Improvement_of_a_Question_Answering_System_by_Re-ranking_Answer_Candidates_using_Machine_Learning.pdf https://arxiv.org/abs/1908.10149}
}

@article{pub11528,
    abstract = {Currently an increasing number of head mounted displays (HMD) for virtual and augmented reality (VR/AR) are equipped with integrated eye trackers. Use cases of these integrated eye trackers include rendering optimization and gaze-based user interaction. In addition, visual attention in VR and AR is interesting for applied research based on eye tracking in cognitive or educational sciences for example. While some research toolkits for VR already exist, only a few target AR scenarios. In this work, we present an open-source eye tracking toolkit for reliable gaze data acquisition in AR based on Unity 3D and the Microsoft HoloLens 2, as well as an R package for seamless data analysis. Furthermore, we evaluate the spatial accuracy and precision of the integrated eye tracker for fixation targets with different distances and angles to the user (n=21). On average, we found that gaze estimates are reported with an angular accuracy of 0.83 degrees and a precision of 0.27 degrees while the user is resting, which is on par with state-of-the-art mobile eye trackers.},
    number = {6},
    year = {2021},
    title = {ARETT: Augmented Reality Eye Tracking Toolkit for Head Mounted Displays},
    journal = {Sensors - Open Access Journal (Sensors)},
    volume = {21},
    pages = {18},
    publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
    doi = {https://doi.org/10.3390/s21062234},
    author = {Sebastian Kapp and Michael Barz and Sergey Mukhametov and Daniel Sonntag and Jochen Kuhn},
    keywords = {accuracy, augmented reality, eye tracking, precision, toolkit},
    url = {https://www.mdpi.com/1424-8220/21/6/2234 https://www.dfki.de/fileadmin/user_upload/import/11528_2021_ARETT-_Augmented_Reality_Eye_Tracking_Toolkit_for_Head_Mounted_Displays.pdf}
}

@techreport{pub11611,
    series = {DFKI Research Reports, RR},
    abstract = {We describe our work on information extraction in medical documents written in German, especially detecting negations using an architecture based on the UIMA pipeline. Based on our previous work on software modules to cover medical concepts like diagnoses, examinations, etc. we employ  a version of the NegEx regular expression algorithm with a large set of triggers as a baseline. We show how a significantly smaller trigger set is sufficient to achieve similar results, in order to reduce adaptation times to new text types. We elaborate on the question whether dependency parsing (based on the Stanford CoreNLP model) is a good alternative and describe the potentials and shortcomings of both approaches.},
    month = {5},
    year = {2021},
    title = {A Case Study on Pros and Cons of Regular Expression Detection and Dependency Parsing for Negation Extraction from German Medical Documents. Technical Report},
    type = {Technical Report},
    volume = {1},
    pages = {30},
    address = {Bundesministerium für Bildung und Forschung
Kapelle-Ufer 1
D-10117 Berlin},
    institution = {BMBF},
    author = {Hans-Jürgen Profitlich and Daniel Sonntag},
    keywords = {information extraction (IE); negation detection, regular expression detection, natural language processing; dependency parsing; electronic health record (EHR)},
    url = {http://arxiv.org/abs/2105.09702 https://www.dfki.de/fileadmin/user_upload/import/11611_CaseStudy_TR_final.pdf}
}

@article{pub11612,
    abstract = {Künstliche Intelligenz (KI) hat in den letzten Jahren eine neue Reifephase erreicht und entwickelt sich zum Treiber der Digitalisierung in allen Lebensbereichen. Die KI ist eine Querschnittstechnologie, die für alle Bereiche der Medizin mit Bild‑, Text- und Biodaten von großer Bedeutung ist. Es gibt keinen medizinischen Bereich, der nicht von KI beeinflusst werden wird. Dabei spielt die klinische Entscheidungsunterstützung eine wichtige Rolle. KI-Methoden etablieren sich gerade beim medizinischen Workflow-Management und bei der Vorhersage des Behandlungserfolgs bzw. des Behandlungsergebnisses. KI-Systeme können bereits in Bilddiagnose und im Patientenmanagement unterstützen, aber keine kritischen Entscheidungen vorschlagen. Die jeweiligen Präventions- oder Therapiemaßnahmen können mit KI-Unterstützung sinnvoller bewertet werden, allerdings ist die Abdeckung der Krankheiten noch viel zu gering, um robuste Systeme für den klinischen Alltag zu erstellen. Der flächendeckende Einsatz setzt Fortbildungsmaßnahmen für Ärzte voraus, um die Entscheidung treffen zu können, wann auf automatische Entscheidungsunterstützung vertraut werden kann.

Artificial intelligence (AI) has attained a new level of maturity in recent years and is becoming the driver of digitalization in all areas of life. AI is a cross-sectional technology with great importance for all areas of medicine employing image data, text data and bio-data. There is no medical field that will remain unaffected by AI, with AI-assisted clinical decision-making assuming a particularly important role. AI methods are becoming established in medical workflow management and for prediction of treatment success or treatment outcome. AI systems are already able to lend support to imaging-based diagnosis and patient management, but cannot suggest critical decisions. The corresponding preventive or therapeutic measures can be more rationally assessed with the help of AI, although the number of diseases covered is currently too low to create robust systems for routine clinical use. Prerequisite for the widespread use of AI systems is appropriate training to enable physicians to decide when computer-assisted decision-making can be relied upon.},
    month = {4},
    year = {2021},
    title = {Künstliche Intelligenz in der Medizin und Gynäkologie – Holzweg oder Heilversprechen?},
    journal = {Der Gynäkologe},
    volume = {1},
    pages = {1-7},
    publisher = {Springer},
    author = {Daniel Sonntag},
    keywords = {AI-assisted clinical decision-making, imaging-based diagnosis, robust systems for routine clinical use},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11612_sonntag-gyn.pdf https://link.springer.com/article/10.1007/s00129-021-04808-2}
}

@article{pub11613,
    abstract = {Our goal is to bridge human and machine intelligence in melanoma detection. We develop a classification system exploiting a combination of visual pre-processing, deep learning, and ensembling for providing explanations to experts and to minimize false negative rate while maintaining high accuracy in melanoma detection. Source images are first automatically segmented using a U-net CNN. The result of the segmentation is then used to extract image sub-areas and specific parameters relevant in human evaluation, namely center, border, and asymmetry measures. These data are then processed by tailored neural networks which include structure searching algorithms. Partial results are then ensembled by a committee machine. Our evaluation on the largest skin lesion dataset which is publicly available today, ISIC-2019, shows improvement in all evaluated metrics over a baseline using the original images only. We also showed that indicative scores computed by the feature classifiers can provide useful insight into the various features on which the decision can be based.},
    year = {2021},
    title = {Minimizing false negative rate in melanoma detection and providing insight into the causes of classification},
    journal = {Computing Research Repository eprint Journal (CoRR)},
    volume = {abs/2102.09199},
    pages = {1-14},
    publisher = {arXiv},
    author = {Ellák Somfai and Benjámin Baffy and Kristian Fenech and Changlu Guo and Rita Hosszú and Dorina Korózs and Fabrizio Nunnari and Marcell Pólik and Daniel Sonntag and Attila Ulbert and András Lorincz},
    url = {https://arxiv.org/abs/2102.09199 https://www.dfki.de/fileadmin/user_upload/import/11613_2021_Minimizing_false_negative_rate_in_melanoma_detection_and_providing_insight_into_the_causes_of_classification.pdf}
}

@inproceedings{pub11614,
    series = {ETRA '21 Adjunct},
    abstract = {Scanning and processing visual stimuli in a scene is essential for the human brain to make situation-aware decisions. Adding the ability to observe the scanning behavior and scene processing to intelligent mobile user interfaces can facilitate a new class of cognition-aware user interfaces. As a first step in this direction, we implement an augmented reality (AR) system that classifies objects at the user’s point of regard, detects visual attention to them, and augments the real objects with virtual labels that stick to the objects in real-time. We use a head-mounted AR device (Microsoft HoloLens 2) with integrated eye tracking capabilities and a front-facing camera for implementing our prototype.},
    month = {5},
    year = {2021},
    title = {Automatic Recognition and Augmentation of Attended Objects in Real-Time Using Eye Tracking and a Head-Mounted Display},
    booktitle = {ACM Symposium on Eye Tracking Research and Applications. Symposium on Eye Tracking Research & Applications (ETRA-2021), May 24-27, Virtual, Germany},
    pages = {4},
    isbn = {9781450383578},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {10.1145/3450341.3458766},
    author = {Michael Barz and Sebastian Kapp and Jochen Kuhn and Daniel Sonntag},
    keywords = {computer vision, visual attention, cognition-aware computing, eye tracking, augmented reality},
    url = {https://doi.org/10.1145/3450341.3458766 https://www.dfki.de/fileadmin/user_upload/import/11614_etra_ar_video.pdf}
}

@inproceedings{pub11616,
    series = {ETRA '21 Short Papers},
    abstract = {The usage of interactive public displays has increased including the number of sensitive applications and, hence, the demand for user authentication methods. In this context, gaze-based authentication was shown to be effective and more secure, but significantly slower than touch- or gesture-based methods. We implement a calibration-free and fast authentication method for situated displays based on saccadic eye movements. In a user study (n = 10), we compare our new method with CueAuth from Khamis et al. (IMWUT’18), an authentication method based on smooth pursuit eye movements. The results show a significant improvement in accuracy from 82.94% to 95.88%. At the same time, we found that the entry speed can be increased enormously with our method, on average, 18.28s down to 5.12s, which is comparable to touch-based input.},
    year = {2021},
    title = {EyeLogin - Calibration-Free Authentication Method for Public Displays Using Eye Gaze},
    booktitle = {ACM Symposium on Eye Tracking Research and Applications. Symposium on Eye Tracking Research & Applications (ETRA-2021), May 24-27, Virtual, Germany},
    isbn = {9781450383455},
    publisher = {Association for Computing Machinery},
    doi = {10.1145/3448018.3458001},
    author = {Omair Shahzad Bhatti and Michael Barz and Daniel Sonntag},
    keywords = {Calibration-free Eye Tracking, Authentication, Gaze-based Interaction, Eye Tracking, Public Displays},
    url = {https://doi.org/10.1145/3448018.3458001 https://www.dfki.de/fileadmin/user_upload/import/11616_EyeLogin.pdf}
}

@inproceedings{pub11664,
    series = {EICS '21},
    abstract = {We describe the software architecture of a toolbox of reusable components for the configuration of convolutional neural networks (CNNs) for classification and labeling problems. The toolbox architecture has been designed to maximize the reuse of established algorithms and to include domain experts in the development and evaluation process across different projects and challenges. In addition, we implemented easy-to-edit input formats and modules for XAI (eXplainable AI) through visual inspection capabilities. The toolbox is available for the research community to implement applied artificial intelligence projects.},
    year = {2021},
    title = {A Software Toolbox for Deploying Deep Learning Decision Support Systems with XAI Capabilities},
    booktitle = {Companion of the 2021 ACM SIGCHI Symposium on Engineering Interactive Computing Systems. ACM SIGCHI Symposium on Engineering Interactive Computing Systems (EICS-2021), June 8-11, Eindhoven/Virtual, Netherlands},
    isbn = {9781450384490},
    publisher = {Association for Computing Machinery},
    doi = {10.1145/3459926.3464753},
    author = {Fabrizio Nunnari and Daniel Sonntag},
    keywords = {design patterns, object-oriented architecture, deep learning, Software toolbox, convolutional neural networks, explainable AI.},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11664_nunnari21EICS-TIML.pdf https://doi.org/10.1145/3459926.3464753}
}

@article{pub11668,
    abstract = {Processing visual stimuli in a scene is essential for the human brain to make situation-aware decisions. These stimuli, which are prevalent subjects of diagnostic eye tracking studies, are commonly encoded as rectangular areas of interest (AOIs) per frame. Because it is a tedious manual annotation task, the automatic detection and annotation of visual attention to AOIs can accelerate and objectify eye tracking research, in particular for mobile eye tracking with egocentric video feeds. In this work, we implement two methods to automatically detect visual attention to AOIs using pre-trained deep learning models for image classification and object detection. Furthermore, we develop an evaluation framework based on the VISUS dataset and well-known performance metrics from the field of activity recognition. We systematically evaluate our methods within this framework, discuss potentials and limitations, and propose ways to improve the performance of future automatic visual attention detection methods.},
    number = {12},
    year = {2021},
    title = {Automatic Visual Attention Detection for Mobile Eye Tracking Using Pre-Trained Computer Vision Models and Human Gaze},
    journal = {Sensors - Open Access Journal (Sensors)},
    volume = {21},
    pages = {21},
    publisher = {MDPI},
    doi = {10.3390/s21124143},
    author = {Michael Barz and Daniel Sonntag},
    url = {https://www.mdpi.com/1424-8220/21/12/4143 https://www.dfki.de/fileadmin/user_upload/import/11668_sensors-21-04143-v2.pdf}
}

@inproceedings{pub11703,
    abstract = {Most cognitive assessments, for dementia screening for example, are conducted with a pen on normal paper. We record these tests with a digital pen as part of a new interactive cognitive assessment tool with automatic analysis of pen input. The clinician can, first, observe the sketching process in real-time on a mobile tablet, e.g., in telemedicine settings or to follow Covid-19 distancing regulations. Second, the results of an automatic test analysis are presented to the clinician in real-time, thereby reducing manual scoring effort and producing objective reports. The presented research describes the architecture of our cognitive assessment tool and examines how accurately different machine learning (ML) models can automatically score cognitive tests, without a semantic content analysis. Our system uses a set of more than 170 pen features, calculated directly from the raw digital pen signal. We evaluate our system with 40 subjects from a geriatrics daycare clinic. Using standard ML techniques our feature set outperforms previous approaches on the cognitive tests we consider, i.e., the Clock Drawing, the Rey-Osterrieth Complex Figure, and the Trail Making Test, by automatically scoring tests with up to 82% accuracy in a binary classification task.},
    year = {2021},
    title = {Assessing Cognitive Test Performance Using Automatic Digital Pen Features Analysis},
    booktitle = {Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization. International Conference on User Modeling, Adaptation, and Personalization (UMAP-2021), June 21-25, Utrecht/Virtual, Netherlands},
    isbn = {9781450383660},
    publisher = {Association for Computing Machinery},
    doi = {https://doi.org/10.1145/3450613.3456812},
    author = {Alexander Prange and Daniel Sonntag},
    keywords = {Rey-Osterrieth Complex Figure, Cognitive Assessments, Clock Drawing Test, Pen Features, Digital Pen, Deep Learning, Neurocognitive Testing, Machine Learning, Trail Making Test},
    url = {https://dl.acm.org/doi/10.1145/3450613.3456812}
}

@inproceedings{pub11715,
    abstract = {This paper investigates the problem of domain adaptation for diabetic retinopathy (DR) grading. We learn invariant target-domain features by defining a novel self-supervised task based on retinal vessel image reconstructions, inspired by medical domain knowledge. Then, a benchmark of current state-of-the-art unsupervised domain adaptation methods on the DR problem is provided. It can be shown that our approach outperforms existing domain adaption strategies. Furthermore, when utilizing entire training data in the target domain, we are able to compete with several state-of-the-art approaches in final classification accuracy just by applying standard network architectures and using image-level labels.},
    year = {2021},
    title = {Self-Supervised Domain Adaptation for Diabetic Retinopathy Grading using Vessel Image Reconstruction},
    booktitle = {Proceedings of the 44th German Conference on Artificial Intelligence. German Conference on Artificial Intelligence (KI-2021), September 27 - October 1, Berlin/Virtual, Germany},
    publisher = {Springer},
    author = {Ho Minh Duy Nguyen and Truong Thanh-Nhat Mai and Ngoc Trong Tuong Than and Alexander Prange and Daniel Sonntag},
    keywords = {Domain Adaption, Diabetic Retinopathy, Self-Supervised Learning, Deep Learning, Interactive Machine Learning},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11715_KI_2021_Self_Supervised_Domain_Adaptation_for_Diabetic_Retinopathy_Grading.pdf https://link.springer.com/chapter/10.1007/978-3-030-87626-5_26}
}

@inproceedings{pub11802,
    abstract = {Dermatologists recognize melanomas by inspecting images in which they identify human-comprehensible visual features. In this paper, we investigate to what extent such features correspond to the saliency areas identified on CNNs trained for classification. Our experiments, conducted on two neural architectures characterized by different depth and different resolution of the last convolutional layer, quantify to what extent thresholded Grad-CAM saliency maps can be used to identify visual features of skin cancer. We found that the best threshold value, i.e., the threshold at which we can measure the highest Jaccard index, varies significantly among features; ranging from 0.3 to 0.7. In addition, we measured Jaccard indices as high as 0.143, which is almost 50% of the performance of state-of-the-art architectures specialized in feature mask prediction at pixel-level, such as U-Net. Finally, a breakdown test between malignancy and classification correctness shows that higher resolution saliency maps could help doctors in spotting wrong classifications.},
    year = {2021},
    title = {On the Overlap Between Grad-CAM Saliency Maps and Explainable Visual Features in Skin Cancer Images},
    booktitle = {Machine Learning and Knowledge Extraction. International IFIP Cross Domain (CD) Conference for Machine Learning & Knowledge Extraction (MAKE) (CD-MAKE-2021), August 17-20, Virtual},
    editor = {Andreas Holzinger and Peter Kieseberg and A. Min Tjoa and Edgar Weippl},
    volume = {12844},
    pages = {241-253},
    isbn = {978-3-030-84060-0},
    publisher = {Springer International Publishing},
    author = {Fabrizio Nunnari and Md Abdul Kadir and Daniel Sonntag},
    url = {https://doi.org/10.1007/978-3-030-84060-0_16 https://www.dfki.de/fileadmin/user_upload/import/11802_2021_CD_MAKE_XAI_and_SkinFeatures.pdf}
}

@inproceedings{pub11803,
    abstract = {This paper presents an investigation on the task of anomaly detection for images of skin lesions. The goal is to provide a decision support system with an extra filtering layer to inform users if a classifier should not be used for a given sample. We tested anomaly detectors based on autoencoders and three discrimination methods: feature vector distance, replicator neural networks, and support vector data description fine-tuning. Results show that neural-based detectors can perfectly discriminate between skin lesions and open world images, but class discrimination cannot easily be accomplished and requires further investigation.},
    year = {2021},
    title = {Anomaly Detection for Skin Lesion Images Using Replicator Neural Networks},
    booktitle = {Machine Learning and Knowledge Extraction. International IFIP Cross Domain (CD) Conference for Machine Learning & Knowledge Extraction (MAKE) (CD-MAKE-2021), August 17-20, Virtual},
    editor = {Andreas Holzinger and Peter Kieseberg and A. Min Tjoa and Edgar Weippl},
    volume = {12844},
    pages = {225-240},
    isbn = {978-3-030-84060-0},
    publisher = {Springer International Publishing},
    author = {Fabrizio Nunnari and Hasan Md Tusfiqur Alam and Daniel Sonntag},
    url = {https://doi.org/10.1007/978-3-030-84060-0_15 https://www.dfki.de/fileadmin/user_upload/import/11803_2021_CD_MAKE_AnomalyDetection.pdf}
}

@inproceedings{pub11805,
    series = {Lecture Notes in Computer Science / Lecture Notes in Artificial Intelligence, LNCS / LNAI},
    abstract = {Image captioning is a complex artificial intelligence task that involves many fundamental questions of data representation, learning, and natural language processing. In addition, most of the work in this domain addresses the English language because of the high availability of annotated training data compared to other languages. Therefore, we investigate methods for image captioning in German that transfer knowledge from English training data. We explore four different methods for generating image captions in German, two baseline methods and two more advanced ones based on transfer learning. The baseline methods are based on a state-of-the-art model which we train using a translated version of the English MS COCO dataset and the smaller German Multi30K dataset, respectively. Both advanced methods are pre-trained using the translated MS COCO dataset and fine-tuned for German on the Multi30K dataset. One of these methods uses an alternative attention mechanism from the literature that showed a good performance in English image captioning. We compare the performance of all methods for the Multi30K test set in German using common automatic evaluation metrics. We show that our advanced method with the alternative attention mechanism presents a new baseline for German BLEU, ROUGE, CIDEr, and SPICE scores, and achieves a relative improvement of 21.2 % in BLEU-4 score compared to the current state-of-the-art in German image captioning.},
    month = {11},
    year = {2021},
    title = {Improving German Image Captions using Machine Translation and Transfer Learning},
    booktitle = {Statistical Language and Speech Processing SLSP 2021. International Conference on Statistical Language and Speech Processing (SLSP), 8th-9th, November 22-26, Cardiff, United Kingdom},
    editor = {Luis Espinosa-Anke and Carlos Martin-Vide and Irena Spasic},
    address = {Council Chamber
Glamorgan Building
King Edward VII Ave
Cathays Park
Cardiff
CF10 3WT},
    publisher = {Springer, Heidelberg},
    author = {Rajarshi Biswas and Michael Barz and Mareike Hartmann and Daniel Sonntag},
    keywords = {Natural language understanding and generation, Multimodal technologies, Image Captioning, Natural Language Processing},
    organization = {Cardiff University},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11805_SLSP2021Paper.pdf}
}

@inproceedings{pub11845,
    abstract = {Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a language model on domain-specific text, improves the modelling of text for downstream tasks within the domain. Numerous real-world applications are based on domain-specific text, e.g. working with financial or biomedical documents, and these applications often need to support multiple languages. However, large-scale domain-specific multilingual pretraining data for such scenarios can be difficult to obtain, due to regulations, legislation, or simply a lack of language- and domain-specific text. One solution is to train a single multilingual model, taking advantage of the data available in as many languages as possible. In this work, we explore the benefits of domain adaptive pretraining with a focus on adapting to multiple languages within a specific domain. We propose different techniques to compose pretraining corpora that enable a language model to both become domain-specific and multilingual. Evaluation on nine domain-specific datasets---for biomedical named entity recognition and financial sentence classification---covering seven different languages show  that a single multilingual domain-specific model can outperform the general multilingual model, and performs close to its monolingual counterpart. This finding holds across two different pretraining methods, adapter-based pretraining and full model pretraining.},
    month = {11},
    year = {2021},
    title = {mDAPT: Multilingual Domain Adaptive Pretraining in a Single Model},
    booktitle = {Findings of the Association for Computational Linguistics - EMNLP 2021. Conference on Empirical Methods in Natural Language Processing (EMNLP-2021), November 7-11, Online},
    volume = {1},
    pages = {3404-3018},
    publisher = {Association for Computational Linguistics},
    author = {Rasmus Kær Jørgensen and Mareike Hartmann and Xiang Dai and Desmond Elliott},
    keywords = {domain adaptive pretraining, multilingual language model},
    url = {https://aclanthology.org/2021.findings-emnlp.290.pdf https://www.dfki.de/fileadmin/user_upload/import/11845_2021.findings-emnlp.290.pdf}
}

@inproceedings{pub11846,
    abstract = {Negation is one of the most fundamental concepts in human cognition and language, and several natural language inference (NLI) probes have been designed to investigate pretrained language models' ability to detect and reason with negation. However, the existing probing datasets are limited to English only, and do not enable controlled probing of performance in the absence or presence of negation. In response, we present a multilingual (English, Bulgarian, German, French and Chinese) benchmark collection of NLI examples that are grammatical and correctly labeled, as a result of manual inspection and editing. We use the benchmark to probe the negation-awareness of multilingual language models and find that models that correctly predict examples with negation cues often fail to correctly predict their counter-examples {\em without} negation cues, even when the cues are irrelevant for semantic inference.},
    month = {11},
    year = {2021},
    title = {A Multilingual Benchmark for Probing Negation-Awareness with Minimal Pairs},
    booktitle = {Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL). Conference on Computational Natural Language Learning (CoNLL-2021), November 10-11, Online},
    pages = {224-257},
    publisher = {Association for Computational Linguistics},
    author = {Mareike Hartmann and Miryam de Lhoneux and Daniel Hershcovich and Yova Kementchedjhieva and Lukas Nielsen and Chen Qiu and Anders Søgaard},
    keywords = {negation, probing, multilingual},
    url = {https://aclanthology.org/2021.conll-1.19/ https://www.dfki.de/fileadmin/user_upload/import/11846_2021.conll-1.19.pdf}
}

@inproceedings{pub11859,
    abstract = {To improve the accuracy of convolutional neural networks in discriminating between nevi and melanomas, we test nine different combinations of masking and cropping on three datasets of skin lesion images (ISIC2016, ISIC2018, and MedNode). Our experiments, confirmed by 10-fold cross-validation, show that cropping increases classification performances, but specificity decreases when cropping is applied together with masking out healthy skin regions. An analysis of Grad-CAM saliency maps shows that in fact our CNN models have the tendency to focus on healthy skin at the border when a nevus is classified.},
    year = {2021},
    title = {Crop It, but Not Too Much: The Effects of Masking on the Classification of Melanoma Images},
    booktitle = {KI 2021: Advances in Artificial Intelligence. German Conference on Artificial Intelligence (KI-2021), September 27 - October 1, Germany},
    editor = {Stefan Edelkamp and Ralf Möller and Elmar Rueckert},
    pages = {179-193},
    isbn = {978-3-030-87626-5},
    publisher = {Springer International Publishing},
    author = {Fabrizio Nunnari and Abraham Ezema and Daniel Sonntag},
    keywords = {skin cancer, convolutional neural networks, image segmentation, masking, preprocessing, reducing bias},
    url = {https://link.springer.com/chapter/10.1007/978-3-030-87626-5_13 https://www.dfki.de/fileadmin/user_upload/import/11859_2021_KIconference_SkinLesionMasking.pdf}
}

@article{pub11866,
    abstract = {Augmenting reality via head-mounted displays (HMD-AR) is an emerging technology in education. The interactivity provided by HMD-AR devices is particularly promising for learning, but presents a challenge to human activity recognition, especially with children. Recent technological advances regarding speech and gesture recognition concerning Microsoft’s HoloLens 2 may address this prevailing issue. In a within-subjects study with 47 elementary school children (2nd to 6th grade), we examined the usability of the HoloLens 2 using a standardized tutorial on multimodal interaction in AR. The overall system usability was rated “good”. However, several behavioral metrics indicated that specific interaction modes differed in their efficiency. The results are of major importance for the development of learning applications in HMD-AR as they partially deviate from previous findings. In particular, the well-functioning recognition of children’s voice commands that we observed represents a novelty. Furthermore, we found different interaction preferences in HMD-AR among the children. We also found the use of HMD-AR to have a positive effect on children’s activity-related achievement emotions. Overall, our findings can serve as a basis for determining general requirements, possibilities, and limitations of the implementation of educational HMD-AR environments in elementary school classrooms.},
    number = {19},
    year = {2021},
    title = {Investigating the Usability of a Head-Mounted Display Augmented Reality Device in Elementary School Children},
    journal = {Sensors - Open Access Journal (Sensors)},
    volume = {21},
    pages = {20},
    publisher = {MDPI},
    doi = {10.3390/s21196623},
    author = {Luisa Lauer and Kristin Altmeyer and Sarah Malone and Michael Barz and Roland Brünken and Daniel Sonntag and Markus Peschel},
    url = {https://www.mdpi.com/1424-8220/21/19/6623 https://www.dfki.de/fileadmin/user_upload/import/11866_sensors-21-06623.pdf}
}

@misc{pub11867,
    abstract = {AI systems are increasingly pervasive and their large-scale adoption makes it necessary to explain their behaviour, for example to their users who are impacted by their decisions, or to their developers who need to ensure their functionality. This requires, on the one hand, to obtain an accurate representation of the chain of events that caused the system to behave in a certain way (e.g., to make a specific decision). On the other hand, this causal chain needs to be communicated to the users depending on their needs and expectations. In this phase of explanation delivery, allowing interaction between user and model has the potential to improve both model quality and user experience. In this abstract, we present our planned and on-going work on the interaction with explanations as part of the XAINES project. The project investigates the explanation of AI systems through narratives targeted to the needs of a specific audience, and our work focuses on the question of how and in which way human-model interaction can enable successful explanation.},
    month = {9},
    year = {2021},
    title = {Interaction with Explanations in the XAINES Project},
    booktitle = {Trustworthy AI in the wild},
    howpublished = {Trustworthy AI in the Wild Workshop 2021},
    publisher = {-},
    author = {Mareike Hartmann and Ivana Kruijff-Korbayová and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11867_AI_in_the_wild__Xaines.pdf}
}

@misc{pub11868,
    abstract = {New methods are constantly being developed to optimize and adapt cognitive load measurement to different contexts (Korbach et al., 2018). It is noteworthy, however, that research on cognitive load measurement in elementary school students is rare. Although there is some evidence that they might be able to report their total cognitive load (Ayres, 2006), there are also reasons to doubt the quality of children’s self-reports (e.g., Chambers & Johnson, 2002). To avoid these issues, behavioral and objective online-measures are promising. A novel approach – the use of smartpen data generated by natural use of a pen during task completion – seems particularly encouraging as these measures proved to be predictive of cognitive load in adults (e.g., Yu, Epps, & Chen, 2011). Moreover, Barz et al. (2020) demonstrated the predictive power of smartpen data for performance in children. The present research addressed two prevailing gaps in research on cognitive load assessment in elementary school students. We developed a subjective rating scale and investigated whether this instrument can provide valid measurements of ICL and ECL (Research Question 1). Moreover, we researched whether smartpen data can be used as a valid process measurement of cognitive load (Research Question 2).},
    year = {2021},
    title = {Measuring Intrisic and Extraneous Cognitive Load in Elementary School Students Using Subjective Ratings and Smart Pen Data},
    howpublished = {13th International Cognitive Load Theory  Conference},
    author = {Sarah Malone and Kristin Altmeyer and Michael Barz and Luisa Lauer and Daniel Sonntag and Markus Peschel and Roland Brünken},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11868_Cl_measurement_in_children.pdf}
}

@misc{pub11870,
    abstract = {In science education, hands-on student experiments are used to explore cause and effect relationships. Conventional lab work requires students to interact with physical experimentation objects and observe additional information like measurement values to deduce scientific laws and interrelations. The observable information, however, are usually presented in physical distance to the setting, e.g., on a separate display of a measuring device. The resulting spatial split (Chandler & Sweller, 1991) between representations hampers global coherence formation (Seufert & Brünken, 2004): Mapping processes between the spatially distant sources of information are assumed to lead to an increase in extraneous cognitive load (ECL; Ayres & Sweller, 2014). Consequently, learning outcomes can be impaired (Kalyuga et al., 1999).
Augmented Reality (AR) can be used to overcome the split-attention effect by allowing additional information to be virtually integrated into the real-world set-up (Azuma, 1997). A study by Altmeyer et al. (2020) with university students showed that AR-support during experimentation led to a higher conceptual knowledge gain but had no effect on ECL. The current study provides a conceptual replication of Altmeyer et al.’s (2020) research and focuses on three main objectives:
First, we aimed at investigating the generalizability of the advantage of AR on experimental learning in a sample of elementary school children. Second, we examined if low prior-knowledge of children even amplifies the split-attention effect, as proposed by Kalyuga et al. (1998). Finally, we focused on obtaining deeper insights into global coherence formation processes during lab work using specific tests and eye tracking measures.},
    year = {2021},
    title = {The effect of augmented reality on global coherence formation processes during STEM laboratory work in elementary school children},
    howpublished = {13th International Cognitive Load Theory Conference},
    author = {Kristin Altmeyer and Sarah Malone and Sebastian Kapp and Michael Barz and Luisa Lauer and Michael Thees and Jochen Kuhn and Markus Peschel and Daniel Sonntag and Roland Brünken},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11870_ICLTC_2021_Altmeyer_final.pdf}
}

@misc{pub11871,
    abstract = {Augmented Reality (AR) lässt sich als eine Form virtueller Umgebungen auf einem Realitäts-Virtualitäts-Kontinuum (Milgram & Kishino, 1994) der gemischten Realität zuordnen. AR erweitert die Realität durch die Integration virtueller Objekte. Ein vielversprechendes Anwendungsgebiet für AR im Bildungsbereich bietet das technologiegestützte Experimentieren:
Experimente bilden ein wesentliches Merkmal der Naturwissenschaften und werden im MINT-Unterricht eingesetzt, um Zusammenhänge zu untersuchen. Bisherige Forschung deutet darauf hin, dass bereits Kinder im Grundschulalter (natur)wissenschaftliches Denken und die Fähigkeit zum Experimentieren entwickeln können (z.B. Osterhaus et al., 2015). Um Ursache-Wirkung-Beziehungen aus einem Experiment abzuleiten, müssen Lernende meist reale Informationen der Experimentierumgebung mit virtuellen Informationen, wie z.B. Messwerten auf Messwertdisplays, mental verknüpfen. Im Sinne der Cognitive Theory of Multimedia Learning (Mayer, 2005) und der Cognitive Load Theory (Sweller et al., 1998) stellt die Verknüpfung räumlich getrennter Informationen eine besondere Herausforderung an das Arbeitsgedächtnis dar. AR kann dazu genutzt werden, reale und virtuelle Informationen beim Experimentieren integriert darzustellen. Vorausgehende Studienergebnisse (z.B. Altmeyer et al., 2020) implizieren, dass AR die globale Kohärenzbildung (Seufert & Brünken, 2004) unterstützt und zu besseren Lernergebnissen führen kann (Altmeyer et. al., 2020).
In der vorliegenden Studie wurde der Effekt von AR-Unterstützung beim Experimentieren in einer Stichprobe von Grundschulkindern untersucht. Nach einem Vorwissenstest führten 59 Kinder Experimente zu elektrischen Schaltkreisen durch. Einer Gruppe wurden Echzeit-Messwerte für die Stromstärke in einer Tabelle auf einem separaten Tabletbildschirm präsentiert. Dagegen sah die AR-unterstützte Gruppe die Messwerte beim Blick durch eine Tabletkamera in die Experimentierumgebung integriert. Während des Experimentierens wurden die Blickbewegungen der Kinder erfasst. Danach bearbeiteten beide Gruppen Posttests, welche in ihren Anforderungen an die globale Kohärenzbildung zwischen realen und virtuellen Elementen beim Experimentieren variierten. Erste Ergebnisse zeigen, dass Kinder insbesondere hinsichtlich Aufgaben, die eine starke globale Kohärenz erfordern, von der AR-Umgebung profitieren. Blickbewegungsanalysen sollen weitere Aufschlüsse über den Prozess der Kohärenzbildung während des Experimentierens in AR geben.},
    year = {2021},
    title = {Augmented Reality zur Förderung globaler Kohärenzbildungsprozesse beim Experimentieren im Sachunterricht},
    howpublished = {Tagung der Fachgruppe Pädagogische Psychologie},
    author = {Kristin Altmeyer and Sarah Malone and Sebastian Kapp and Michael Barz and Luisa Lauer and Michael Thees and Jochen Kuhn and Markus Peschel and Daniel Sonntag and Roland Brünken},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11871_v3_Altmeyer_VR_Symposium_PAEPSY_2021.pdf}
}

@inproceedings{pub11886,
    abstract = {We present a virtual reality (VR) application that enables us to interactively explore and manipulate image clusters based on layer activations of convolutional neural networks (CNNs). We apply dimensionality reduction techniques to project images into the 3D space, where the user can directly interact with the model. The user can change the position of an image by using natural hand gestures. This manipulation triggers additional training steps of the network, based on the new spatial information and new label of the image. After the training step is finished, the visualization is updated according to the new output of the CNN. The goal is to visualize and improve the cluster output of the model, and at the same time, to improve the understanding of the model. We discuss two different approaches for calculating the VR projection, a combined PCA/t-SNE dimensionality reduction based approach and a variational auto-encoder (VAE) based approach.},
    year = {2021},
    title = {A Demonstrator for Interactive Image Clustering and Fine-Tuning Neural Networks in Virtual Reality},
    booktitle = {KI 2021: Advances in Artificial Intelligence. German Conference on Artificial Intelligence (KI-2021), Germany},
    editor = {Stefan Edelkamp and Ralf Möller and Elmar Rueckert},
    pages = {194-203},
    isbn = {978-3-030-87626-5},
    publisher = {Springer International Publishing},
    author = {Alexander Prange and Daniel Sonntag},
    keywords = {Virtual Reality, Deep Learning, Convolutional Neural Network, Variational Auto-Encoder, PCA, t-SNE},
    url = {https://link.springer.com/chapter/10.1007/978-3-030-87626-5_14}
}

@inproceedings{pub11927,
    abstract = {In the DESIGNETZ project real flexibility units were connected to a distribution grid simulation to investigate the integration of decentralized flexibilities for different use-cases. The simulation determines the demand for unit flexibility and communicates the demand to the flexibilities. In return, the response of the flexibilities is integrated back into the simulation to consider not-simulated effects, too. This paper presents the simulation setup and discusses lessons learnt from deploying the simulation into operation.},
    month = {10},
    year = {2021},
    title = {Live Testing of Flexibilities on Distribution Grid Level – Simulation Setup and Lessons Learned},
    booktitle = {IEEE Electric Power and Energy Conference. IEEE Electric Power and Energy Conference (EPEC-2021), October 22-31, Toronto,, Ontario, Canada},
    address = {IEEE Operations Center
445 Hoes Lane
Piscataway, NJ 08854-4141 USA
Phone: +1 732 981 0060},
    publisher = {IEEE Xplore},
    author = {Fabian Erlemeyer and Christian Rehtanz and Annegret Hermanns and Bengt Lüers and Marvin Nebel-Wenner and Reef Janes Eilers},
    keywords = {flexibility, real-world application, active distribution grids, congestion management, energy system simulation},
    organization = {IEEE},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11927_2021199998.pdf https://ieeexplore.ieee.org/document/9621559}
}

@inproceedings{pub11981,
    abstract = {We present the multisensor-pipeline (MSP), a lightweight, flexible, and extensible framework for prototyping multimodal-multisensor
interfaces based on real-time sensor input. Our open-source framework (available on GitHub) enables researchers and developers
to easily integrate multiple sensors or other data streams via source modules, to add stream and event processing capabilities via
processor modules, and to connect user interfaces or databases via sink modules in a graph-based processing pipeline. Our framework
is implemented in Python with a low number of dependencies, which enables a quick setup process, execution across multiple operating
systems, and direct access to cutting-edge machine learning libraries and models. We showcase the functionality and capabilities of
MSP through a sample application that connects a mobile eye tracker to classify image patches surrounding the user’s fixation points
and visualizes the classification results in real-time.},
    year = {2021},
    title = {Multisensor-Pipeline: A Lightweight, Flexible, and Extensible Framework for Building Multimodal-Multisensor Interfaces},
    booktitle = {Companion Publication of the 2021 International Conference on Multimodal Interaction. ACM International Conference on Multimodal Interaction (ICMI-2021), October 18-22, Montréal,, QC, Canada},
    pages = {13-18},
    isbn = {9781450384711},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {https://doi.org/10.1145/3461615.3485432},
    author = {Michael Barz and Omair Shahzad Bhatti and Bengt Lüers and Alexander Prange and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11981_icmi_cr.pdf https://dl.acm.org/doi/10.1145/3461615.3485432}
}

@techreport{pub10809,
    series = {DFKI Technical Memos, TM},
    abstract = {Künstliche Intelligenz hat in den letzten Jahren eine neue Reifephase erreicht und entwickelt sich zum Treiber der Digitalisierung in allen Lebensbereichen. Die KI ist eine Querschnittstechnologie, die für alle Bereiche der Medizin mit Bilddaten, Textdaten und Biodaten von großer Bedeutung ist. Es gibt keinen medizinischen Bereich, der nicht von KI beeinflusst werden wird (siehe auch http://www.dfki.de/ MedicalCPS/?p=1111).
Hier werden vier Felder gegen das Coronavirus beleuchtet, (1) die Bilddiagnostik, (2) Gensequenzierung, (3) die automatische Auswertung medizinischer Texte und (4) das Katastrophenmanagement.},
    year = {2020},
    title = {Künstliche Intelligenz gegen das Coronavirus},
    volume = {1},
    institution = {DFKI, BMBF, BMG},
    author = {Daniel Sonntag},
    keywords = {Bilddiagnostik, Gensequenzierung, NLP, Katastrophenmanagement},
    url = {https://www.dfki.de/fileadmin/user_upload/import/10809_corona2.pages.pdf}
}

@inproceedings{pub10893,
    abstract = {Visual search is a perceptual task in which humans aim at identifying a search target object such as a traffic sign among other objects. Search target inference subsumes computational methods for predicting this target by tracking and analyzing overt behavioral cues of that person, e.g., the human gaze and fixated visual stimuli. We present a generic approach to inferring search targets in natural scenes by predicting the class of the surrounding image segment. Our method encodes visual search sequences as histograms of fixated segment classes determined by SegNet, a deep learning image segmentation model for natural scenes. We compare our sequence encoding and model training (SVM) to a recent baseline from the literature for predicting the target segment. Also, we use a new search target inference dataset. The results show that, first, our new segmentation-based sequence encoding outperforms the method from the literature, and second, that it enables target inference in natural settings.},
    month = {5},
    year = {2020},
    title = {Visual Search Target Inference in Natural Interaction Settings with Machine Learning},
    booktitle = {ACM Symposium on Eye Tracking Research and Applications. Symposium on Eye Tracking Research & Applications (ETRA-2020), Stuttgart, Germany},
    editor = {Andreas Bulling and Anke Huckauf and Eakta Jain and Ralph Radach and Daniel Weiskopf},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {https://doi.org/10.1145/3379155.3391314},
    author = {Michael Barz and Sven Stauden and Daniel Sonntag},
    keywords = {Machine Learning, Search Target Inference, Mobile Eyetracking, Visual Attention},
    url = {https://dl.acm.org/doi/10.1145/3379155.3391314}
}

@inproceedings{pub10894,
    abstract = {Digital pen signals were shown to be predictive for cognitive states, cognitive load and emotion in educational settings. We investigate whether low-level pen-based features can predict the difficulty of tasks in a cognitive test and the learner's performance in these tasks, which is inherently related to cognitive load, without a semantic content analysis. We record data for tasks of varying difficulty in a controlled study with children from elementary school. We include two versions of the Trail Making Test (TMT) and six drawing patterns from the Snijders-Oomen Non-verbal intelligence test (SON) as tasks that feature increasing levels of difficulty. We examine how accurately we can predict the task difficulty and the user performance as a measure for cognitive load using support vector machines and gradient boosted decision trees with different feature selection strategies. The results show that our correlation-based feature selection is beneficial for model training, in particular when samples from TMT and SON are concatenated for joint modelling of difficulty and time. Our findings open up opportunities for technology-enhanced adaptive learning.},
    month = {7},
    year = {2020},
    title = {Digital Pen Features Predict Task Difficulty and User Performance of Cognitive Tests},
    booktitle = {Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization. International Conference on User Modeling, Adaptation, and Personalization (UMAP-2020), July 12-18, Genoa, Italy},
    publisher = {ACM},
    author = {Michael Barz and Kristin Altmeyer and Sarah Malone and Luisa Lauer and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/10894_digital_pen_predicts_task_performance.pdf https://dl.acm.org/doi/abs/10.1145/3340631.3394839}
}

@techreport{pub10912,
    series = {DFKI Research Reports, RR},
    abstract = {A shortage of dermatologists causes long wait times for patients who seek dermatologic care. In addition, the diagnostic accuracy of general practitioners has been reported to be lower than the accuracy of artificial intelligence software. This article describes the Skincare project (H2020, EIT Digital). Contributions include enabling technology for clinical decision support based on interactive machine learning (IML), a reference architecture towards a Digital European Healthcare Infrastructure (also cf. EIT MCPS), technical components for aggregating digitised patient information, and the integration of decision support technology into clinical test-bed environments. However, the main contribution is a diagnostic and decision support system in dermatology for patients and doctors, an interactive deep learning system for differential diagnosis of malignant skin lesions. In this article, we describe its functionalities and the user interfaces to facilitate machine learning from human input. The baseline deep learning system, which delivers state-of-the-art results and the potential to augment general practitioners and even dermatologists, was developed and validated using de-identified cases from a dermatology image data base (ISIC), which has about 20000 cases for development and validation, provided by board-certified dermatologists defining the reference standard for every case. ISIC allows for differential diagnosis, a ranked list of eight diagnoses, that is used to plan treatments in the common setting of diagnostic ambiguity. We give an overall description of the outcome of the Skincare project, and we focus on the steps to support communication and coordination between humans and machine in IML. This is an integral part of the development of future cognitive assistants in the medical domain, and we describe the necessary intelligent user interfaces.},
    month = {5},
    year = {2020},
    title = {The Skincare project, an interactive deep learning system for differential diagnosis of malignant skin lesions.},
    type = {Technical Report},
    volume = {1},
    address = {Bundesministerium für Bildung und Forschung
Kapelle-Ufer 1
D-10117 Berlin},
    institution = {BMBF, H2020},
    author = {Daniel Sonntag and Fabrizio Nunnari and Hans-Jürgen Profitlich},
    keywords = {Machine learning, decision support, dermatology, skin cancer},
    url = {https://arxiv.org/abs/2005.09448 https://www.dfki.de/fileadmin/user_upload/import/10912_main2.pdf}
}

@inproceedings{pub11113,
    abstract = {We present a study on the fusion of pixel data and patient metadata (age, gender, and body location) for improving the classification of skin lesion images. The experiments have been conducted with the ISIC 2019 skin lesion classification challenge data set. Taking two plain convolutional neural networks (CNNs) as a baseline, metadata are merged using either non-neural machine learning methods (tree-based and support vector machines) or shallow neural networks. Results show that shallow neural networks outperform other approaches in all overall evaluation measures. However, despite the increase in the classification accuracy (up to +19.1%), interestingly, the average per-class sensitivity decreases in three out of four cases for CNNs, thus suggesting that using metadata penalizes the prediction accuracy for lower represented classes. A study on the patient metadata shows that age is the most useful metadatum as a decision criterion, followed by body location and gender.},
    year = {2020},
    title = {A Study on the Fusion of Pixels and Patient Metadata in CNN-Based Classification of Skin Lesion Images},
    booktitle = {Machine Learning and Knowledge Extraction. International IFIP Cross Domain (CD) Conference for Machine Learning & Knowledge Extraction (MAKE) (CD-MAKE-2020), August 25-28, Dublin, Ireland},
    editor = {Andreas Holzinger and Peter Kieseberg and A Min Tjoa and Edgar Weippl},
    pages = {191-208},
    isbn = {978-3-030-57321-8},
    publisher = {Springer International Publishing},
    doi = {10.1007/978-3-030-57321-8_11},
    author = {Fabrizio Nunnari and Chirag Bhuvaneshwara and Abraham Obinwanne Ezema and Daniel Sonntag},
    url = {https://link.springer.com/chapter/10.1007/978-3-030-57321-8_11 https://www.dfki.de/fileadmin/user_upload/import/11113_Nunnari20CD-MAKE.pdf}
}

@inproceedings{pub11178,
    series = {Lecture Notes in Computer Science, LNCS},
    abstract = {In this work, we propose a new approach to automatically predict the locations of visual dermoscopic attributes for Task 2 of the ISIC 2018 Challenge. Our method is based on the Attention U-Net with multi-scale images as input. We apply a new strategy based on transfer learning, i.e., training the deep network for feature extraction by adapting the weights of the network trained for segmentation. Our tests show that, first, the proposed algorithm is on par or outperforms the best ISIC 2018 architectures (LeHealth and NMN) in the extraction of two visual features. Secondly, it uses only 1/30 of the training parameters; we observed less computation and memory requirements, which are particularly useful for future implementations on mobile devices. Finally, our approach generates visually explainable behaviour with uncertainty estimations to help doctors in diagnosis and treatment decisions.},
    month = {9},
    year = {2020},
    title = {A Visually Explainable Learning System for Skin Lesion Detection Using Multiscale Input with Attention U-Net},
    booktitle = {KI 2020: Advances in Artificial Intelligence. German Conference on Artificial Intelligence (KI-2020), 43rd, September 21-25, Bamberg, Germany},
    volume = {12325},
    pages = {313-319},
    publisher = {Springer},
    author = {Ho Minh Duy Nguyen and Abraham Ezema and Fabrizio Nunnari and Daniel Sonntag},
    keywords = {Skin lesion Diagnose features Attention U-Net},
    url = {https://link.springer.com/chapter/10.1007/978-3-030-58285-2_28 https://www.dfki.de/fileadmin/user_upload/import/11178_KI_2020.pdf}
}

@techreport{pub11188,
    series = {DFKI Documents, D},
    abstract = {The aim of ImageCLEFmed Caption task is to develop a system that automatically labels radiology images with relevant medical concepts. We describe our Deep Neural Network (DNN) based approach for tackling this problem. On the challenge test set of 3,534 radiology images, our system achieves an F1 score of 0.375 and ranks high, 12th among all systems that were successfully submitted to the challenge, whereby we only rely on the provided data sources and do not use any external medical knowledge or ontologies, or pretrained models from other medical image repositories or application domains.},
    year = {2020},
    title = {A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption 2020 Task},
    volume = {o.A.},
    institution = {German Research Center for Artificial Intelligence},
    author = {Marimuthu Kalimuthu and Fabrizio Nunnari and Daniel Sonntag},
    url = {https://arxiv.org/pdf/2007.14226v1 https://www.dfki.de/fileadmin/user_upload/import/11188_A_Competitive_Deep_Neural_Network_Approach.pdf}
}

@article{pub11236,
    abstract = {Image captioning is a challenging multimodal task. Significant improvements could be obtained by deep learning. Yet, captions generated by humans are still considered better, which makes it an interesting application for interactive machine learning and explainable artificial intelligence methods. In this work, we aim at improving the performance and explainability of the state-of-the-art method Show, Attend and Tell by augmenting their attention mechanism using additional bottom-up features. We compute visual attention on the joint embedding space formed by the union of high-level features and the low-level features obtained from the object specific salient regions of the input image. We embed the content of bounding boxes from a pre-trained Mask R-CNN model. This delivers state-of-the-art performance, while it provides explanatory features. Further, we discuss how interactive model improvement can be realized through re-ranking caption candidates using beam search decoders and explanatory features. We show that interactive re-ranking of beam search candidates has the potential to outperform the state-of-the-art in image captioning.},
    month = {7},
    year = {2020},
    title = {Towards Explanatory Interactive Image Captioning Using Top-Down and Bottom-Up Features, Beam Search and Re-ranking},
    journal = {KI - Künstliche Intelligenz, German Journal on Artificial Intelligence - Organ des Fachbereiches "Künstliche Intelligenz" der Gesellschaft für Informatik e.V. (KI)},
    volume = {36},
    pages = {1-14},
    publisher = {Springer},
    doi = {10.1007/s13218-020-00679-2},
    author = {Rajarshi Biswas and Michael Barz and Daniel Sonntag},
    keywords = {Image Captioning, Deep Learning, Explainable AI, Visual Explanations, Interactive Machine Learning, Beam Search, Re-ranking},
    url = {https://doi.org/10.1007/s13218-020-00679-2 https://www.dfki.de/fileadmin/user_upload/import/11236_2021_TOWARDS_EXPLANATORY_INTERACTIVE_IMAGE_CAPTIONING_USING_TOP-DOWN_AND_BOTTOM-UP_FEATURES,_BEAM_SEARCH_AND_RE-RANKING.pdf}
}

@inproceedings{pub11368,
    abstract = {The classification of skin lesion images is known to be biased by artifacts of the surrounding skin, but it is still not clear to what extent masking out healthy skin pixels influences classification performances, and why. To better understand this phenomenon, we apply different strategies of image masking (rectangular masks, circular masks, full masking, and image cropping) to three datasets of skin lesion images (ISIC2016, ISIC2018, and MedNode). We train CNN-based classifiers, provide performance metrics through a 10-fold cross-validation, and analyse the behaviour of Grad-CAM saliency maps through an automated visual inspection. Our experiments show that cropping is the best strategy to maintain classification performance and to significantly re- duce training times as well. Our analysis through visual inspection shows that CNNs have the tendency to focus on pixels of healthy skin when no malignant features can be identified. This suggests that CNNs have the tendency of "eagerly" looking for pixel areas to justify a classification choice, potentially leading to biased discriminators. To mitigate this effect, and to standardize image preprocessing, we suggest to crop images during dataset construction or before the learning step.},
    month = {12},
    year = {2020},
    title = {The effects of masking in melanoma image classification with CNNs towards international standards for image preprocessing},
    booktitle = {2020 EAI International Symposium on Medical Artificial Intelligence. EAI International Symposium on Medical Artificial Intelligence (MedAI-2020), December 18, Online-Conference},
    note = {Virtual Conference},
    publisher = {EAI},
    author = {Fabrizio Nunnari and Abraham Ezema and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11368_2020_EAI_MedAI_StudyOnDatasetBias.pdf https://link.springer.com/chapter/10.1007/978-3-030-70569-5_16}
}

@article{pub11374,
    abstract = {Many digitalized cognitive assessments exist to increase reliability, standardization, and objectivity. Particularly in older adults, the performance of digitized cognitive assessments can lead to poorer test results if they are unfamiliar with the computer, mouse, keyboard, or touch screen. In a cross-over design study, 40 older adults (age M = 74.4 ± 4.1 years) conducted the Trail Making Test A and B with a digital pen (digital pen tests, DPT) and a regular pencil (pencil tests, PT) to identify differences in performance. Furthermore, the tests conducted with a digital pen were analyzed manually (manual results, MR) and electronically (electronic results, ER) by an automized system algorithm to determine the possibilities of digital pen evaluation. ICC(2,k) showed a good level of agreement for TMT A (ICC(2,k) = 0.668) and TMT B (ICC(2,k) = 0.734) between PT and DPT. When comparing MR and ER, ICC(2,k) showed an excellent level of agreement in TMT A (ICC(2,k) = 0.999) and TMT B (ICC(2,k) = 0.994). The frequency of pen lifting correlates significantly with the execution time in TMT A (r = 0.372, p = 0.030) and TMT B (r = 0.567, p < 0.001). A digital pen can be used to perform the Trail Making Test, as it has been shown that there is no difference in the results due to the type of pen used. With a digital pen, the advantages of digitized testing can be used without having to accept the disadvantages.},
    year = {2020},
    title = {Digital pen technology for conducting cognitive assessments: a cross-over study with older adults},
    journal = {Psychological Research},
    volume = {85},
    pages = {1-9},
    publisher = {Springer},
    author = {A. Heimann-Steinert and A. Latendorf and Alexander  Prange and Daniel Sonntag and U. Müller-Werdan},
    keywords = {digital pen, machine learning, medicine, dementia, cognitive assessments},
    url = {https://link.springer.com/article/10.1007/s00426-020-01452-8#citeas https://www.dfki.de/fileadmin/user_upload/import/11374_Heimann-Steinert-2020-DigitalPenTechnologyForConduct.pdf}
}

@inproceedings{pub12112,
    abstract = {We present Game of TUK, a gamified mobile app to increase physical activity among students at TU Kaiserslautern. The scale of our project with almost 2,000 players over the course of four weeks is unique for a project in a university context. We present feedback we received and share our insights. Our results show that location-based activities in particular were very popular. In contrast, mini-games included in the app did not contribute as much to user activity as expected.},
    year = {2020},
    title = {Game of TUK: deploying a large-scale activity-boosting gamification project in a university context},
    booktitle = {Mensch und Computer. Mensch und Computer (MuC-2020)},
    publisher = {ACM},
    author = {Julia Müller and Max Sprenger and Tobias Franke and Paul Lukowicz and Claudia Reidick and Marc Herrlich},
    url = {https://dl.acm.org/doi/abs/10.1145/3404983.3410008 https://www.dfki.de/fileadmin/user_upload/import/12112_2020_GAME_OF_TUK-_DEPLOYING_A_LARGE-SCALE_ACTIVITY-BOOSTING_GAMIFICATION_PROJECT_IN_A_UNIVERSITY_CONTEXT.pdf}
}

@inbook{pub10812,
    abstract = {In this chapter, we discuss the trends of mutlimodal-multisensor interfaces for medical and health systems. We emphasize the theoretical foundations of multimodal interfaces and systems in the healthcare domain. We aim to provide a basis for motivating and accelerating future interfaces for medical and health systems. Therefore, we provide many examples of existing and futuristic systems. For each of these systems, we define a classification into clinical systems and non-clinical systems, as well as sub-classes of multimodal and multisensor interfaces, to help structure the recent work in this emerging research field of medical and health systems.},
    year = {2019},
    title = {Medical and Health Systems},
    booktitle = {Sharon Oviatt; Björn W Schuller; Philip R Cohen; Daniel Sonntag; Gerasimos Potamianos; Antonio Krüger;: The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions - Volume 3},
    pages = {423-476},
    isbn = {9781970001754},
    publisher = {Association for Computing Machinery and Morgan & Claypool},
    author = {Daniel Sonntag},
    url = {https://doi.org/10.1145/3233795.3233808 https://www.dfki.de/fileadmin/user_upload/import/10812_Medical-and-Health-Systems.pdf}
}

@article{pub10833,
    abstract = {Künstliche Intelligenz (KI) hat in den letzten Jahren eine neue Reifephase erreicht und entwickelt sich zum Treiber der Digitalisierung in allen Lebensbereichen. Die KI ist eine Querschnittstechnologie, die für alle Bereiche der Medizin mit Bilddaten, Textdaten und Biodaten von großer Bedeutung ist. Es gibt keinen medizinischen Bereich, der nicht von KI beeinflusst werden wird. Dabei spielt die klinische Entscheidungsunterstützung eine wichtige Rolle. Gerade beim medizinischen Workflow-Management und bei der Vorhersage des Behandlungserfolgs bzw. Behandlungsergebnisses etablieren sich KI-Methoden. In der Bilddiagnose und im Patientenmanagement können KI-Systeme bereits unterstützen, aber sie können keine kritischen Entscheidungen vorschlagen. Die jeweiligen Präventions- oder Therapiemaßnahmen können mit KI-Unterstützung sinnvoller bewertet werden, allerdings ist die Abdeckung der Krankheiten noch viel zu gering, um robuste Systeme für den klinischen Alltag zu erstellen. Der flächendeckende Einsatz setzt Fortbildungsmaßnahmen für Ärzte voraus, um die Entscheidung treffen zu können, wann auf automatische Entscheidungsunterstützung vertraut werden kann.},
    number = {5},
    year = {2019},
    title = {Künstliche Intelligenz in der Medizin -- Holzweg oder Heilversprechen?},
    journal = {HNO},
    volume = {67},
    pages = {343-349},
    publisher = {Springer},
    doi = {10.1007/s00106-019-0665-z},
    author = {Daniel Sonntag},
    url = {https://doi.org/10.1007/s00106-019-0665-z https://www.dfki.de/fileadmin/user_upload/import/10833_sonntag-hno-ki-DFKI-repository.pdf}
}

@article{pub10895,
    abstract = {We implement a method for re-ranking top-10 results of a state-of-the-art question answering (QA) system. The goal of our re-ranking approach is to improve the answer selection given the user question and the top-10 candidates. We focus on improving deployed QA systems that do not allow re-training or re-training comes at a high cost. Our re-ranking approach learns a similarity function using n-gram based features using the query, the answer and the initial system confidence as input. Our contributions are: (1) we generate a QA training corpus starting from 877 answers from the customer care domain of T-Mobile Austria, (2) we implement a state-of-the-art QA pipeline using neural sentence embeddings that encode queries in the same space than the answer index, and (3) we evaluate the QA pipeline and our re-ranking approach using a separately provided test set. The test set can be considered to be available after deployment of the system, e.g., based on feedback of users. Our results show that the system performance, in terms of top-n accuracy and the mean reciprocal rank, benefits from re-ranking using gradient boosted regression trees. On average, the mean reciprocal rank improves by 9.15%.},
    month = {8},
    year = {2019},
    title = {Incremental Improvement of a Question Answering System by Re-ranking Answer Candidates using Machine Learning},
    journal = {Computing Research Repository eprint Journal (CoRR)},
    volume = {abs/1908.10149},
    pages = {1-13},
    publisher = {arXiv},
    author = {Michael Barz and Daniel Sonntag},
    url = {https://arxiv.org/abs/1908.10149 https://www.dfki.de/fileadmin/user_upload/import/10895_1908.10149.pdf}
}

@techreport{pub11177,
    series = {DFKI Research Reports, RR},
    abstract = {Scoring systems are linear classification models that only require users to add or subtract a few small numbers in order to make a prediction. They are used for example by clinicians to assess the risk of medical conditions. This work focuses on our approach to implement an intuitive user interface to allow a clinician to generate such scoring systems interactively, based on the RiskSLIM machine learning library. We describe the technical architecture which allows a medical professional who is not specialised in developing and applying machine learning algorithms to create competitive transparent supersparse linear integer models in an interactive way. We demonstrate our prototype machine learning system in the nephrology domain, where doctors can interactively sub-select datasets to compute models, explore scoring tables that correspond to the learned models, and check the quality of the transparent solutions from a medical perspective.},
    year = {2019},
    title = {Interactivity and Transparency in Medical Risk Assessment with Supersparse Linear Integer Models},
    journal = {CoRR},
    volume = {abs/1911.12119},
    institution = {BMBF},
    publisher = {ArXiv},
    author = {Hans-Jürgen Profitlich and Daniel Sonntag},
    url = {http://arxiv.org/abs/1911.12119 https://www.dfki.de/fileadmin/user_upload/import/11177_integer.pdf}
}

@inproceedings{pub10896,
    abstract = {Visual Search target inference subsumes methods for predicting the target object through eye tracking. A person intents to find an object in a visual scene which we predict based on the fixation behavior. Knowing about the search target can improve intelligent user interaction. In this work, we implement a new feature encoding, the Bag of Deep Visual Words, for search target inference using a pre-trained convolutional neural network (CNN). Our work is based on a recent approach from the literature that uses Bag of Visual Words, common in computer vision applications. We evaluate our method using a gold standard dataset.
The results show that our new feature encoding outperforms the baseline from the literature, in particular, when excluding fixations on the target.},
    month = {8},
    year = {2018},
    title = {Visual Search Target Inference Using Bag of Deep Visual Words},
    booktitle = {KI 2018: Advances in Artificial Intelligence - 41st German Conference on AI. German Conference on Artificial Intelligence (KI-2018), September 24-28, Berlin, Germany},
    editor = {Frank Trollmann and Anni-Yasmin Turhan},
    publisher = {Springer},
    doi = {https://doi.org/10.1007/978-3-030-00111-7_25},
    author = {Sven Stauden and Michael Barz and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/10896_2018_Visual_Search_Target_Inference_Using_Bag_of_Deep_Visual_Words.pdf https://link.springer.com/content/pdf/10.1007/978-3-030-00111-7_25.pdf}
}

@article{pub11491,
    abstract = {This article presents our steps to integrate complex and partly unstructured medical data into a clinical research database with subsequent decision support. Our main application is an integrated faceted search tool, accompanied by the visualisation of results of automatic information extraction from textual documents. We describe the details of our technical architecture (open-source tools), to be replicated at other universities, research institutes, or hospitals. Our exemplary use cases are nephrology and mammography. The software was first developed in the nephrology domain and then adapted to the mammography use case. We report on these case studies, illustrating how the application can be used by a clinician and which questions can be answered. We show that our architecture and the employed software modules are suitable for both areas of application with a limited amount of adaptations. For example, in nephrology we try to answer questions about the temporal characteristics of event sequences to gain significant insight from the data for cohort selection. We present a versatile time-line tool that enables the user to explore relations between a multitude of diagnosis and laboratory values.},
    year = {2018},
    title = {An architecture of open-source tools to combine textual information extraction, faceted search and information visualisation},
    journal = {Computing Research Repository eprint Journal (CoRR)},
    volume = {abs/1810.12627},
    pages = {13-28},
    publisher = {Elsevier},
    author = {Daniel Sonntag and Hans-Jürgen Profitlich},
    keywords = {clinical decision support; information extraction; natural language processing; medical data analysis; data management; faceted search; human-computer interaction; visualisation; electronic health record;},
    url = {http://arxiv.org/abs/1810.12627 https://www.dfki.de/fileadmin/user_upload/import/11491_1810.12627.pdf}
}

@inproceedings{pub11235,
    abstract = {In order to improve medical processes in nephrology, we present an application that allows doctors to create biopsy protocols by using a digital pen on a tablet. The biopsy protocol app is seamlessly integrated into the existing infrastructure at the hospital (see figure 1). Compared to other reporting tools, we provide (1) real-time hand-writing/gesture recognition and real-time feedback on the recognition results on the screen; (2) a real-time digitisation into structured data and PDF documents; and (3) the mapping of the transcribed contents into concepts of the Banff classification. Our approach combines the benefits of paper with the automatic digitisation and digitalisation of hand-written user input. A fully digital and mobile approach should empower nephrologists to produce high quality data more effectively and in real-time so that it can be directly used in hospital processes.},
    month = {6},
    year = {2017},
    title = {A Digital Pen Based Tool for Instant Digitisation and Digitalisation of Biopsy Protocols},
    booktitle = {2017 IEEE 30th International Symposium on Computer-Based Medical Systems (CBMS). IEEE International Symposium on Computer-Based Medical Systems (CBMS-2017)},
    pages = {773-774},
    publisher = {IEEE Xplore},
    doi = {10.1109/CBMS.2017.132},
    author = {Alexander Prange and Danilo Schmidt and Daniel Sonntag},
    keywords = {gesture recognition, health care, interactive devices, medical computing, medical information systems, mobile computing, mobile approach, hand-written user input, automatic digitisation, real-time digitisation, real-time feedback, real-time hand-writing/g}
}

@misc{pub11489,
    abstract = {Cognitive assistance may be valuable in applications for doctors and therapists that reduce costs and improve quality in healthcare systems. Use cases and scenarios include the assessment of dementia. In this paper, we present our approach to the (semi-)automatic assessment of dementia.},
    year = {2017},
    title = {Interakt - A Multimodal Multisensory Interactive Cognitive Assessment Tool},
    volume = {abs/1709.01796},
    pages = {4},
    author = {Daniel Sonntag},
    url = {http://arxiv.org/abs/1709.01796 https://www.dfki.de/fileadmin/user_upload/import/11489_1709.01796.pdf}
}

@inproceedings{pub11492,
    abstract = {This work focusses on our integration steps of complex and partly unstructured medical data into a clinical research database with subsequent decision support. Our main application is an integrated facetted search tool, followed by information visualisation based on automatic information extraction results from textual documents. We describe the details of our technical architecture (open-source tools), to be replicated at other universities, research institutes, or hospitals. Our exemplary use case is nephrology, where we try to answer questions about the temporal characteristics of sequences and gain significant insight from the data for cohort selection. We report on this case study, illustrating how the application can be used by a clinician and which questions can be answered.},
    year = {2017},
    title = {Integrated Decision Support by Combining Textual Information Extraction, Facetted Search and Information Visualisation},
    booktitle = {2017 IEEE 30th International Symposium on Computer-Based Medical Systems (CBMS). IEEE International Symposium on Computer-Based Medical Systems (CBMS-2017), June 22-24, Thessaloniki, Greece},
    pages = {95-100},
    publisher = {IEEE},
    doi = {10.1109/CBMS.2017.119},
    author = {Daniel Sonntag and Hans-Jürgen Profitlich},
    url = {https://ieeexplore.ieee.org/document/8104164}
}

@article{pub11493,
    abstract = {This work focuses on the integration of multifaceted extensive data sets (e.g. laboratory values, vital data, medications) and partly unstructured medical data such as discharge letters, diagnostic reports, clinical notes etc. in a research database. Our main application is an integrated faceted search in nephrology based on information extraction results. We describe the details of the application of transplant medicine and the resulting technical architecture of the faceted search application.},
    year = {2017},
    title = {A novel tool for the identification of correlations in medical data by faceted search},
    journal = {Computers in Biology and Medicine - An International Journal},
    volume = {85},
    pages = {98-105},
    publisher = {Elsevier},
    doi = {https://doi.org/10.1016/j.compbiomed.2017.04.011},
    author = {Danilo Schmidt and Klemens Budde and Daniel Sonntag and Hans-Jürgen Profitlich and Matthias Ihle and Oliver Staeck},
    keywords = {Faceted search, Knowledge based systems, Medical domain, Decision support},
    url = {https://www.sciencedirect.com/science/article/pii/S0010482517300975}
}

@inproceedings{pub11234,
    abstract = {In order to improve reporting practices for the detection of prostate cancer, we present an application that allows urologists to create structured reports by using a digital pen on a smartphone. In this domain, printed documents cannot be easily replaced by computer systems because they contain free-form sketches and textual annotations, and the acceptance of traditional PC reporting tools is rather low among the doctors. Our approach provides an instant knowledge acquisition system by automatically interpreting the written strokes, texts, and sketches. We have incorporated this structured reporting system for MRI of the prostate (PI-RADS). Our system imposes only minimal overhead on traditional form-filling processes and provides for a direct, ontology-based structuring of the user input for semantic search and retrieval applications.},
    month = {6},
    year = {2016},
    title = {Digital PI-RADS: Smartphone Sketches for Instant Knowledge Acquisition in Prostate Cancer Detection},
    booktitle = {2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS). IEEE International Symposium on Computer-Based Medical Systems (CBMS-2016)},
    pages = {13-18},
    publisher = {IEEE Xplore},
    doi = {10.1109/CBMS.2016.23},
    author = {Alexander Prange and Daniel Sonntag},
    keywords = {biomedical MRI, cancer, information retrieval, knowledge acquisition, medical image processing, ontologies (artificial intelligence), smart phones, digital PI-RADS, smartphone, prostate cancer detection, structured reports, digital pen, printed documents,}
}

@article{pub11490,
    abstract = {This article is about a new project that combines clinical data intelligence and smart data. It provides an introduction to the “Klinische Datenintelligenz” (KDI) project which is founded by the Federal Ministry for Economic Affairs and Energy (BMWi); we transfer research and development results (R&D) of the analysis of data which are generated in the clinical routine in specific medical domain. We present the project structure and goals, how patient care should be improved, and the joint efforts of data and knowledge engineering, information extraction (from textual and other unstructured data), statistical machine learning, decision support, and their integration into special use cases moving towards individualised medicine. In particular, we describe some details of our medical use cases and cooperation with two major German university hospitals.},
    number = {4},
    year = {2016},
    title = {The Clinical Data Intelligence Project - A smart data initiative},
    journal = {Informatik Spektrum},
    volume = {39},
    pages = {290-300},
    publisher = {Springer},
    doi = {https://doi.org/10.1007/s00287-015-0913-x},
    author = {Daniel Sonntag and Volker Tresp and Sonja Zillner and Alexander Cavallaro and Matthias Hammon and André Reis and Peter A. Fasching and Martin Sedlmayr and Thomas Ganslandt and Hans-Ulrich Prokosch and Klemens Budde and Danilo Schmidt and Carl Hinrichs and Thomas Wittenberg and Philipp Daumke and Patricia G. Oppelt},
    url = {https://doi.org/10.1007/s00287-015-0913-x https://www.dfki.de/fileadmin/user_upload/import/11490_The_Clinical_Data_Intelligence_Project_-_A_smart_data_initiative.pdf}
}

@inproceedings{pub11231,
    series = {Communications in Computer and Information Science},
    abstract = {Smartwatches are becoming increasingly sophisticated and popular as several major smartphone manufacturers, including Apple, have released their new models recently. We believe that these devices can serve as smart objects for people suffering from mental disorders such as memory loss. In this paper, we describe how to utilise smartwatches to create intelligent user interfaces that can be used to provide cognitive assistance in daily life situations of dementia patients. By using automatic speech recognisers and text-to-speech synthesis, we create a dialogue application that allows patients to interact through natural language. We compare several available libraries for Android and show an example of integrating a smartwatch application into an existing healthcare infrastructure.},
    year = {2015},
    title = {Easy Deployment of Spoken Dialogue Technology on Smartwatches for Mental Healthcare},
    booktitle = {Pervasive Computing Paradigms for Mental Health - 5th International Conference, MindCare 2015, Milan, Italy, September 24-25, 2015, Revised Selected Papers. International Symposium on Pervasive Computing Paradigms for Mental Health (MindCare-2015)},
    volume = {604},
    pages = {150-156},
    publisher = {Springer},
    doi = {https://doi.org/10.1007/978-3-319-32270-4_15},
    author = {Alexander Prange and Daniel Sonntag},
    keywords = {Smartwatch, Speech dialogue, Text-to-speech, Automatic speech recognition, Mental health},
    url = {https://doi.org/10.1007/978-3-319-32270-4_15 https://www.dfki.de/fileadmin/user_upload/import/11231_Easy_Deployment_of_Spoken_Dialogue_Technology_on_Smartwatches_for_Mental_Healthcare.pdf}
}

@inproceedings{pub11232,
    series = {IUI Companion '15},
    abstract = {In this demo paper we describe how a digital pen and a humanoid robot companion can improve the social communication of a dementia patient. We propose the use of NAO, a humanoid robot, as a companion to the dementia patient in order to continuously monitor his or her activities and provide cognitive assistance in daily life situations. For example, patients can communicate with NAO through natural language by the speech dialogue functionality we integrated. Most importantly, to improve communication, i.e., sending digital messages (texting, emails), we propose the usage of a smartpen, where the patients write messages on normal paper with an invisible dot pattern to initiate hand-writing and sketch recognition in real-time. The smartpen application is embedded into the human-robot speech dialogue.},
    year = {2015},
    title = {Robot Companions and Smartpens for Improved Social Communication of Dementia Patients},
    booktitle = {Proceedings of the 20th International Conference on Intelligent User Interfaces Companion. International Conference on Intelligent User Interfaces (IUI-2015), New York, NY, USA},
    isbn = {9781450333085},
    publisher = {Association for Computing Machinery},
    doi = {https://doi.org/10.1145/2732158.2732174},
    author = {Alexander Prange and Indra Praveen Sandrala and Markus Weber and Daniel Sonntag},
    keywords = {pen/ink interface, healthcare, reality orientation dialogue, speech dialogue, design, realtime interaction},
    url = {https://doi.org/10.1145/2732158.2732174 https://www.dfki.de/fileadmin/user_upload/import/11232_Robot_Companions_and_Smartpens_for_Improved_Social_Communication_of_Dementia_Patients.pdf}
}

@inproceedings{pub11233,
    abstract = {Gaze and gestures are important modalities in human-human interactions and hence important to human-robot interaction. We describe how to use human gaze and robot pointing gestures to disambiguate and extend a human-robot speech dialogue developed for aiding people suffering from dementia.},
    year = {2015},
    title = {Towards Gaze and Gesture Based Human-Robot Interaction for Dementia Patients},
    booktitle = {2015 AAAI Fall Symposia, Arlington, Virginia, USA, November 12-14, 2015. AAAI Fall Symposium (AAAI-2015), November 12-14},
    pages = {111-113},
    publisher = {AAAI Press},
    author = {Alexander Prange and Takumi Toyama and Daniel Sonntag},
    keywords = {Human-Robot Interaction, Gaze-Based HRI, Gaze, Pointing Gesture, Dementia Patients},
    url = {https://www.dfki.de/fileadmin/user_upload/import/11233_Towards_Gaze,_Gesture_and_Speech-Based.pdf https://cdn.aaai.org/ocs/11696/11696-51323-1-PB.pdf#:~:text=Gaze,%20gestures,%20and%20speech%20are%20important}
}

