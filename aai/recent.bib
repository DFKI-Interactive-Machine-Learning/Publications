@techreport{pub15565,
    series = {Technical Note},
    abstract = {Computational sustainability (CS) is the scientific field that aims to balance societal, economic, and environmental
resources using methods from computer science and artificial intelligence (AI). AI models, e.g., machine learning
models, enrich models of computational sustainability. Research in interactive machine learning can make important
contributions to help address key challenges of sustainability (AI for CS). Computational sustainability questions enrich
AI research, not only by providing problems that involve uncertainty or vagueness, thus generating compelling new AI
challenges, but also by providing a requirement framework for resource-bounded computation (CS for AI).
The research department Interactive machine learning of the German Research Center for Artificial Intelligence hosts
“Computational Sustainability & Technology”; we use applied artificial intelligence methods in the areas of machine
learning, knowledge representation, and intelligent user interfaces to help achieve more sustainable systems (AI for CS)
or to build more sustainable AI systems (CS for AI).
Using the power of, for example, deep learning computers, we can process large quantities of information and
allocate resources based on real-time information. On the other hand, we have to decide when to regulate the power
consumption of such AI systems. Applications are widespread. For example, smart grids implement renewable resources
and storage capabilities to control the production and expenditure of energy. In the project Seadash, we work on
integrating machine learning methods for event detection and classification of underwater signals to preserve marine
fauna. Further, together with edge computing (the new distributed computing paradigm that brings computation and
data storage closer to the location where it is needed) we do not only improve response times and save bandwidth, but
also reduce energy consumption (Mobile AI Lab).
The theory of computational sustainability includes aspects from game theory, machine learning theory and human
computer interaction theory. For example, climate change, pollution, and other environmental crises can be explained by
theories of human psychology (e.g., the individual in a social world) and can hence be computed by (machine learning)
models with computational models of the Prisoner’s Dilemma. More is More? More computation is not always more,
as unsustainable consumption of energy should be avoided. There are already interesting approaches in the machine
learning community, e.g., towards the systematic reporting of the energy and carbon footprints of machine learning or
looking at methodological issues related to training on big data and large web corpora where billions or even trillions
of parameters are tuned. Humans, on the contrary, can do such “training” with only a few examples or from simple
instructions (cf. interactive machine learning, https://www.dfki.de/iml/).
AI for CS and CS for AI and the application domains bring us back to the main challenges of artificial intelligence
research and applied research in the area of CS technology: (1) Incompleteness, (2) vagueness, (3) uncertainty and
reasoning (in deep learning), and (4) resource-bounded computation and learning. In our projects, we tackle these
theoretical challenges and focus on imitation learning, learning with small datasets, transfer learning, long term
autonomy of sustainable AI systems, never ending learning, hybrid teams, IoT, multi-sensor streams for small interaction devices, mobile computing platforms (Mobile AI Lab), and the efficient use of big deep learning clusters.},
    year = {2025},
    title = {Computational Sustainability and Technology (CST)},
    volume = {-},
    institution = {Deutsches Forschungszentrum für Künstliche Intelligenz GmbH},
    author = {Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15565_Technical_Note__CST_.pdf}
}

@inproceedings{pub15620,
    abstract = {Prompt learning methods are gaining increasing attention due to their ability to customize large vision-language models to new domains using pre-trained contextual knowledge and minimal training data. However, existing works typically rely on optimizing unified prompt inputs, often struggling with fine-grained classification tasks due to insufficient discriminative attributes. To tackle this, we consider a new framework based on a dual context of both domain-shared and class-specific contexts, where the latter is generated by Large Language Models (LLMs) such as GPTs. Such dual prompt methods enhance the model’s feature representation by joining implicit and explicit factors encoded in LLM knowledge. Moreover, we formulate the Unbalanced Optimal Transport (UOT) theory to quantify the relationships between constructed prompts and visual tokens. Through partial matching, UOT can properly align discrete sets of visual tokens and prompt embeddings under different mass distributions, which is particularly valuable for handling irrelevant or noisy elements, ensuring that the preservation of mass does not restrict transport solutions. Furthermore, UOT’s characteristics integrate seamlessly with image augmentation, expanding the training sample pool while maintaining a reasonable distance between perturbed images and prompt inputs. Extensive experiments across few-shot classification and adapter settings substantiate the superiority of our model over current state-of-the-art baselines.},
    year = {2025},
    title = {Dude: Dual Distribution-Aware Context Prompt Learning For Large Vision-Language Model},
    booktitle = {The 16th Asian Conference on Machine Learning. Asian Conference on Machine Learning (ACML-2024), December 5-8},
    publisher = {Proceedings of Machine Learning Research},
    author = {Ho Minh Duy Nguyen and An T. Le and Trung Q. Nguyen and Nghiem T. Diep and Tai Nguyen and Duy Duong-Tran and Jan Peters and Li Shen and Mathias Niepert and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15620_199_Dude_Dual_Distribution_Awa.pdf}
}

@inproceedings{pub15769,
    abstract = {Integrating Artificial Intelligence (AI) into Clinical Decision Support Systems (CDSS) presents significant opportunities for improving healthcare delivery, particularly in fields like ophthalmology. This paper explores the usability and trustworthiness of an AI-driven CDSS designed to assist ophthalmologists in treating diabetic retinopathy and age-related macular degeneration. Therefore, we created a CDSS and evaluated its impact on efficiency, informedness, and user experience through task-based semi-structured interviews and questionnaires with 11 ophthalmologists. The usability of the CDSS was rated highly, with a SUS of 81.75. Additionally, results show that participants felt like the CDSS would improve their efficiency and informedness with one major aspect being integrating Electronic Health Records (EHR) and Optical Coherence Tomography (OCT) data into a single interface. Additionally, we explored aspects of the trustworthiness of AI components, specifically OCT segmentation, treatment recommendation, and visual acuity forecasting. Through thematic analysis, we identified key factors influencing trustworthiness and clinical adoption. Results show that a larger degree of abstraction from input to output of a model correlates with decreased trust. From our findings, we propose three guidelines for designing trustworthy CDSS.},
    month = {3},
    year = {2025},
    title = {Towards Trustable Clinical Decision Support Systems: A User Study with Ophthalmologists},
    booktitle = {IUI '25: Proceedings of the 30th International Conference on Intelligent User Interfaces. International Conference on Intelligent User Interfaces (IUI-2025), March 24-27, Cagliari, Italy},
    isbn = {979-8-4007-1306-4},
    publisher = {Association for Computing Machinery, New York, NY, United States},
    author = {Robert Leist and Hans-Jürgen Profitlich and Tim Hunsicker and Daniel Sonntag},
    keywords = {Clinical Decision Support Systems (CDSS), Interactive Machine Learning (IML), Human-AI collaboration, AI-assisted decision making, user trust, domain experts, ophthalmology},
    url = {https://dl.acm.org/doi/10.1145/3708359.3712136 https://www.dfki.de/fileadmin/user_upload/import/15769_3708359.3712136.pdf}
}

@proceedings{pub15775,
    abstract = {Interpreting fundus images is an essential skill for diagnosing eye diseases, such as diabetic retinopathy (DR), one of the leading causes of visual impairment. However, the training of junior doctors relies on experienced ophthalmologists, who often lack time for teaching, or on printed training material that lacks variability in examples. Additionally, machine learning (ML) models successfully detect pathologies relevant to DR and grade the corresponding severity level of cases. With that, our work combines advances in ML with the need of junior doctors to learn independently. We present an interactive learning tool for ophthalmology, which lets junior doctors mark pathologies in fundus images and check them upon the solution of an applied ML algorithm. By aligning the learning concept with theories of cognitive load, usability, and e-learning factors, this system serves as a testbed to explore the potential of ML-supported learning tools for medical education, advancing interactive e-learning.},
    year = {2025},
    title = {FunduScope: A Human-centered Tool for ML-assisted e-Learning in Ophthalmology. International Conference on Intelligent User Interfaces (IUI-2025), located at IUI-2025, March 24-27, Cagliari, Italy},
    editor = {Sara-Jane Bittner and Michael Barz and Hans-Jürgen Profitlich and Mika P. Nieminen and Daniel Sonntag},
    isbn = {979-8-4007-1409-2},
    publisher = {ACM},
    doi = {https://doi.org/10.1145/3708557.3716356},
    keywords = {human-centered design, cognitive load, usability, e-learning, machine learning, learning tool},
    organization = {ACM}
}

@proceedings{pub15776,
    abstract = {The integration of Multimodal-Multisensor Interface (MMI) technologies into Virtual Reality (VR) enables users to engage with computational systems in a natural and immersive way. However, these technologies remain underexplored when applied to deep learning (DL) systems in VR. This paper introduces a VR-based system designed to evaluate how users interact with DL models in virtual environments using MMI technologies, demonstrated through a photobook co-creation use case. The system facilitates human-AI collaboration (co-creation) by allowing users to work with DL models to create photobooks and supports incremental model learning based on user behaviour (Interactive DL) to produce personalised outputs. The tool features a Unity VR frontend that incorporates speech, gaze, and controller inputs. It has a modular backend architecture that allows seamless integration and testing of different DL models. This tool serves as a testbed for exploring MMI in immersive VR environments for both IDL and co-creation.},
    year = {2025},
    title = {Interactive Multimodal Photobook Co-Creation in Virtual Reality. International Conference on Intelligent User Interfaces (IUI-2025), located at IUI-2025, March 24-27, Cagliari, Italy},
    editor = {Sara-Jane Bittner and Robert Leist and László Kopácsi and Omair Shahzad Bhatti and Abdulrahman Mohamed Selim and Michael Barz and Daniel Sonntag},
    isbn = {979-8-4007-1409-2},
    publisher = {ACM},
    doi = {https://doi.org/10.1145/3708557.3716355},
    keywords = {Virtual Reality, Interaction Design, Co-Creation, Interactive Deep Learning, Photobook Creation},
    organization = {ACM}
}

@inproceedings{pub15777,
    abstract = {Verification of biomedical claims is critical for healthcare decision-making, public health policy and scientific research.  We present an interactive biomedical claim verification system by integrating LLMs, transparent model explanations, and user-guided justification. In the system, users first retrieve relevant scientific studies from a persistent medical literature corpus and explore how different LLMs perform natural language inference (NLI) within task-adaptive reasoning framework to classify each study as "Support," "Contradict," or "Not Enough Information" regarding the claim. Users can examine the model's reasoning process with additional insights provided by SHAP values that highlight word-level contributions to the final result. This combination enables a more transparent and interpretable evaluation of the model's decision-making process. A summary stage allows users to consolidate the results by selecting a result with narrative justification generated by LLMs. As a result, a consensus-based final decision is summarized for each retrieved study, aiming safe and accountable AI-assisted decision-making in biomedical contexts. We aim to integrate this explainable verification system as a component within a broader evidence synthesis framework to support human-AI collaboration.},
    month = {3},
    year = {2025},
    title = {Explainable Biomedical Claim Verification with Large Language Models},
    booktitle = {Joint Proceedings of the ACM IUI Workshops 2025. International Conference on Intelligent User Interfaces (IUI-2025), ACM IUI Workshops 2025, located at IUI-2025, March 24-27, Cagliari, Italy},
    publisher = {Joint Proceedings of the ACM IUI Workshops 2025},
    author = {Siting Liang and Daniel Sonntag},
    keywords = {Biomedical Claim Verification, Large Language Models, Natural Language Inference, Explainable AI},
    url = {https://axai.trx.li/papers/5.pdf https://www.dfki.de/fileadmin/user_upload/import/15777_explainable_biomedical_claim_verification_with_LLMs.pdf}
}

@inproceedings{pub15810,
    abstract = {EcoScape Analyzer addresses the need for flexible soundscape analysis to evaluate biodiversity and ecosystem health. Designed for Passive Acoustic Monitoring (PAM) data, it integrates diverse feature extraction methods (acoustic indices, self-supervised, and transfer learning embeddings), dimensionality reduction techniques, and clustering approaches. The tool offers adaptable pipelines, performance feedback, and visualizations, enabling ecologists to explore soundscape patterns and dynamics efficiently, without the need for custom implementations.},
    month = {3},
    year = {2025},
    title = {EcoScape Analyzer: A Tool for Performing Soundscape Analysis With Flexible Pipeline for Biodiversity Assessment},
    booktitle = {IUI '25: Proceedings of the 30th International Conference on Intelligent User Interfaces. International Conference on Intelligent User Interfaces (IUI-2025), March 24-27, Cagliari, Italy},
    pages = {137-140},
    isbn = {9781450375139},
    publisher = {Association for Computing Machinery, New York, NY, United States},
    doi = {https://dl.acm.org/doi/10.1145/3708557.3716359},
    author = {Rida Saghir and Ivan Braga Campos and Thiago Gouvea and Daniel Sonntag},
    url = {https://dl.acm.org/doi/10.1145/3708557.3716359}
}

@inproceedings{pub15811,
    abstract = {Biodiversity loss is a major threat to global sustainability and achieving conservation goals requires informed governance, which depends on robust biodiversity monitoring. Passive Acoustic Monitoring (PAM) enables scalable, continuous data collection, but the vast amount of unlabelled audio data necessitates efficient analysis techniques. While traditional methods focus on species identification, soundscape analysis provides a broader view of ecosystem health by capturing acoustic diversity, temporal patterns, and human impact. To address these challenges, researchers have explored various feature extraction methods, including acoustic indices, predefined acoustical features (PAFs), and AI-based techniques like self-supervised and transfer learning. However, their effectiveness varies by task at hand, requiring careful selection, comparison and technical domain knowledge. This research focuses on development of a tool for soundscape analysis that allows users to flexibly switch between methods and compare outputs in ecologically meaningful ways. By integrating computational techniques with domain relevant information, this research aims to improve biodiversity monitoring and ecosystem assessment.},
    month = {3},
    year = {2025},
    title = {Flexible and Interpretable Soundscape Analysis for Biodiversity Assessment and Ecosystem Health for Domain Experts},
    booktitle = {IUI '25: Proceedings of the 30th International Conference on Intelligent User Interfaces. International Conference on Intelligent User Interfaces (IUI-2025), March 24-27, Cagliari, Italy},
    pages = {218-221},
    isbn = {9781450375139},
    publisher = {Association for Computing Machinery, New York, NY, United States},
    doi = {https://dl.acm.org/doi/full/10.1145/3708557.3716144},
    author = {Rida Saghir},
    url = {https://dl.acm.org/doi/full/10.1145/3708557.3716144}
}

@inproceedings{pub15814,
    abstract = {Large language models remain constrained by the limitations of current user interfaces and interaction paradigms, which hinder their ability to process complex, multimodal information beyond simple text input and output. Our proposed interface, TextVision, aims to address this limitation by enhancing how researchers interact with AI, providing a wide range of functionalities for analyzing, editing, creating new documents, and facilitating collaboration. TextVision advances state-of-the-art human-AI interaction through improved usability and novel interaction techniques, enhancing scientific research and development workflows. As a result, the user can access integrated tools, including a text editor, a PDF viewer, and an AI assistant in a chatbot format. The AI assistant can provide answers based on user input and is context-aware. This output can be enhanced using the built-in prompt designing tool to create efficient, AI-optimized prompts. Users can also select between the latest proprietary LLMs and fine-tuned open-source models tailored for specific tasks.},
    year = {2025},
    title = {TextVision: A more efficient way to work with research},
    booktitle = {Joint Proceedings of the ACM IUI Workshops 2025. International Conference on Intelligent User Interfaces (IUI-2025), March 24-28, Cagliari, Italy},
    editor = {-},
    publisher = {CEUR Proceedings},
    author = {Melis Aslan and Maximilian Bosse and Daniel Christian Helmuth Ehlers and Marlon Hinz and Philipp Olschewski and Jannik Podszun and Elias Scharlach and Leon Selzer and Yukun Wu and Aliki Anagnostopoulou and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15814_TextVision_MIND_submission.pdf}
}

@misc{pub15815,
    abstract = {We propose an approach for personalised and contextualised image captioning. As pre-trained vision-language systems fail to capture details about the user’s intent, occasion, and other information related to the image, we envision a system that addresses these limitations. This approach has two key components for which we need to find suitable practical implementations: multimodal RAG and automatic prompt engineering. We outline our idea and review different possibilities to address these tasks.},
    month = {3},
    year = {2025},
    title = {Self-improving Scene Understanding with Vision-Language Knowledge Integration [Extended Abstract]},
    howpublished = {MIND workshop at the IUI'25 Conference},
    author = {Aliki Anagnostopoulou and Hasan Md Tusfiqur Alam and Daniel Sonntag},
    status_notes = {nicht publiziert - als Poster vorgestellt},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15815_ssuvlaki_extended_abstract.pdf}
}

@inproceedings{pub15818,
    abstract = {Diabetic Retinopathy (DR) and Age-related Macular Degeneration (AMD) are among the leading causes of blindness worldwide. Despite the availability of treatments to prevent disease progression, the effectiveness of these interventions is often limited by inefficiencies in existing clinical software. Recent advancements in Artificial Intelligence (AI) offer the potential to enhance Clinical Decision Support Systems (CDSS), streamlining workflows and reducing the burden on healthcare providers. This paper introduces a CDSS designed to assist ophthalmologists in the management of DR and AMD, integrating three AI-driven components. First, we developed a segmentation model for automated analysis of medical imaging data. Second, we implemented a recommendation algorithm to guide treatment decisions. Finally, we utilized a time series forecasting model to enable predictive medicine. Our models were trained using real-world clinical data from 913 patients with AMD and 461 patients with DR. The system demonstrates promising performance, underscoring the importance of high-performing AI models in advancing CDSS for ophthalmology. The code for our CDSS is available here: https://github.com/DFKI-Interactive-Machine-Learning/ophthalmo-cdss.},
    month = {5},
    year = {2025},
    title = {An AI-driven Clinical Decision Support System for the Treatment of Diabetic Retinopathy and Age-related Macular Degeneration},
    booktitle = {Joint Proceedings of the ACM IUI Workshops 2025. Workshop on Intelligent and Interactive Health User Interfaces (HealthIUI-2025), located at IUI-2025, March 24, Cagliary, Italy},
    publisher = {Association of Computing Machinery, New York, NY, United States},
    author = {Robert Leist and Hans-Jürgen Profitlich and Daniel Sonntag},
    keywords = {Health informatics, Interactive systems and tools, Visualization, Clinical Decision Support Systems (CDSS), Interactive Machine Learning (IML), Human-AI collaboration, AI-assisted decision making, ophthalmology},
    organization = {Association of Computing Machinery},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15818_submission.pdf}
}

@inproceedings{pub15823,
    series = {Lecture Notes in Computer Science},
    abstract = {Deep learning has advanced medical image classification, but interpretability challenges hinder its clinical adoption. This study enhances interpretability in Chest X-ray (CXR) classification by using concept bottleneck models (CBMs) and a multi-agent Retrieval-Augmented Generation (RAG) system for report generation. By modeling relationships between visual features and clinical concepts, we create interpretable concept vectors that guide a multi-agent RAG system to generate radiology reports, enhancing clinical relevance, explainability, and transparency. Evaluation of the generated reports using an LLM-as-a-judge confirmed the interpretability and clinical utility of our model’s outputs. On the COVID-QU dataset, our model achieved 81% classification accuracy and demonstrated robust report generation performance, with five key metrics ranging between 84% and 90%. This interpretable multi-agent framework bridges the gap between high-performance AI and the explainability required for reliable AI-driven CXR analysis in clinical settings. Our code will be released at https://github.com/tifat58/IRR-with-CBM-RAG},
    month = {4},
    year = {2025},
    title = {Towards Interpretable Radiology Report Generation via Concept Bottlenecks Using a Multi-agentic RAG},
    booktitle = {Advances in Information Retrieval - 47th European Conference on Information Retrieval, ECIR 2025, Lucca, Italy, April 6-10, 2025, Proceedings, Part III. European Conference on Information Retrieval (ECIR-2025), 47th European Conference on Information Retrieval, located at ECIR 2025, April 6-10, Lucca, Italy},
    editor = {Claudia Hauff and Craig Macdonald and Dietmar Jannach and Gabriella Kazai and Franco Maria Nardini and Fabio Pinelli and Fabrizio Silvestri and Nicola Tonellotto},
    volume = {15574},
    pages = {201-209},
    publisher = {Springer},
    doi = {https://doi.org/10.1007/978-3-031-88714-7_18},
    author = {Hasan Md Tusfiqur Alam and Devansh Srivastav and Md Abdul Kadir and Daniel Sonntag},
    keywords = {Interpretable Radiology Report Generation, Concept Bottleneck Models , Multi-Agent RAG, Explainable AI, LLMs, VLMs},
    url = {https://doi.org/10.1007/978-3-031-88714-7_18 https://www.dfki.de/fileadmin/user_upload/import/15823_Towards_interpretable_cbm.pdf}
}

@inproceedings{pub15937,
    series = {EICS '25 Companion, EICS '25 Companion},
    abstract = {Advancements in generative Artificial Intelligence (AI) hold great promise for automating radiology workflows, yet challenges in interpretability and reliability hinder clinical adoption. This paper presents an automated radiology report generation framework that combines Concept Bottleneck Models (CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge AI performance with clinical explainability. CBMs map chest X-ray features to human-understandable clinical concepts, enabling transparent disease classification. Meanwhile, the RAG system integrates multi-agent collaboration and external knowledge to produce contextually rich, evidence-based reports. Our demonstration showcases the system’s ability to deliver interpretable predictions, mitigate hallucinations, and generate high-quality, tailored reports with an interactive interface addressing accuracy, trust, and usability challenges. This framework provides a pathway to improving diagnostic consistency and empowering radiologists with actionable insights.},
    month = {6},
    year = {2025},
    title = {CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models},
    booktitle = {Companion Proceedings of the 17th ACM SIGCHI Symposium on Engineering Interactive Computing Systems. ACM SIGCHI Symposium on Engineering Interactive Computing Systems (EICS-2025), 17th ACM SIGCHI Symposium on Engineering Interactive Computing Systems, located at EICS-2025, June 23-27, Trier, Germany},
    pages = {59-61},
    isbn = {9798400718663},
    publisher = {Association for Computing Machinery},
    doi = {https://doi.org/10.1145/3731406.3731970},
    author = {Hasan Md Tusfiqur Alam and Devansh Srivastav and Abdulrahman Mohamed Selim and Md Abdul Kadir and Md Moktadirul Hoque Shuvo and Daniel Sonntag},
    keywords = {Interpretable Radiology report generation, Disease classification, Medical imaging, Concept Bottleneck Models (CBM), Retrieval-Augmented Generation (RAG), Information Retrieval, VLMs, LLMs.},
    url = {https://doi.org/10.1145/3731406.3731970 https://www.dfki.de/fileadmin/user_upload/import/15937_cbm_rag_eics_2025.pdf}
}

@inproceedings{pub15938,
    series = {EICS '25 Companion, EICS '25 Companion},
    abstract = {This paper presents InFL-UX, an interactive, proof-of-concept browser-based Federated Learning (FL) toolkit designed to integrate user contributions into the machine learning (ML) workflow. InFL-UX enables users across multiple devices to upload datasets, define classes, and collaboratively train classification models directly in the browser using modern web technologies. Unlike traditional FL toolkits, which often focus on backend simulations, InFL-UX provides a simple user interface for researchers to explore how users interact with and contribute to FL systems in real-world, interactive settings. InFL-UX bridges the gap between FL and interactive ML by prioritising usability and decentralised model training, empowering non-technical users to actively participate in ML classification tasks.},
    month = {6},
    year = {2025},
    title = {InFL-UX: A Toolkit for Web-Based Interactive Federated Learning},
    booktitle = {Companion Proceedings of the 17th ACM SIGCHI Symposium on Engineering Interactive Computing Systems. ACM SIGCHI Symposium on Engineering Interactive Computing Systems (EICS-2025), 17th ACM SIGCHI Symposium on Engineering Interactive Computing Systems, located at EICS-2025, June 23-27, Trier, Germany},
    pages = {65-67},
    isbn = {9798400718663},
    publisher = {Association for Computing Machinery},
    doi = {https://doi.org/10.1145/3731406.3731972},
    author = {Tim Maurer and Abdulrahman Mohamed Selim and Hasan Md Tusfiqur Alam and Matthias Eiletz and Michael Barz and Daniel Sonntag},
    keywords = {Federated Learning; Interactive Machine Learning; Browser-based Deep Learning},
    url = {https://doi.org/10.1145/3731406.3731972 https://www.dfki.de/fileadmin/user_upload/import/15938_inFL-UX.pdf}
}

@article{pub15948,
    abstract = {Mobile eye tracking is an important tool in psychology and human-centered interaction design for understanding how people process visual scenes and user interfaces. However, analyzing recordings from head-mounted eye trackers, which typically include an egocentric video of the scene and a gaze signal, is a time-consuming and largely manual process. To address this challenge, we develop eyeNotate, a web-based annotation tool that enables semi-automatic data annotation and learns to improve from corrective user feedback. Users can manually map fixation events to areas of interest (AOIs) in a video-editing-style interface (baseline version). Further, our tool can generate fixation-to-AOI mapping suggestions based on a few-shot image classification model (IML-support version). We conduct an expert study with trained annotators (n = 3) to compare the baseline and IML-support versions. We measure the perceived usability, annotations' validity and reliability, and efficiency during a data annotation task. We asked our participants to re-annotate data from a single individual using an existing dataset (n = 48). Further, we conducted a semi-structured interview to understand how participants used the provided IML features and assessed our design decisions. In a post hoc experiment, we investigate the performance of three image classification models in annotating data of the remaining 47 individuals.},
    number = {4},
    month = {7},
    year = {2025},
    title = {eyeNotate: Interactive Annotation of Mobile Eye Tracking Data Based on Few-Shot Image Classification},
    journal = {Journal of Eye Movement Research (JEMR)},
    volume = {18},
    pages = {1-35},
    publisher = {MDPI},
    doi = {https://doi.org/10.3390/jemr18040027},
    author = {Michael Barz and Omair Shahzad Bhatti and Hasan Md Tusfiqur Alam and Ho Minh Duy Nguyen and Kristin Altmeyer and Sarah Malone and Daniel Sonntag},
    keywords = {Eye Tracking; Interactive Machine Learning; Few-Shot Learning; Human-Computer Interaction},
    url = {https://www.mdpi.com/1995-8692/18/4/27 https://www.dfki.de/fileadmin/user_upload/import/15948_jemr-18-00027.pdf}
}

@article{pub15949,
    abstract = {Comprehending how humans process visual information in dynamic settings is crucial for psychology and designing user-centered interactions. While mobile eye-tracking systems combining egocentric video and gaze signals can offer valuable insights, manual analysis of these recordings is time-intensive. In this work, we present a novel human-centered learning algorithm designed for automated object recognition within mobile eye-tracking settings. Our approach seamlessly integrates an object detector with a spatial relation-aware inductive message-passing network (I-MPN), harnessing node profile information and capturing object correlations. Such mechanisms enable us to learn embedding functions capable of generalizing to new object angle views, facilitating rapid adaptation and efficient reasoning in dynamic contexts as users navigate their environment. Through experiments conducted on three distinct video sequences, our interactive-based method showcases significant performance improvements over fixed training/testing algorithms, even when trained on considerably smaller annotated samples collected through user feedback. Furthermore, we demonstrate exceptional efficiency in data annotation processes and surpass prior interactive methods that use complete object detectors, combine detectors with convolutional networks, or employ interactive video segmentation.},
    number = {1},
    month = {4},
    year = {2025},
    title = {I-MPN: inductive message passing network for efficient human-in-the-loop annotation of mobile eye tracking data},
    journal = {Scientific Reports (Sci Rep)},
    volume = {15},
    pages = {1-17},
    publisher = {Springer Nature},
    doi = {https://doi.org/10.1038/s41598-025-94593-y},
    author = {Hoang H. Le and Ho Minh Duy Nguyen and Omair Shahzad Bhatti and László Kopácsi and Thinh P. Ngo and Binh T. Nguyen and Michael Barz and Daniel Sonntag},
    url = {https://doi.org/10.1038/s41598-025-94593-y https://www.dfki.de/fileadmin/user_upload/import/15949_s41598-025-94593-y.pdf}
}

@inproceedings{pub15963,
    abstract = {Biomedical claim verification involves determining the entailment relationship between a claim and evidence derived from medical studies or clinical trial reports (CTRs). In this work, we propose a structured four-step prompting strategy that explicitly guides large language models (LLMs) through (1) claim comprehension, (2) evidence analysis, (3) intermediate conclusion, and (4) entailment decision-making to improve the accuracy of biomedical claim verification. This strategy leverages compositional and human-like reasoning to enhance logical consistency and factual grounding to reduce reliance on memorizing few-shot exemplars and help LLMs generalize reasoning patterns across different biomedical claim verification tasks. Through extensive evaluation on biomedical NLI benchmarks, we analyze the individual contributions of each reasoning step. Our findings demonstrate that comprehension, evidence analysis, and intermediate conclusion each play distinct yet complementary roles. Systematic prompting and carefully designed step-wise instructions not only unlock the latent cognitive abilities of LLMs but also enhance interpretability by making it easier to trace errors and understand the model’s reasoning process. This research aims to improve the reliability of AI-driven biomedical claim verification.},
    year = {2025},
    title = {Advancing Biomedical Claim Verification by Using Large Language Models with Better Structured Prompting Strategies},
    booktitle = {Proceedings of the 23rd Workshop on Biomedical Natural Language Processing. Workshop on Biomedical Natural Language Processing (BioNLP-2025), located at ACL 2025, August 1, Vienna, Austria},
    publisher = {ACL Anthology},
    author = {Siting Liang and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15963_Advancing_Biomedical_Claim_Verification_by_Using_Large_Language_Models_with_Better_Structured_Prompting_Strategies_camera_ready.pdf}
}

@inbook{pub14702,
    abstract = {Many industries are transitioning to Industry 4.0 production models by adopting robots in their manufacturing processes. In parallel, Extended Reality (XR) technologies have reached sufficient maturity to enter the industrial applications domain, with early success cases often related to training workers, remote assistance, access to contextual information, and interaction with digital twins. In the future, robots will be increasingly enhanced with XR applications, which requires that industrial workers understand both technologies and use and control hybrid solutions confidently. Specific education and training programs will be essential to this transition, especially for vocational school students and professionals in upskilling. They must learn how to program robots and establish a safe and productive human-robot collaboration. The new EU-funded project MASTER will improve the XR ecosystem for teaching and training robotics in manufacturing by providing an open XR platform that integrates key functionalities like creating safe robotic environments, programming flexible robotic applications, and integrating advanced interaction mechanisms based on eye tracking. It will also provide high-quality training materials for robotics. We report on the project plan, our objectives, and milestones.},
    year = {2024},
    title = {MASTER-XR: Mixed Reality Ecosystem for Teaching Robotics in Manufacturing},
    booktitle = {Integrated Systems: Data Driven Engineering},
    editor = {Mohammad-Reza Alam and Madjid Fathi},
    pages = {167-182},
    isbn = {978-3-031-53652-6},
    publisher = {Springer},
    doi = {https://doi.org/10.1007/978-3-031-53652-6_10},
    author = {Michael Barz and Panagiotis Karagiannis and Johan Kildal and Andoni Rivera Pinto and Judit Ruiz de Munain and Jesús Rosel and Maria Madarieta and Konstantina Salagianni and Panagiotis Aivaliotis and Sotiris Makris and Daniel Sonntag},
    keywords = {Industry 4.0, Extended Reality (XR), Robotics, Worker Training, Manufacturing, Human-Robot Collaboration, Eye Tracking},
    status_notes = {to appear},
    url = {https://www.dfki.de/fileadmin/user_upload/import/14702_MASTER_XR_-_ISDT_23_-_Preprint.pdf https://link.springer.com/chapter/10.1007/978-3-031-53652-6_10}
}

@article{pub14704,
    abstract = {As automation increases qualitatively and quantitatively in safety-critical human cyber-physical systems, it
is becoming more and more challenging to increase the probability or ensure that human operators still per-
ceive key artifacts and comprehend their roles in the system. In the companion paper, we proposed an abstract
reference architecture capable of expressing all classes of system-level interactions in human cyber-physical
systems. Here we demonstrate how this reference architecture supports the analysis of levels of communi-
cation between agents and helps to identify the potential for misunderstandings and misconceptions. We
then develop a metamodel for safe human machine interaction. Therefore, we ask what type of information
exchange must be supported on what level so that humans and systems can cooperate as a team, what is
the criticality of exchanged information, what are timing requirements for such interactions, and how can
we communicate highly critical information in a limited time frame in spite of the many sources of a dis-
torted perception. We highlight shared stumbling blocks and illustrate shared design principles, which rest
on established ontologies specific to particular application classes. In order to overcome the partial opacity
of internal states of agents, we anticipate a key role of virtual twins of both human and technical cooperation
partners for designing a suitable communication.},
    month = {1},
    year = {2024},
    title = {A References Architecture for Human Cyber Physical Systems, Part II: Fundamental Design Principles for Human-CPS Interaction},
    journal = {ACM Transactions on Cyber-Physical Systems (TCPS)},
    volume = {8},
    pages = {1-27},
    publisher = {ACM},
    author = {Klaus Bengler and Werner Damm and Andreas Luedtke and Jochem Rieger and Benedikt Austel and Bianca Biebl and Martin Fränzle and Willem Hagemann and Moritz Held and David Hess and Klas Ihme and Severin Kacianka and Alyssa J Kerscher and Forrest Laine and Sebastian Lehnhoff and Alexander Pretschner and Astrid Rakow and Daniel Sonntag and Janos Sztipanovits and Maike Schwammberger and Mark Schweda and Anirudh Unni and Eric Veith},
    url = {https://scholar.google.de/citations?view_op=view_citation&hl=en&user=v7i6Uz4AAAAJ&sortby=pubdate&citation_for_view=v7i6Uz4AAAAJ:IUKN3-7HHlwC https://www.dfki.de/fileadmin/user_upload/import/14704_A_references_Architecture_for_Human_Cyber_Physical_Systems_Part_II.pdf}
}

@article{pub14705,
    abstract = {We propose a reference architecture of safety-critical or industry-critical human cyber-physical systems
(CPSs) capable of expressing essential classes of system-level interactions between CPS and humans rele-
vant for the societal acceptance of such systems. To reach this quality gate, the expressivity of the model
must go beyond classical viewpoints such as operational, functional, and architectural views and views used
for safety and security analysis. The model does so by incorporating elements of such systems for mutual
introspections in situational awareness, capabilities, and intentions to enable a synergetic, trusted relation in
the interaction of humans and CPSs, which we see as a prerequisite for their societal acceptance. The refer-
ence architecture is represented as a metamodel incorporating conceptual and behavioral semantic aspects.
We illustrate the key concepts of the metamodel with examples from cooperative autonomous driving, the
operating room of the future, cockpit-tower interaction, and crisis management.},
    month = {1},
    year = {2024},
    title = {A Reference Architecture of Human Cyber-Physical Systems – Part I: Fundamental Concepts},
    journal = {ACM Transactions on Cyber-Physical Systems (TCPS)},
    volume = {8},
    pages = {1-32},
    publisher = {ACM},
    author = {Werner Damm and David Hess and Mark Schweda and Janos Sztipanovits and Klaus Bengler and Bianca Biebl and Martin Fränzle and Willem Hagemann and Moritz Held and Klas Ihme and Severin Kacianka and Alyssa J Kerscher and Sebastian Lehnhoff and Andreas Luedtke and Alexander Pretschner and Astrid Rakow and Jochem Rieger and Daniel Sonntag and Maike Schwammberger and Benedikt Austel and Anirudh Unni and Eric Veith},
    url = {https://scholar.google.de/citations?view_op=view_citation&hl=en&user=v7i6Uz4AAAAJ&sortby=pubdate&citation_for_view=v7i6Uz4AAAAJ:mlAyqtXpCwEC https://www.dfki.de/fileadmin/user_upload/import/14705_A_Reference_Architecture_of_Human_Cyber-Physical_Systems_Part_I.pdf}
}

@inproceedings{pub14721,
    abstract = {Our work explores the effectiveness of employing Clinical BERT for Relation Extraction (RE) tasks in medical texts within an Active Learning (AL) framework. Our main objective is to optimize RE in medical texts through AL while examining the trade-offs between performance and computation time, comparing it with alternative methods like Random Forest and BiLSTM networks. Comparisons extend to feature engineering requirements, performance metrics, and considerations of annotation costs, including AL step times and annotation rates. The utilization of AL strategies aligns with our broader goal of enhancing the efficiency of relation classification models, particularly when dealing with the challenges of annotating complex medical texts in a Human-in-the-Loop (HITL) setting. The results indicate that uncertainty-based sampling achieves comparable performance with significantly fewer annotated samples across three categories of supervised learning methods, thereby reducing annotation costs for clinical and biomedical corpora. While Clinical BERT exhibits clear performance advantages across two different corpora, the trade-off involves longer computation times in interactive annotation processes. In real-world applications, where practical feasibility and timely results are crucial, optimizing this trade-off becomes imperative.},
    year = {2024},
    title = {Optimizing Relation Extraction in Medical Texts through Active Learning: A Comparative Analysis of Trade-offs},
    booktitle = {Association for Computational Linguistics. Conference of the European Chapter of the Association for Computational Linguistics (EACL-2024), March 17-22, St. Julians, Malta},
    publisher = {ACL Anthology},
    author = {Siting Liang and Pablo Valdunciel Sánchez and Daniel Sonntag},
    url = {https://aclanthology.org/2024.uncertainlp-1.3/ https://www.dfki.de/fileadmin/user_upload/import/14721_Optimizing_Relation_Extraction_in_Medical_Texts_through_Active_Learning.pdf}
}

@article{pub14732,
    abstract = {The design and analysis of multi-agent human cyber-physical systems in safety-critical or industry-critical domains calls for an adequate semantic foundation capable of exhaustively and rigorously describing all emergent effects in the joint dynamic behavior of the agents that are relevant to their safety and well-behavior. We present such a semantic foundation. This framework extends beyond previous approaches by extending the agent-local dynamic state beyond state components under direct control of the agent and belief about other agents (as previously suggested for understanding cooperative as well as rational behavior) to agent-local evidence and belief about the overall cooperative, competitive, or coopetitive game structure. We argue that this extension is necessary for rigorously analyzing systems of human cyber-physical systems because humans are known to employ cognitive replacement models of system dynamics that are both non-stationary and potentially incongruent. These replacement models induce visible and potentially harmful effects on their joint emergent behavior and the interaction with cyber-physical system components.},
    number = {1},
    year = {2024},
    title = {A Reference Architecture of Human Cyber-Physical Systems - Part III: Semantic Foundations},
    journal = {ACM Transactions on Cyber-Physical Systems (TCPS)},
    volume = {8},
    pages = {1-23},
    publisher = {ACM},
    doi = {https://doi.org/10.1145/3622881},
    author = {Werner Damm and Martin Fränzle and Alyssa J. Kerscher and Forrest Laine and Klaus Bengler and Bianca Biebl and Willem Hagemann and Moritz Held and David Hess and Klas Ihme and Severin Kacianka and Sebastian Lehnhoff and Andreas Lüdtke and Alexander Pretschner and Astrid Rakow and Jochem W. Rieger and Daniel Sonntag and Janos Sztipanovits and Maike Schwammberger and Mark Schweda and Alexander Trende and Anirudh Unni and Eric M. S. P. Veith},
    url = {https://dl.acm.org/doi/10.1145/3622881 https://www.dfki.de/fileadmin/user_upload/import/14732_A_Reference_Architecture_of_Human_Cyber-Physical_Part_III.pdf}
}

@inproceedings{pub14737,
    abstract = {Passive Acoustic Monitoring (PAM) has emerged as a pivotal technology for wildlife monitoring, generating vast amounts of acoustic data. However, the successful application of machine learning methods for sound event detection in PAM datasets heavily relies on the availability of annotated data, which can be laborious to acquire. In this study, we investigate the effectiveness of transfer learning and active learning techniques to address the data annotation challenge in PAM. Transfer learning allows us to use pre-trained models from related tasks or datasets to bootstrap the learning process for sound event detection. Furthermore, active learning promises strategic selection of the most informative samples for annotation, effectively reducing the annotation cost and improving model performance. We evaluate an approach that combines transfer learning and active learning to efficiently exploit existing annotated data and optimize the annotation process for PAM datasets. Our transfer learning observations show that embeddings produced by BirdNet, a model trained on high signal-to-noise recordings of bird vocalisations, can be effectively used for predicting anurans in PAM data: a linear classifier constructed using these embeddings outperforms the benchmark by 21.7%. Our results indicate that active learning is superior to random sampling, although no clear winner emerges among the strategies employed. The proposed method holds promise for facilitating broader adoption of machine learning techniques in PAM and advancing our understanding of biodiversity dynamics through acoustic data analysis.},
    year = {2024},
    title = {Leveraging Transfer Learning and Active Learning for Sound Event Detection in Passive Acoustic Monitoring of Wildlife},
    booktitle = {3rd Annual AAAI Workshop on AI to Accelerate Science and Engineering. AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE-2024), located at AAAI, February 26, Vancouver,, BC, Canada},
    publisher = {o.A.},
    author = {Hannes Kath and Patricia P. Serafini and Ivan Braga Campos and Thiago Gouvea and Daniel Sonntag},
    url = {https://ai-2-ase.github.io/papers/35%5cCameraReady.pdf https://www.dfki.de/fileadmin/user_upload/import/14737_Kath_et_al_2024_Leveraging_Transfer_Learning_and_Active_Learning_for_Sound_Event_Detection_in.pdf}
}

@inproceedings{pub14738,
    abstract = {Monitoring biodiversity in biosphere reserves is challenging due to the vast regions to be monitored. Thus, conservation- ists have resorted to employing passive acoustic monitoring (PAM), which automates the audio recording process. PAM can create large, unlabeled datasets, but deriving knowledge from such recordings is usually still done manually.
Machine learning enables the detection of vocalizations of species automatically, allowing summarizing the biodiversity in an area in terms of species richness. While pre-trained neu- ral network models for bird vocalization detection exist, they are often not-reliable enough to do way with the need for manual labeling of audio files.
In this paper, we present BirdNET-Annotator, a tool for AI- assisted labeling of audio datasets co-developed by ecoacous- tics and ML experts. BirdNET-Annotator runs in the cloud free of charge, enabling end users to scale beyond the limita- tions of their local hardware. We evaluated the performance of our solution in the context of its intended workflow and found a reduction in annotation times. While our results show that our application now meets the user requirements, there are still opportunities to seize for additional performance and usability improvement.
Our application illustrates how large, pre-trained neural mod- els can be integrated into the workflow of domain experts when packaged in a user-friendly manner. We observe that although our solution adds a step to the preexisting workflow, the overall annotation speed is significantly improved. This hints at further improvement to be realized in the future by consolidating more steps of the workflow into fewer tools.},
    year = {2024},
    title = {BirdNET-Annotator: AI-Assisted Strong Labelling of Bird Sound Datasets},
    booktitle = {3rd Annual AAAI Workshop on AI to Accelerate Science and Engineering. AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE), located at AAAI, February 26, Vancouver,, BC, Canada},
    publisher = {o.A.},
    author = {Bengt Lüers and Patricia P. Serafini and Ivan Braga Campos and Thiago Gouvea and Daniel Sonntag},
    url = {https://ai-2-ase.github.io/papers/15%5cCameraReady%5c20231211_AI2ASE_AAAI_BirdNET_Annotator_CameraReady.pdf https://www.dfki.de/fileadmin/user_upload/import/14738_Lueers_et_al_2024_BirdNET-Annotator.pdf}
}

@inproceedings{pub14739,
    abstract = {The utilization of Passive Acoustic Monitoring (PAM) for wildlife monitoring remains hindered by the challenge of data analysis. While numerous supervised ML algorithms exist, their application is constrained by the scarcity of annotated data. Expert-curated sound collections are valuable knowl- edge sources that could bridge this gap. However, their uti- lization is hindered by the sporadic sounds to be identified in these recordings. In this study, we propose a weakly su- pervised approach to tackle this challenge and assess its per- formance using the AnuraSet dataset. We employ TALNet, a Convolutional Recurrent Neural Network (CRNN) model and train it on 60-second sound recordings labeled for the presence of 42 different anuran species. We conduct the eval- uation on 1-second segments, enabling precise sound event localization. Furthermore, we investigate the impact of vary- ing the length of the training input and explore different pool- ing functions’ effects on TALNet’s performance on AnuraSet. Our findings demonstrate the effectiveness of TALNet in har- nessing weakly annotated sound collections for wildlife mon- itoring.},
    year = {2024},
    title = {Leveraging Sound Collections for Animal Species Classification with Weakly Supervised Learning},
    booktitle = {3rd Annual AAAI Workshop on AI to Accelerate Science and Engineering. AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE-2024), located at AAAI, February 26, Vancouver,, BC, Canada},
    publisher = {o.A.},
    author = {Ilira Troshani and Thiago Gouvea and Daniel Sonntag},
    url = {https://ai-2-ase.github.io/papers/19%5cCameraReady.pdf https://www.dfki.de/fileadmin/user_upload/import/14739_Troshani_et_al_2024_Leveraging_Sound_Collections_for_Animal_Species_Classification_with_Weakly.pdf}
}

@misc{pub14772,
    abstract = {Image annotation is one of the most essential tasks for guaranteeing proper treatment for patients and tracking progress over the course of therapy in the field of medical imaging and disease diagnosis. However, manually annotating a lot of 2D and 3D imaging data can be extremely tedious. Deep Learning (DL) based segmentation algorithms have completely transformed this process and made it possible to automate image segmentation. By accurately segmenting medical images, these algorithms can greatly minimize the time and effort necessary for manual annotation. Additionally, by incorporating Active Learning (AL) methods, these segmentation algorithms can perform far more effectively with a smaller amount of ground truth data. We introduce MedDeepCyleAL, an end-to-end framework implementing the complete AL cycle. It provides researchers with the flexibility to choose the type of deep learning model they wish to employ and includes an annotation tool that supports the classification and segmentation of medical images. The user-friendly interface allows for easy alteration of the AL and DL model settings through a configuration file, requiring no prior programming experience. While MedDeepCyleAL can be applied to any kind of image data, we have specifically applied it to ophthalmology data in this project.},
    month = {3},
    year = {2024},
    title = {Modular Deep Active Learning Framework for Image Annotation: A Technical Report for the Ophthalmo-AI Project},
    author = {Md Abdul Kadir and Hasan Md Tusfiqur Alam and Pascale Maul and Hans-Jürgen Profitlich and Moritz Wolf and Daniel Sonntag},
    url = {https://arxiv.org/abs/2403.15143 https://www.dfki.de/fileadmin/user_upload/import/14772_2403.15143.pdf}
}

@inproceedings{pub14838,
    abstract = {Clinical Named Entity Recognition (NER) is essential for extracting important medical insights from clinical narratives. Given the challenges in obtaining expert training datasets for real-world clinical applications related to data protection regulations and the lack of standardised entity types, this work represents a collaborative initiative aimed at building a German clinical NER system with a focus on addressing these obstacles effectively. In response to the challenge of training data scarcity, we propose a \textbf{Conditional Relevance Learning (CRL)} approach in low-resource transfer learning scenarios. \textbf{CRL} effectively leverages a pre-trained language model and domain-specific open resources, enabling the acquisition of a robust base model tailored for clinical NER tasks, particularly in the face of changing label sets. This flexibility empowers the implementation of a \textbf{Multilayered Semantic Annotation (MSA)} schema in our NER system, capable of organizing a diverse array of entity types, thus significantly boosting the NER system's adaptability and utility across various clinical domains. In the case study, we demonstrate how our NER system can be applied to overcome resource constraints and comply with data privacy regulations. Lacking prior training on in-domain data, feedback from expert users in respective domains is essential in identifying areas for system refinement. Future work will focus on the integration of expert feedback to improve system performance in specific clinical contexts.},
    year = {2024},
    title = {Building A German Clinical Named Entity Recognition System without In-domain Training Data},
    booktitle = {Association for Computational Linguistics. Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-2024), June 17-21, Mexico City, Mexico},
    publisher = {ACL Anthology},
    author = {Siting Liang and Hans-Jürgen Profitlich and Maximilian Klass and Niko Möller-Grell and Celine-Fabienne Bergmann and Simon Heim and Christian Niklas and Daniel Sonntag},
    keywords = {Clinical NLP, Named Entity Recognition, Low-resource Language, Transfer Learning},
    url = {https://aclanthology.org/2024.clinicalnlp-1.7/ https://www.dfki.de/fileadmin/user_upload/import/14838_Building_A_German_NER_without_In_domain_Training_Data__clinical_NLP__(1).pdf}
}

@inproceedings{pub14962,
    abstract = {Games are used in multiple fields of brain-computer interface (BCI) research and applications to improve participants’ engagement and enjoyment during electroencephalogram (EEG) data collection. However, despite potential benefits, no current studies have reported on implemented games for Speech Imagery BCI. Imagined speech is speech produced without audible sounds or active movement of the articulatory muscles. Collecting imagined speech EEG data is a time-consuming, mentally exhausting, and cumbersome process, which requires participants to read words off a computer screen and produce them as imagined speech. To improve this process for study participants, we implemented a maze-like game where a participant navigated a virtual robot capable of performing five actions that represented our words of interest while we recorded their EEG data. The study setup was evaluated with 15 participants. Based on their feedback, the game improved their engagement and enjoyment while resulting in a 69.10% average classification accuracy using a random forest classifier.},
    month = {6},
    year = {2024},
    title = {Speech Imagery BCI Training Using Game with a Purpose},
    booktitle = {Proceedings of the 2024 International Conference on Advanced Visual Interfaces. International Working Conference on Advanced Visual Interfaces (AVI-2024), June 3-7, Arenzano, Genoa, Italy},
    pages = {1-5},
    isbn = {9798400717642},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {https://doi.org/10.1145/3656650.3656654},
    author = {Abdulrahman Mohamed Selim and Maurice Rekrut and Michael Barz and Daniel Sonntag},
    keywords = {BCI, EEG, Game with a purpose (GWAP), Imagined speech, User study},
    url = {https://doi.org/10.1145/3656650.3656654 https://www.dfki.de/fileadmin/user_upload/import/14962_avi2024-4.pdf}
}

@article{pub14976,
    abstract = {The scanpath is an important concept in eye tracking. It refers to a person's eye movements over a period of time, commonly represented as a series of alternating fixations and saccades. Machine learning has been increasingly used for the automatic interpretation of scanpaths over the past few years, particularly in research on passive gaze-based interaction, i.e., interfaces that implicitly observe and interpret human eye movements, with the goal of improving the interaction. This literature review investigates research on machine learning applications in scanpath analysis for passive gaze-based interaction between 2012 and 2022, starting from 2,425 publications and focussing on 77 publications. We provide insights on research domains and common learning tasks in passive gaze-based interaction and present common machine learning practices from data collection and preparation to model selection and evaluation. We discuss commonly followed practices and identify gaps and challenges, especially concerning emerging machine learning topics, to guide future research in the field.},
    month = {6},
    year = {2024},
    title = {A review of machine learning in scanpath analysis for passive gaze-based interaction},
    editor = {Maria Chiara Caschera},
    journal = {Frontiers in Artificial Intelligence (Front. Artif. Intell.)},
    volume = {7},
    pages = {1-28},
    publisher = {Frontiers Media SA, Avenue du Tribunal-Fédéral 34 1005 Lausanne Switze},
    doi = {https://doi.org/10.3389/frai.2024.1391745},
    author = {Abdulrahman Mohamed Selim and Michael Barz and Omair Shahzad Bhatti and Hasan Md Tusfiqur Alam and Daniel Sonntag},
    keywords = {machine learning, eye tracking, scanpath, passive gaze-based interaction, literature review},
    url = {https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1391745/full}
}

@techreport{pub15026,
    series = {DFKI Technical Report},
    abstract = {This DFKI technical report presents the anatomy of the No-IDLE prototype system (funded by the German Federal Ministry of Education and Research) that provides not only basic and fundamental research in interactive machine learning, but also reveals deeper insights into users' behaviours, needs, and goals. Machine learning and deep learning should become accessible to millions of end users. No-IDLE's goals and scienfific challenges centre around the desire to increase the reach of interactive deep learning solutions for non-experts in machine learning. One of the key innovations described in this technical report is a methodology for interactive machine learning combined with multimodal interaction which will become central when we start interacting with semi-intelligent machines in the upcoming area of neural networks and large language models.},
    month = {6},
    year = {2024},
    title = {A look under the hood of the Interactive Deep Learning Enterprise (No-IDLE)},
    volume = {-},
    institution = {German Research Center for AI},
    author = {Daniel Sonntag and Michael Barz and Thiago Gouvea},
    url = {https://arxiv.org/abs/2406.19054 https://www.dfki.de/fileadmin/user_upload/import/15026_2406.19054v1.pdf}
}

@inproceedings{pub15055,
    abstract = {Passive Acoustic Monitoring (PAM) has become a key technology in wildlife monitoring, generating large amounts of acoustic data. However, the effective application of machine learning methods for sound event detection in PAM datasets is highly dependent on the accessibility of annotated data, a process that can be labour intensive. As a team of domain experts and machine learning researchers, in this paper we present a no-code annotation tool designed for PAM datasets that incorporates transfer learning and active learning strategies to address the data annotation challenge inherent in PAM. Transfer learning is applied to use pre-trained models to compute meaningful embeddings from the PAM audio files. Active learning iteratively identifies the most informative samples and then presents them to the user for annotation. This iterative approach improves the performance of the model compared to random sample selection. In a preliminary evaluation of the tool, a domain expert annotated part of a real PAM data set. Compared to conventional tools, the workflow of the proposed tool showed a speed improvement of 2-4 times. Further enhancements, such as the incorporation of sound examples, have the potential to further improve efficiency.},
    month = {8},
    year = {2024},
    title = {Demo: Enhancing Wildlife Acoustic Data Annotation Efficiency through Transfer and Active Learning},
    booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence. International Joint Conference on Artificial Intelligence (IJCAI-2024), located at IJCAI, August 03-09, Jeju. International Joint Conference on Artificial Intelligence (IJCAI-2024), August 3-9, Jeju, Korea, Republic of},
    publisher = {International Joint Conferences on Artificial Intelligence},
    author = {Hannes Kath and Patricia P. Serafin and Ivan B. Campos and Thiago Gouvea and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15055_Demo_TransferLearning_ActiveLearning_Bioacoustic.pdf https://www.ijcai.org/proceedings/2024/1010}
}

@inproceedings{pub15056,
    abstract = {Passive Acoustic Monitoring (PAM) has become a key technology in wildlife monitoring, providing vast amounts of acoustic data.
The recording process naturally generates multi-label datasets; however, due to the significant annotation time required, most available datasets use exclusive labels. While active learning (AL) has shown the potential to speed up the annotation process of multi-label PAM data, it lacks standardized performance metrics across experimental setups. We present a novel performance metric for AL, the `speedup factor', which remains constant across experimental setups. 
It quantifies the fraction of samples required by an AL strategy compared to random sampling to achieve equivalent model performance.
Using two multi-label PAM datasets, we investigate the effects of class sparsity, ceiling performance, number of classes, and different AL strategies on AL performance. Our results show that AL performance is superior on datasets with sparser classes, lower ceiling performance, fewer classes, and when using uncertainty sampling strategies.},
    month = {9},
    year = {2024},
    title = {Active Learning in Multi-label Classification of Bioacoustic Data},
    booktitle = {KI 2024: Advances in Artificial Intelligence. German Conference on Artificial Intelligence (KI-2024), 47th German Conference on AI, Würzburg, Germany, September 25–27, 2023, Proceedings, located at 47th German Conference on AI, September 25-27, Würzburg, Germany, Germany},
    editor = {Dietmar Seipel and Alexander Steen},
    publisher = {Springer, Heidelberg},
    author = {Hannes Kath and Thiago Gouvea and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15056_Active_Learning_in_Multi-label_Classification_of_Bioacoustic_Data.pdf}
}

@inproceedings{pub15057,
    abstract = {Passive Acoustic Monitoring (PAM) has become a key technology in wildlife monitoring, generating large amounts of acoustic data. However, the effective application of machine learning methods for sound event detection in PAM datasets is highly dependent on the availability of annotated data, which requires a labour-intensive effort to generate. This paper summarises two iterative, human-centred approaches that make efficient use of expert annotation time to accelerate understanding of the data: Combining transfer learning and active learning, we present an annotation tool that selects and annotates the most informative samples one at a time. To annotate multiple samples simultaneously, we present a tool that allows annotation in the embedding space of a variational autoencoder manipulated by a classification head. For both approaches, we provide no-code web applications for intuitive use by domain experts.},
    month = {9},
    year = {2024},
    title = {A Human-in-the-Loop Tool for Annotating Passive Acoustic Monitoring Datasets\\(Extended Abstract)},
    booktitle = {KI 2024: Advances in Artificial Intelligence. German Conference on Artificial Intelligence (KI-2024), 47th German Conference on AI, Würzburg, Germany, September 25–27, 2023, Proceedings, located at 47th German Conference on AI, September 25-27, Würzburg, Germany, Germany},
    editor = {Dietmar Seipel and Alexander Steen},
    publisher = {Springer, Heidelberg},
    author = {Hannes Kath and Thiago Gouvea and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15057_Bioacoustic_Annotation_Extended_Abstract.pdf}
}

@inproceedings{pub15069,
    abstract = {While data collection and annotation is crucial for training super- vised machine learning models and improving their accuracy, it can be resource-intensive. In this paper, we propose a weakly su- pervised method to extract fine-grained information from existing weakly-annotated data accumulated over time and alleviate the need for collection and annotation of fresh data. We also integrate it in an interactive tool that facilitates training and annotation. Communities comprising ecologists and other domain experts can use it to train machine learning models to detect animal species and monitor wildlife in protected areas. Our method not only improves the extraction of information from coarse labels but also simplifies the process of annotating new data for experts.. By lowering the time and expertise barrier to data annotation, we also aim to en- courage individuals with varying levels of expertise to participate more in citizen science and contribute to preserving ecosystems.},
    month = {5},
    year = {2024},
    title = {Wild Data Treasures: Towards Sustainable Practices in Deep Learning for Wildlife Monitoring},
    booktitle = {CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems. CHI Workshop on Sustaining Scalable Sustainability, Human-Centered Green Technology for Community-wide Carbon Reduction, located at ACM CHI 2024, May 11, Honolulu,, HI, USA},
    isbn = {979-8-4007-0331-7},
    publisher = {ACM, New York, USA},
    author = {Ilira Troshani and Thiago Gouvea and Daniel Sonntag},
    url = {https://sustainingscalablesustainability.wordpress.com/wp-content/uploads/2024/04/wild-data-treasures.pdf https://www.dfki.de/fileadmin/user_upload/import/15069_wild-data-treasures.pdf}
}

@inproceedings{pub15118,
    abstract = {Data collection and annotation are time-consuming, resource-intensive processes that often require domain expertise. Existing data collections such as animal sound collections provide valuable data sources, but their utilization is often hindered by the lack of fine-grained labels. In this study, we examine the use of existing weakly supervised methods 
to extract fine-grained information from existing weakly-annotated data accumulated over time and alleviate the need for collection and annotation of fresh data. We employ TALNet, a Convolutional Recurrent Neural Network (CRNN) model and train it on 60-second sound recordings labeled for the presence of 42 different anuran species and compare it to other models such as BirdNet, a model for detection of bird vocalisation. We conduct the evaluation on 1-second segments, enabling precise sound event localization.
Furthermore, we investigate the impact of varying the length of the training input and explore different pooling functions' effects on the model's performance on AnuraSet. Finally, we integrate it in an interactive user interface that facilitates training and annotation. Our findings demonstrate the effectiveness of TALNet and BirdNet in harnessing weakly annotated sound collections for wildlife monitoring. Our method not only improves the extraction of information from coarse labels but also simplifies the process of annotating new data for experts.},
    year = {2024},
    title = {Leveraging Weakly Supervised and Multiple Instance Learning for Multi-label Classification of Passive Acoustic Monitoring Data},
    booktitle = {KI 2024: Advances in Artificial Intelligence. German Conference on Artificial Intelligence (KI-2024), 47th German Conference on AI, Würzburg, Germany, September 25–27, 2023, located at 47th German Conference on AI, September 25-27, Würzburg, Germany},
    publisher = {Springer},
    author = {Ilira Troshani and Thiago Gouvea and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15118__KI24__WSL4bioacoustics-3.pdf}
}

@misc{pub15145,
    abstract = {Large language models (LLMs) and large multimodal models (LMMs) have significantly impacted the AI community, industry, and various economic sectors. In journalism, integrating AI poses unique challenges and opportunities, particularly in enhancing the quality and efficiency of news reporting. This study explores how LLMs and LMMs can assist journalistic practice by generating contextualised captions for images accompanying news articles. We conducted experiments using the GoodNews dataset to evaluate the ability of LMMs (BLIP-2, GPT-4v, or LLaVA) to incorporate one of two types of context: entire news articles, or extracted named entities. In addition, we compared their performance to a two-stage pipeline composed of a captioning model (BLIP-2, OFA, or ViT-GPT2) with post-hoc contextualisation with LLMs (GPT-4 or LLaMA). We assess a diversity of models, and we find that while the choice of contextualisation model is a significant factor for the two-stage pipelines, this is not the case in the LMMs, where smaller, open-source models perform well compared to proprietary, GPT-powered ones. Additionally, we found that controlling the amount of provided context enhances performance. These results highlight the limitations of a fully automated approach and underscore the necessity for an interactive, human-in-the-loop strategy.},
    month = {8},
    year = {2024},
    title = {Enhancing Journalism with AI: A Study of Contextualized Image Captioning for News Articles using LLMs and LMMs},
    howpublished = {Trustworthy Interactive Decision-Making with Foundation Models Workshop @ IJCAI 2024},
    author = {Aliki Anagnostopoulou and Thiago Gouvea and Daniel Sonntag},
    keywords = {contextualised image captioning, foundation models, large language models, large multimodal models, AI in journalism},
    url = {https://openreview.net/forum?id=L6OosgRO0K https://www.dfki.de/fileadmin/user_upload/import/15145_Enhancing_Journalism_with_AI.pdf}
}

@inproceedings{pub15147,
    series = {GoodIT '24},
    abstract = {For dialogues in which teachers explain difficult concepts to students, didactics research often debates which teaching strategies lead to the best learning outcome. In this paper, we test if LLMs can reliably annotate such explanation dialogues, s.t. they could assist in lesson planning and tutoring systems. We first create a new annotation scheme of teaching acts aligned with contemporary teaching models and re-annotate a dataset of conversational explanations about communicating scientific understanding in teacher-student settings on five levels of the explainee’s expertise: ReWIRED contains three layers of acts (Teaching, Explanation, Dialogue) with increased granularity (span-level). We then evaluate language models on the labeling of such acts and find that the broad range and structure of the proposed labels is hard to model for LLMs such as GPT-3.5/-4 via prompting, but a fine-tuned BERT can perform both act classification and span labeling well. Finally, we operationalize a series of quality metrics for instructional explanations in the form of a test suite, finding that they match the five expertise levels well.},
    month = {9},
    year = {2024},
    title = {Towards Modeling and Evaluating Instructional Explanations in Teacher-Student Dialogues},
    booktitle = {Proceedings of the 2024 International Conference on Information Technology for Social Good. ACM International Conference on Information Technology for Social Good (GoodIT-2024), September 4-6, Bremen, Germany},
    isbn = {9798400710940},
    publisher = {Association for Computing Machinery},
    doi = {https://doi.org/10.1145/3677525.3678665},
    author = {Nils Feldhus and Aliki Anagnostopoulou and Qianli Wang and Milad Alshomary and Henning Wachsmuth and Daniel Sonntag and Sebastian Möller},
    keywords = {Dialogue, Discourse Analysis, Evaluation, Explanations},
    url = {https://doi.org/10.1145/3677525.3678665}
}

@article{pub15156,
    abstract = {Passive Acoustic Monitoring (PAM) has emerged as a pivotal technology for wildlife monitoring, generating vast amounts of acoustic data. However, the successful application of machine learning methods for sound event detection in PAM datasets heavily relies on the availability of annotated data, which can be laborious to acquire. In this study, we investigate the effectiveness of transfer learning and active learning techniques to address the data annotation challenge in PAM. Transfer learning allows us to use pre-trained models from related tasks or datasets to bootstrap the learning process for sound event detection. Furthermore, active learning promises strategic selection of the most informative samples for annotation, effectively reducing the annotation cost and improving model performance. We evaluate an approach that combines transfer learning and active learning to efficiently exploit existing annotated data and optimize the annotation process for PAM datasets. Our transfer learning observations show that embeddings produced by BirdNet, a model trained on high signal-to-noise recordings of bird vocalisations, can be effectively used for predicting anurans in PAM data: a linear classifier constructed using these embeddings outperforms the benchmark by 21.7%. Our results indicate that active learning is superior to random sampling, although no clear winner emerges among the strategies employed. The proposed method holds promise for facilitating broader adoption of machine learning techniques in PAM and advancing our understanding of biodiversity dynamics through acoustic data analysis.},
    year = {2024},
    title = {Leveraging transfer learning and active learning for data annotation in passive acoustic monitoring of wildlife},
    journal = {Ecological Informatics},
    volume = {82},
    pages = {1-9},
    publisher = {Elsevier},
    doi = {https://doi.org/10.1016/j.ecoinf.2024.102710},
    author = {Hannes Kath and Patricia P. Serafini and Ivan B. Campos and Thiago Gouvea and Daniel Sonntag},
    keywords = {Passive acoustic monitoring, Active learning, Transfer learning, BirdNet},
    url = {https://www.sciencedirect.com/science/article/pii/S1574954124002528 https://www.dfki.de/fileadmin/user_upload/import/15156_Kath_et_al_2024_Leveraging_transfer_learning_and_active_learning_for_data_annotation_in_passive.pdf}
}

@inproceedings{pub15162,
    abstract = {The complex system of life on Earth, biodiversity, provides the essential resources for human survival. However, humanity is the primary driver of species extinction, accelerating the extinction rate to 100-1000 times higher than pre-industrial times. To combat this alarming trend, detailed information on biodiversity is needed, making effective monitoring technologies essential. Passive Acoustic Monitoring (PAM) has emerged as a key technology for scalable wildlife monitoring. While PAM is effectively used to record vast amounts of acoustic data, the automatic identification of species remains an unsolved challenge. This elaboration formally describes the problem of identifying as many species as possible in an unlabelled PAM dataset while examining the fewest samples. A pilot study is conducted to investigate the potential of four approaches combining transfer learning with adapted uncertainty or diversity active learning sampling strategies. The findings of this study indicate that uncertainty-based sampling strategies yield superior performance to random sampling. In contrast, the diversity-based strategies used demonstrate inferior performance, with improvements observed when fine-tuning the embedding space using already labelled data. This study lays the groundwork for future research aimed at iteratively fine-tuning the embedding space in combination with uncertainty and diversity methods.},
    year = {2024},
    title = {Active and Transfer Learning for Efficient Identification of Species in Multi-Label Bioacoustic Datasets},
    booktitle = {Proceedings of the 2024 International Conference on Information Technology for Social Good. ACM International Conference on Information Technology for Social Good (GoodIT-2024), September 4-6, Bremen, Germany},
    pages = {22-25},
    isbn = {9798400710940},
    publisher = {ACM},
    doi = {https://doi.org/10.1145/3677525.3678635},
    author = {Hannes Kath and Thiago Gouvea and Daniel Sonntag},
    url = {https://dl.acm.org/doi/10.1145/3677525.3678635}
}

@inproceedings{pub15196,
    abstract = {This paper explores the integration of graph knowledge from linguistic ontologies into multilingual Large Language Models (LLMs) using adapters to improve performance for low-resource languages (LRLs) in sentiment analysis (SA) and named entity recognition (NER). Building upon successful parameter-efficient fine-tuning techniques, such as K-ADAPTER and MAD-X, we propose a similar approach for incorporating knowledge from multilingual graphs, connecting concepts in various languages with each other through linguistic relationships, into multilingual LLMs for LRLs. Specifically, we focus on eight LRLs --- Maltese, Bulgarian, Indonesian, Nepali, Javanese, Uyghur, Tibetan, and Sinhala --- and employ language-specific adapters fine-tuned on data extracted from the language-specific section of ConceptNet, aiming to enable knowledge transfer across the languages covered by the knowledge graph. We compare various fine-tuning objectives, including standard Masked Language Modeling (MLM), MLM with full-word masking, and MLM with targeted masking, to analyze their effectiveness in learning and integrating the extracted graph data. Through empirical evaluation on language-specific tasks, we assess how structured graph knowledge affects the performance of multilingual LLMs for LRLs in SA and NER, providing insights into the potential benefits of adapting language models for low-resource scenarios.},
    year = {2024},
    title = {Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters},
    booktitle = {Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024). Workshop on Knowledge Graphs and Large Language Models (KaLLM-2024), August 15, Bangkok, Thailand},
    editor = {Russa Biswas and Lucie-Aimée Kaffee and Oshin Agarwal and Pasquale Minervini and Sameer Singh and Gerard de Melo},
    pages = {63-74},
    publisher = {Association for Computational Linguistics},
    author = {Daniil Gurgurov and Mareike Hartmann and Simon Ostermann},
    url = {https://aclanthology.org/2024.kallm-1.7 https://www.dfki.de/fileadmin/user_upload/import/15196_2024.kallm-1.7.pdf}
}

@article{pub15260,
    abstract = {Active learning (AL) algorithms are increasingly being used to train models with limited data for annotation tasks. However, the selection of data for AL is a complex issue due to the restricted information on unseen data. To tackle this problem, a technique we refer to as Partial Image Active Annotation (PIAA) employs the edge information of unseen images as prior knowledge to gauge uncertainty. This uncertainty is determined by examining the divergence and entropy in model predictions across edges. The resulting measure is then applied to choose superpixels from input images for active annotation. We demonstrate the effectiveness of PIAA in multi-class Optical Coherence Tomography (OCT) segmentation tasks, attaining a Dice score comparable to state-of-the-art OCT segmentation algorithms trained with extensive annotated data. Concurrently, we successfully reduce annotation label costs to 12%, 2.3%, and 3%, respectively, across three publicly accessible datasets (Duke, AROI, and UMN).},
    month = {6},
    year = {2024},
    title = {Partial Image Active Annotation (PIAA): An Efficient Active Learning Technique Using Edge Information in Limited Data Scenarios},
    journal = {KI - Künstliche Intelligenz, German Journal on Artificial Intelligence - Organ des Fachbereiches "Künstliche Intelligenz" der Gesellschaft für Informatik e.V. (KI)},
    volume = {-},
    pages = {1-12},
    publisher = {Springer},
    doi = {https://doi.org/10.1007/s13218-024-00849-6},
    author = {Md Abdul Kadir and Hasan Md Tusfiqur Alam and Devansh Srivastav and Hans-Jürgen Profitlich and Daniel Sonntag},
    url = {https://link.springer.com/article/10.1007/s13218-024-00849-6}
}

@misc{pub15261,
    abstract = {Explainable Artificial Intelligence (XAI) strategies play a crucial part in increasing the understanding and trustworthiness of neural networks. Nonetheless, these techniques could potentially generate misleading explanations. Blinding attacks can drastically alter a machine learning algorithm's prediction and explanation, providing misleading information by adding visually unnoticeable artifacts into the input, while maintaining the model's accuracy. It poses a serious challenge in ensuring the reliability of XAI methods. To ensure the reliability of XAI methods poses a real challenge, we leverage statistical analysis to highlight the changes in CNN weights within a CNN following blinding attacks. We introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase, avoiding the need for extra training. The method we suggest defences against most modern explanation-aware adversarial attacks, achieving an approximate decrease of ~99\% in the Attack Success Rate (ASR) and a ~91\% reduction in the Mean Square Error (MSE) between the original explanation and the defended (post-attack) explanation across three unique types of attacks.},
    month = {3},
    year = {2024},
    title = {Revealing Vulnerabilities of Neural Networks in Parameter Learning and Defense Against Explanation-Aware Backdoors},
    author = {Md Abdul Kadir and Gowthamkrishna Addluri and Daniel Sonntag},
    status_notes = {preprint},
    url = {https://arxiv.org/abs/2403.16569 https://www.dfki.de/fileadmin/user_upload/import/15261_2403.16569v1.pdf}
}

@inproceedings{pub15331,
    abstract = {Paper and digital forms are widely used to collect user information across multiple domains, such as research, healthcare, and education. However, both types still lack application and follow-up interpretation: Paper forms need to be digitised meticulously to be analyzed or shared with team members efficiently; in comparison, digital forms cannot convey handwriting and might require technical literacy. We present the FormTwin data collection tool—an alternative to online forms, which allows for the efficient reuse of existent paper forms while providing the convenience of digital forms. FormTwin can digitise a wide range of forms with the integrated form annotation application. Then, it combines two input channels: A stylus on a tablet and a digital smart pen on plain paper, which duplicates the input on a mobile application in real-time. We aim to improve the efficiency and accessibility of data collection for practitioners with a modular system that combines both the digital accessibility of digital forms with keeping the needed technical literacy low and retaining the quality of hand-drawn sketches.},
    month = {7},
    year = {2024},
    title = {FormTwin: A Framework for Pen-based Data Collection},
    booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization. International Conference on User Modeling, Adaptation, and Personalization (UMAP-2024), July 1-4, Cagliari, Italy},
    pages = {132-135},
    isbn = {979-8-4007-0466-6/24/07},
    address = {1601 Broadway, 10th Floor
New York, NY 10019-7434},
    publisher = {ACM Digital Library},
    doi = {https://doi.org/10.1145/3631700.3664875},
    author = {Konstantin Kuznetsov and Sara-Jane Bittner and Abdulrahman Mohamed Selim and Michael Barz and Daniel Sonntag},
    keywords = {Digital Pen, Digital Forms, Data Collection Methods, Research Tools, Intelligent User Interfaces},
    organization = {Association for Computing Machinery (ACM)},
    url = {https://doi.org/10.1145/3631700.3664875 https://www.dfki.de/fileadmin/user_upload/import/15331_3631700.3664875.pdf}
}

@techreport{pub15382,
    series = {DFKI Technical Report},
    abstract = {This DFKI technical report presents the anatomy of the No-IDLE meets ChatGPT prototype system (funded by the German Federal
Ministry of Education and Research) that provides not only basic and fundamental research in interactive machine learning, but
also reveals deeper insights into how to leverage the opportunities arising from large language models and technologies for the
No-IDLE project. No-IDLE’s goals and scientific challenges centre around the desire to increase the reach of interactive deep learning
solutions for non-experts in machine learning. No-IDLE aims to enhance the interaction between humans and machines for the
purpose of updating deep learning models, integrating cutting-edge human-computer interaction techniques and advanced deep
learning approaches. Considering the recent advances in LLMs and their multimodal capabilities, the overall objective of "No-IDLE
meets ChatGPT" should be well motivated. One of the key innovations described in this technical report is a methodology including
benchmark studies for interactive machine learning combined with LLMs which will become central when we start interacting with
semi-intelligent machines based on optimisation methods like automatic prompt engineering or natural language inference. Our main
research question is how ChatGPT and other variants can help improve the accuracy of (semi-) automatic subtasks in image retrieval,
captioning, and person/scene recognition.},
    year = {2024},
    title = {The Interactive Deep Learning Enterprise (No-IDLE) meets ChatGPT},
    volume = {-},
    institution = {German Research Center for AI},
    author = {Daniel Sonntag and Thiago Gouvea and Michael Barz and Aliki Anagnostopoulou and Siting Liang and Sara-Jane Bittner and Franziska Scheurer},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15382_No_IDLE_meets_ChatGPT_Technical_Report.pdf}
}

@inproceedings{pub15402,
    series = {ICMI Companion '24},
    abstract = {The pervasive integration of artificial intelligence (AI) into daily life has led to a growing interest in AI agents that can learn continuously. Interactive Machine Learning (IML) has emerged as a promising approach to meet this need, essentially involving human experts in the model training process, often through iterative user feedback. However, repeated feedback requests can lead to frustration and reduced trust in the system. Hence, there is increasing interest in refining how these systems interact with users to ensure efficiency without compromising user experience. Our research investigates the potential of eye tracking data as an implicit feedback mechanism to detect user disagreement with AI-generated captions in image captioning systems. We conducted a study with 30 participants using a simulated captioning interface and gathered their eye movement data as they assessed caption accuracy. The goal of the study was to determine whether eye tracking data can predict user agreement or disagreement effectively, thereby strengthening IML frameworks. Our findings reveal that, while eye tracking shows promise as a valuable feedback source, ensuring consistent and reliable model performance across diverse users remains a challenge.},
    year = {2024},
    title = {Detecting when Users Disagree with Generated Captions},
    booktitle = {Companion Proceedings of the 26th International Conference on Multimodal Interaction. ACM International Conference on Multimodal Interaction (ICMI-2024), November 4, San José, Costa Rica},
    note = {HumanEYEze Workshop},
    pages = {195-203},
    isbn = {9798400704635},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {https://doi.org/10.1145/3686215.3688382},
    author = {Omair Shahzad Bhatti and Harshinee Sriram and Abdulrahman Mohamed Selim and Cristina Conati and Michael Barz and Daniel Sonntag},
    keywords = {disagreement detection, emotion detection, eye tracking, gaze, interactive machine learning, user disagreement},
    url = {https://doi.org/10.1145/3686215.3688382 https://www.dfki.de/fileadmin/user_upload/import/15402_3686215.3688382.pdf}
}

@inproceedings{pub15403,
    series = {ICMI '24},
    abstract = {The HumanEYEze 2024 workshop aims to explore the role of eye tracking in developing human-centered multimodal AI systems. Over the past two decades, eye tracking has evolved from a diagnostic tool to an important input modality for real-time interactive systems, driven by advancements in hardware that have improved its affordability, availability, and performance. Initially used in specialized applications, eye tracking now significantly impacts research on gaze-based multimodal interaction. Recently, eye-based user and context modeling has emerged, utilizing eye movements to provide rich insights into user behavior and interaction contexts. The workshop aims to bring together researchers from eye tracking, multimodal human-computer interaction, and AI. It aims to enhance understanding of integrating eye tracking into multimodal human-centered computing. The expected outcomes include fostering collaborations and promoting knowledge exchange.},
    year = {2024},
    title = {HumanEYEze 2024: Workshop on Eye Tracking for Multimodal Human-Centric Computing},
    booktitle = {Proceedings of the 26th International Conference on Multimodal Interaction. ACM International Conference on Multimodal Interaction (ICMI-2024), November 4-8, San José, Costa Rica},
    isbn = {9798400704628},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {https://doi.org/10.1145/3678957.3688384},
    author = {Michael Barz and Roman Bednarik and Andreas Bulling and Cristina Conati and Daniel Sonntag},
    keywords = {Eye Tracking, Gaze, Human-centered AI, Human-centric Computing, Multimodal Interaction, User Modeling, Workshop},
    url = {https://doi.org/10.1145/3678957.3688384 https://www.dfki.de/fileadmin/user_upload/import/15403_humaneyeze.pdf}
}

@inproceedings{pub15407,
    series = {ICMI '24},
    abstract = {A scanpath is an important concept in eye tracking that represents a person’s eye movements in a graph-like structure. Passive gaze-based interfaces, in which users do not consciously interact using their eyes, typically interpret users’ scanpaths to enable adaptive and personalised interaction. Despite the benefits of graph neural networks (GNNs) in graph processing, this technology has not been considered for that purpose. An example application is perceived relevance estimation, which still suffers from low classification performance. In this work, we investigate how and whether GNNs can be used to analyse scanpaths for readers’ perceived relevance estimation using the gazeRE dataset. This dataset contains eye tracking data from 24 participants, who rated the relevance of 12 short and 12 long documents in relation to a given query. The relevance was assigned either to an entire short document or to each paragraph within a long document, which allowed us to investigate two different GNN tasks. For comparison, we reproduced the gazeRE baseline using Random Forest and Support Vector classifiers, and an additional Convolutional Neural Network (CNN) from the literature. All models were evaluated using leave-users-out cross-validation. For short documents, the GNNs surpassed the baseline methods, with certain experiments showing an absolute balanced accuracy improvement of 7.6% and 14.3% over the CNN and gazeRE baselines, respectively. However, similar improvements were not observed in long documents. This work investigates and discusses the future potential of using GNNs as a scanpath analysis method for passive gaze-based applications, such as implicit relevance estimation.},
    month = {11},
    year = {2024},
    title = {Perceived Text Relevance Estimation Using Scanpaths and GNNs},
    booktitle = {Proceedings of the 26th International Conference on Multimodal Interaction. ACM International Conference on Multimodal Interaction (ICMI-2024), November 4-8, San Jose, Costa Rica},
    pages = {418-427},
    isbn = {9798400704628},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {https://doi.org/10.1145/3678957.3685736},
    author = {Abdulrahman Mohamed Selim and Omair Shahzad Bhatti and Michael Barz and Daniel Sonntag},
    keywords = {Eye Tracking, GNN, Passive Gaze-based Application, Scanpath},
    url = {https://doi.org/10.1145/3678957.3685736 https://www.dfki.de/fileadmin/user_upload/import/15407_3678957.3685736.pdf}
}

@inproceedings{pub15412,
    abstract = {With the integration of eye tracking technologies in Augmented Reality (AR) and Virtual Reality (VR) headsets, gaze-based interactions have opened up new possibilities for user interface design, including menu navigation. Prior research in gaze-based menu navigation in VR has predominantly focused on pie menus, yet recent studies indicate a user preference for list layouts. However, the comparison of gaze-based interactions on list menus is lacking in the literature. This work aims to fill this gap by exploring the viability of list menus for multi-level gaze-based menu navigation in VR and evaluating the efficiency of various gaze-based interactions, such as dwelling and border-crossing, against traditional controller navigation and multi-modal interaction using gaze and button press.},
    number = {40},
    month = {10},
    year = {2024},
    title = {Exploring Gaze-Based Menu Navigation in Virtual Environments},
    booktitle = {Proceedings of the 2024 ACM Symposium on Spatial User Interaction (SUI '24). ACM Symposium on Spatial User Interaction (SUI-2024), October 7-8, Trier, Germany},
    pages = {1-2},
    isbn = {9798400710889},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {https://doi.org/10.1145/3677386.3688887},
    author = {László Kopácsi and Albert Klimenko and Michael Barz and Daniel Sonntag},
    keywords = {Extended Reality (XR), Eye Tracking, Gaze-based Interaction, Menu Navigation},
    url = {https://dl.acm.org/doi/10.1145/3677386.3688887 https://www.dfki.de/fileadmin/user_upload/import/15412_3677386.3688887.pdf}
}

@inproceedings{pub15413,
    abstract = {The MASTER project introduces an open Extended Reality (XR) platform designed to enhance human-robot collaboration and train workers in robotics within manufacturing settings. It includes modules for creating safe workspaces, intuitive robot programming, and user-friendly human-robot interactions (HRI), including eye-tracking technologies. The development of the platform is supported by two open calls targeting technical SMEs and educational institutes to enhance and test its functionalities. By employing the learning-by-doing methodology and integrating effective teaching principles, the MASTER platform aims to provide a comprehensive learning environment, preparing students and professionals for the complexities of flexible and collaborative manufacturing settings.},
    number = {67},
    month = {10},
    year = {2024},
    title = {The MASTER XR Platform for Robotics Training in Manufacturing},
    booktitle = {Proceedings of the 30th ACM Symposium on Virtual Reality Software and Technology (VRST '24). ACM Symposium on Virtual Reality Software and Technology (VRST-2024), October 9-11, Trier, Germany},
    pages = {1-2},
    isbn = {9798400705359},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {https://doi.org/10.1145/3641825.3689514},
    author = {László Kopácsi and Panagiotis Karagiannis and Sotiris Makris and Johan Kildal and Andoni Rivera-Pinto and Judit Ruiz de Munain and Jesús Rosel and Maria Madarieta and Nikolaos Tseregkounis and Konstantina Salagianni and Panagiotis Aivaliotis and Michael Barz and Daniel Sonntag},
    keywords = {Extended Reality (XR), Eye Tracking, Human-Robot Collaboration, Industry 4.0, Manufacturing, Robotics, Worker Training},
    url = {https://doi.org/10.1145/3641825.3689514 https://www.dfki.de/fileadmin/user_upload/import/15413_3641825.3689514.pdf}
}

@inproceedings{pub15414,
    abstract = {Password entry is common authentication approach in Extended Reality (XR) applications for its simplicity and familiarity, but it faces challenges in public and dynamic environments due to its cumbersome nature and susceptibility to observation attacks. Manual password input can be disruptive and prone to theft through shoulder surfing or surveillance. While alternative knowledge-based approaches exist, they often require complex physical gestures and are impractical for frequent public use. We present GazeLock, an eye-tracking and lock pattern-based authentication method. This method aims to provide an easy-to-learn and efficient alternative by leveraging familiar lock patterns operated through gaze. It ensures resilience to external observation, as physical interaction is unnecessary and eyes are obscured by the headset. Its hands-free, discreet nature makes it suitable for secure public use. We demonstrate this method by simulating the unlocking of a smart lock via an XR headset, showcasing its potential applications and benefits in real-world scenarios.},
    number = {94},
    month = {10},
    year = {2024},
    title = {GazeLock: Gaze- and Lock Pattern-Based Authentication},
    booktitle = {Proceedings of the 30th ACM Symposium on Virtual Reality Software and Technology (VRST '24). ACM Symposium on Virtual Reality Software and Technology (VRST-2024), October 9-11, Trier, Germany},
    pages = {1-2},
    isbn = {9798400705359},
    publisher = {Association for Computing Machinery, New York, NY, USA},
    doi = {https://doi.org/10.1145/3641825.3689520},
    author = {László Kopácsi and Tobias Sebastian Schneider and Chiara Karr and Michael Barz and Daniel Sonntag},
    keywords = {Authentication, Extended Reality (XR), Eye Tracking, Gaze-based Interaction},
    url = {https://doi.org/10.1145/3641825.3689520 https://www.dfki.de/fileadmin/user_upload/import/15414_3641825.3689520.pdf}
}

@inproceedings{pub15416,
    abstract = {Biodiversity loss is a major challenge for humanity, which has increased the rate of species extinction by a factor of 100-1000 compared to pre-industrial times.
XPRIZE Rainforest is a competition focused on developing a pipeline for real-time biodiversity measurement: teams have 24 hours to collect data and another 48 hours to produce a list of species present in the data.
Passive acoustic monitoring (PAM) is a scalable technology for data acquisition in wildlife monitoring.
However, analyzing large PAM datasets poses a significant challenge. 
This paper presents a tool used by the Brazilian team during the XPRIZE Rainforest finals.
Using a combination of audio separation, weakly supervised learning, transfer learning, active learning, multiple-instance learning, and novel class detection, samples are carefully selected and presented to the user for annotation.},
    year = {2024},
    title = {Enhancing Biodiversity Monitoring: An Interactive Tool for Efficient Identification of Species in Large Bioacoustics Datasets},
    booktitle = {ICMI Companion '24: Companion Proceedings of the 26th International Conference on Multimodal Interaction. ACM International Conference on Multimodal Interaction (ICMI-2024), 26th International Conference on Multimodal Interaction, November 4-8, Costa Rica},
    pages = {91-93},
    publisher = {Association for Computing Machinery},
    doi = {https://doi.org/10.1145/3686215.3688374},
    author = {Hannes Kath and Ilira Troshani and Bengt Lüers and Thiago Gouvea and Daniel Sonntag},
    keywords = {passive acoustic monitoring, novel class detection, transfer learning, active learning},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15416_ICMI_Demo_Paper.pdf}
}

@techreport{pub15459,
    series = {DFKI Research Reports, RR},
    abstract = {The emergence of artificial intelligence (AI), particularly Deep Learning (DL), has marked a new era in the realm of ophthalmology, offering the transformative potential for the diagnosis and treatment of posterior segment eye diseases. This review explores the cutting-edge applications of DL across a range of ocular conditions, including diabetic retinopathy, glaucoma, age-related macular degeneration, and retinal vessel segmentation. We provide a comprehensive overview of foundational machine learning techniques and advanced DL architectures, such as convolutional neural networks, attention mechanisms, and transformer-based models, highlighting the evolving role of AI in enhancing diagnostic accuracy, optimizing treatment strategies, and improving overall patient care. Additionally, we present key challenges in integrating AI solutions into clinical practice, including ensuring data diversity, improving algorithm transparency, and effectively leveraging multimodal data. This review emphasizes AI's potential to improve disease diagnosis and enhance patient care while stressing the importance of collaborative efforts to overcome these barriers and fully harness AI's impact in advancing eye care.},
    year = {2024},
    title = {Deep Learning for Ophthalmology - The State-of-the-Art and Future Trends},
    volume = {01},
    institution = {DFKI},
    author = {Ho Minh Duy Nguyen and Hasan Md Tusfiqur Alam and Tai Nguyen and Devansh Srivastav and Hans-Jürgen Profitlich and Ngan Le and Daniel Sonntag},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15459_2501.04073v1.pdf}
}

@inproceedings{pub15474,
    abstract = {Brain-Computer Interface (BCI) systems represent an innovative approach to human-computer interaction, enabling users to control devices and interact with technology solely through brain activity. This study investigates the feasibility and potential of non-invasive EEG-based BCI for elevator control, addressing two primary research questions: 1) Can a person reliably control an elevator through a BCI system? and 2) What are the usability and user experience outcomes of such a system? We integrated a Muse headset with a remote-controllable elevator system using an iPhone as the interface over a local network. This setup allowed users to operate the elevator using blinking, jaw clenching, and mental focussing as triggers. Performance, accuracy, and user experience were evaluated through experiments involving 50 participants aged 12 to 60. Usability was measured with the System Usability Scale (SUS) questionnaire along with additional feedback questions. Key findings indicate that the system achieved an average SUS score of 80.3, reflecting excellent usability on the adjective rating scale. Moreover, 94% of participants successfully controlled the elevator, performing tasks such as activating and deactivating brain control, calling the elevator, and selecting floors. The user experience questionnaires reveal that most participants found the system easy to use, well-integrated, and perceived the introduction of brain-controlled elevators to positively impact accessibility and inclusivity in buildings.},
    year = {2024},
    title = {Mindful Mobility: EEG-Based Brain-Computer Interaction for Elevator Control Using Muse Headset},
    booktitle = {Proceedings of the International Conference on Ubiquitous Computing and Ambient Intelligence (UCAmI 2024). International Conference on Ubiquitous Computing and Ambient Intelligence (UCAmI-2024), November 27-30, Ulster University, Belfast, United Kingdom},
    publisher = {Springer},
    author = {Devansh Srivastav and Thomas Kaltbach and Ahmer Akhtar Mughal and Nischal Giriyan and Moaz Bin Younus and Tobias Jungbluth and Jochen Britz and Jan Alexandersson and Maurice Rekrut},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15474_Mindful_Mobility.pdf}
}

@inproceedings{pub15617,
    abstract = {A molecule's 2D representation consists of its atoms, their attributes, and the molecule's covalent bonds. A 3D (geometric) representation of a molecule is called a conformer and consists of its atom types and Cartesian coordinates. Every conformer has a potential energy, and the lower this energy, the more likely it occurs in nature. Most existing machine learning methods for molecular property prediction consider either 2D molecular graphs or 3D conformer structure representations in isolation. Inspired by recent work on using ensembles of conformers in conjunction with 2D graph representations, we propose E(3)-invariant molecular conformer aggregation networks. The method integrates a molecule's 2D representation with that of multiple of its conformers. Contrary to prior work, we propose a novel 2D-3D aggregation mechanism based on a differentiable solver for the Fused Gromov-Wasserstein Barycenter problem and the use of an efficient conformer generation method based on distance geometry. We show that the proposed aggregation mechanism is E(3) invariant and propose an efficient GPU implementation. Moreover, we demonstrate that the aggregation mechanism helps to significantly outperform state-of-the-art molecule property prediction methods on established datasets.},
    year = {2024},
    title = {Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks},
    booktitle = {Proceedings of the 41st International Conference on Machine Learning. International Conference on Machine Learning (ICML), July 21-27, Austria},
    publisher = {JMLR.org},
    author = {Ho Minh Duy Nguyen and Nina Lukashina and Tai Nguyen and An T. Le and TrungTin Nguyen and Nhat Ho and Jan Peters and Daniel Sonntag and Viktor Zaverkin and Mathias Niepert},
    url = {https://arxiv.org/pdf/2402.01975}
}

@inproceedings{pub15618,
    abstract = {Model compression has been an active research field to reduce the size and complexity of the model. In a recent noteworthy study, ToMe and its variants utilize the Bipartite Soft Matching (BSM) algorithm in which tokens representing patches in an image are split into two sets, and top k similar tokens from one set are merged. This approach not only utilizes pre-trained weights but also enhances speed and reduces memory usage. However, this algorithm has some drawbacks. The choice of a token-splitting strategy significantly influences the algorithm’s performance since tokens in one set can only perceive tokens in the other set, leading to mis-merging issues. Furthermore, although ToMe is effective in the initial layers, it becomes increasingly problematic in deeper layers as the number of tokens diminishes because of damaged informative tokens. To address these limitations, rather than relying on specific splitting strategies like BSM, we propose a new algorithm called PiToMe. Specifically, we prioritize the protection of informative tokens using an additional factor called the energy score. In experiments, PiToMe achieved up to a 50% memory reduction while exhibiting superior off-the-shelf performance on image classification ( keeping 1.71% average performance drop compared to 2.6% for ToMe) and image-text retrieval (1.35% average performance drop compared to 6.89% for ToMe) compared to ToMe and ToMe-based approaches dependent solely on token similarity.},
    year = {2024},
    title = {Energy Minimizing-based Token Merging for Accelerating Transformers},
    booktitle = {International Conference on Learning Representations (ICLR), Practical ML for Low Resource Settings Workshop. International Conference on Learning Representations (ICLR), May 11-12},
    publisher = {JMLR.org},
    author = {Hoai-Chau Tran and Ho Minh Duy Nguyen and Manh-Duy Nguyen and Ngan Hoang Le and Binh T. Nguyen},
    url = {https://www.dfki.de/fileadmin/user_upload/import/15618_84_Energy_Minimizing_based_tok.pdf}
}

@inproceedings{pub15619,
    abstract = {Increasing the throughput of the Transformer architecture, a foundational component used in numerous state-of-the-art models for vision and language tasks
(e.g., GPT, LLaVa), is an important problem in machine learning. One recent and effective strategy is to merge token representations within Transformer models,
aiming to reduce computational and memory requirements while maintaining accuracy. Prior works have proposed algorithms based on Bipartite Soft Matching
(BSM), which divides tokens into distinct sets and merges the top k similar tokens. However, these methods have significant drawbacks, such as sensitivity to token splitting strategies and damage to informative tokens in later layers. This paper presents a novel paradigm called PITOME, which prioritizes the preservation of informative tokens using an additional metric termed the energy score. This score identifies large clusters of similar tokens as high-energy, indicating potential candidates for merging, while smaller (unique and isolated) clusters are considered as low-energy and preserved. Experimental findings demonstrate that PITOME saved from 40-60% FLOPs of the base models while exhibiting superior off-the-shelf performance on image classification (0.5% average performance drop of ViT-MAEH compared to 2.6% as baselines), image-text retrieval (0.3% average performance drop of CLIP on Flickr30k compared to 4.5% as others), and analogously in visual questions answering with LLaVa-7B. Furthermore, PITOME is theoretically shown to preserve intrinsic spectral properties to the original token space under mild conditions.},
    year = {2024},
    title = {Accelerating Transformers with Spectrum-Preserving Token Merging},
    booktitle = {The Thirty-Eighth Annual Conference on Neural Information Processing Systems. Neural Information Processing Systems (NeurIPS-2024), December 10-15, Canada},
    publisher = {JMLR.org},
    author = {Hoai-Chau Tran and Ho Minh Duy Nguyen and Duy M. Nguyen and TrungTin Nguyen and Ngan Le and Pengtao Xie and Daniel Sonntag and James Zou and Binh T. Nguyen and Mathias Niepert},
    url = {https://arxiv.org/pdf/2405.16148}
}

@inbook{pub14630,
    series = {Lecture Notes in Computer Science},
    abstract = {Enhancing the interpretability and consistency of machine learning models is critical to their deployment in real-world applications. Feature attribution methods have gained significant attention, which provide local explanations of model predictions by attributing importance to individual input features. This study examines the generalization of feature attributions across various deep learning architectures, such as convolutional neural networks (CNNs) and vision transformers. We aim to assess the feasibility of utilizing a feature attribution method as a future detector and examine how these features can be harmonized across multiple models employing distinct architectures but trained on the same data distribution. By exploring this harmonization, we aim to develop a more coherent and optimistic understanding of feature attributions, enhancing the consistency of local explanations across diverse deep-learning models. Our findings highlight the potential for harmonized feature attribution methods to improve interpretability and foster trust in machine learning applications, regardless of the underlying architecture.},
    year = {2023},
    title = {Harmonizing Feature Attributions Across Deep Learning Architectures: Enhancing Interpretability and Consistency},
    booktitle = {Dietmar Seipel; Alexander Steen;: German Conference on Artificial Intelligence},
    pages = {90-97},
    isbn = {978-3-031-42607-0},
    publisher = {Springer, Cham},
    author = {Md Abdul Kadir and Gowthamkrishna Addluri and Daniel Sonntag},
    url = {https://rdcu.be/dvrE0 https://www.dfki.de/fileadmin/user_upload/import/14630_Harmonizing_Feature_Attributions_Across_Deep_Learning_Architectures__Enhancing_Interpretability_and_Consistency_KI2023.pdf}
}

@inproceedings{pub14635,
    abstract = {Active learning algorithms have become increasingly popular for training models with limited data. However, selecting data for annotation remains a challenging problem due to the limited information available on unseen data. To address this issue, we propose EdgeAL, which utilizes the edge information of unseen images as a priori information for measuring uncertainty. The uncertainty is quantified by analyzing the divergence and entropy in model predictions across edges. This measure is then used to select superpixels for annotation. We demonstrate the effectiveness of EdgeAL on multi-class Optical Coherence Tomography (OCT) segmentation tasks, where we achieved a 99% dice score while reducing the annotation label cost to 12%, 2.3%, and 3%, respectively, on three publicly available datasets (Duke, AROI, and UMN). The source code is available at https://github.com/Mak-Ta-Reque/EdgeAL.},
    year = {2023},
    title = {EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation},
    booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention. Medical Image Computing and Computer Assisted Intervention (MICCAI-2023)},
    pages = {79-89},
    publisher = {Springer, Cham},
    author = {Md Abdul Kadir and Hasan Md Tusfiqur Alam and Daniel Sonntag},
    organization = {Springer},
    url = {https://rdcu.be/dvx8f https://www.dfki.de/fileadmin/user_upload/import/14635_paper1593.pdf}
}

@inproceedings{pub14648,
    series = {Lecture Notes in Artificial Intelligence, LNAI},
    abstract = {Dialogue systems are an important and very active research area with many practical applications. However, researchers and practitioners new to the field may have difficulty with the categorisation, number and terminology of existing free and commercial systems. Our paper aims to achieve two main objectives. Firstly, based on our structured literature review, we provide a categorisation of dialogue systems according to the objective, modality, domain, architecture, and model, and provide information on the correlations among these categories. Secondly, we summarise and compare frameworks and applications of intelligent virtual assistants, commercial frameworks, research dialogue systems, and large language models according to these categories and provide system recommendations for researchers new to the field.},
    month = {9},
    year = {2023},
    title = {Lost in Dialogue: A Review and Categorisation of Current Dialogue System Approaches and Technical Solutions},
    booktitle = {KI 2023: Advances in Artificial Intelligence. German Conference on Artificial Intelligence (KI-2023), 46th German Conference on AI, Berlin, Germany, September 26–29, 2023, Proceedings, located at 46th German Conference on AI, September 26-29, Berlin, Germany},
    editor = {Dietmar Seipel and Alexander Steen},
    volume = {14236},
    pages = {98-113},
    isbn = {978-3-031-42607-0},
    publisher = {Springer, Heidelberg},
    doi = {https://doi.org/10.1007/978-3-031-42608-7_9},
    author = {Hannes Kath and Bengt Lüers and Thiago S. Gouvêa and Daniel Sonntag},
    keywords = {Dialogue System, Conversational AI, Task-oriented, Natural Language Processing, Survey},
    url = {https://link.springer.com/chapter/10.1007/978-3-031-42608-7_9 https://www.dfki.de/fileadmin/user_upload/import/14648_978-3-031-42608-7-seiten-2.pdf}
}

@inproceedings{pub14666,
    abstract = {Evolving software with an increasing number of features is harder to understand and thus harder to use. Software release planning has been concerned with planning these additions. Moreover, software of increasing size takes more effort to be maintained. In the domain of mobile apps, too much functionality can easily impact usability, maintainability, and resource consumption. Hence, it is important to understand the extent to which the law of continuous growth applies to mobile apps. Previous work showed that the deletion of functionality is common and sometimes driven by user reviews. However, it is unknown whether these deletions are visible or important to the app users. In this study, we surveyed 297 mobile app users to understand the significance of functionality deletion for them. Our results showed that for most users, the deletion of features corresponds with negative sentiments and change in usage and even churn. Motivated by these preliminary results, we propose Radiation to input user reviews and recommend if any functionality should be deleted from an app's User Interface (UI). We evaluate Radiation using historical data and surveying developers' opinions. From the analysis of 190,062 reviews from 115 randomly selected apps, we show that Radiation can recommend functionality deletion with an average F-Score of 74% and if sufficiently many negative user reviews suggest so.},
    year = {2023},
    title = {User Driven Functionality Deletion for Mobile Apps},
    booktitle = {IEEE International Requirements Engineering Conference. IEEE International Requirements Engineering Conference (RE-2023), September 4-8, Hannover, Germany},
    isbn = {979-8-3503-2689-5},
    publisher = {IEEE},
    author = {Maleknaz Nayebi and Konstantin Kuznetsov and Andreas Zeller and Guenther Ruhe},
    keywords = {Mobile apps, Survey, App store mining, Software Release planning, Empirical software engineering},
    url = {https://ieeexplore.ieee.org/document/10260783/}
}

@article{pub14703,
    abstract = {Allocentric semantic 3D maps are highly useful for a variety of human–machine interaction related tasks since egocentric viewpoints can be derived by the machine for the human partner. Class labels and map interpretations, however, may differ or could be missing for the participants due to the different perspectives. Particularly, when considering the viewpoint of a small robot, which significantly differs from the viewpoint of a human. In order to overcome this issue, and to establish common ground, we extend an existing real-time 3D semantic reconstruction pipeline with semantic matching across human and robot viewpoints. We use deep recognition networks, which usually perform well from higher (i.e., human) viewpoints but are inferior from lower viewpoints, such as that of a small robot. We propose several approaches for acquiring semantic labels for images taken from unusual perspectives. We start with a partial 3D semantic reconstruction from the human perspective that we transfer and adapt to the small robot’s perspective using superpixel segmentation and the geometry of the surroundings. The quality of the reconstruction is evaluated in the Habitat simulator and a real environment using a robot car with an RGBD camera. We show that the proposed approach provides high-quality semantic segmentation from the robot’s perspective, with accuracy comparable to the original one. In addition, we exploit the gained information and improve the recognition performance of the deep network for the lower viewpoints and show that the small robot alone is capable of generating high-quality semantic maps for the human partner. The computations are close to real-time, so the approach enables interactive applications.},
    number = {11},
    month = {5},
    year = {2023},
    title = {Cross-Viewpoint Semantic Mapping: Integrating Human and Robot Perspectives for Improved 3D Semantic Reconstruction},
    journal = {Sensors - Open Access Journal (Sensors)},
    volume = {23},
    pages = {1-17},
    publisher = {MDPI},
    doi = {https://doi.org/10.3390/s23115126},
    author = {László Kopácsi and Benjámin Baffy and Gábor Baranyi and Joul Skaf and Gábor Sörös and Szilvia Szeier and András Lőrincz and Daniel Sonntag},
    keywords = {3D semantic maps; semantic matching; superpixel segmentation; semantic segmentation; human–robot collaboration; real-time reconstruction; label transfer; computer vision; deep learning},
    url = {https://doi.org/10.3390/s23115126 https://www.dfki.de/fileadmin/user_upload/import/14703_sensors-23-05126.pdf}
}

@inproceedings{pub14706,
    abstract = {Future healthcare ecosystems integrating human-centered artificial
intelligence (AI) will be indispensable. AI-based healthcare technologies can sup-
port diagnosis processes and make healthcare more accessible globally. In this con-
text, we conducted a design science research project intending to introduce design
principles for user interfaces (UIs) of explainable AI-based (XAI) medical deci-
sion support systems (XAI-based MDSS). We used an archaeological approach
to analyze the UI of an existing web-based system in the context of skin lesion
classification called DIAnA (Dermatological Images – Analysis and Archiving).
One of DIAnA’s unique characteristics is that it should be usable for the stake-
holder groups of physicians and patients. We conducted the in-situ analysis with
these stakeholders using the think-aloud method and semi-structured interviews.
We anchored our interview guide in concepts of the Theory of Interactive Media
Effects (TIME), which formulates UI features as causes and user psychology as
effects. Based on the results, we derived 20 design requirements and developed
nine design principles grounded in TIME for this class of XAI-based MDSS, either
associated with the needs of physicians, patients, or both. Regarding evaluation,
we first conducted semi-structured interviews with software developers to assess
the reusability of our design principles. Afterward, we conducted a survey with
user experience/interface designers. The evaluation uncovered that 77% of the
participants would adopt the design principles, and 82% would recommend them
to colleagues for a suitable project. The findings prove the reusability of the design
principles and highlight a positive perception by potential implementers.},
    month = {5},
    year = {2023},
    title = {Giving DIAnA More TIME – Guidance for the Design of XAI-Based Medical Decision Support Systems},
    booktitle = {18th International Conference on Design Science Research in Information Systems and Technology, DESRIST 2023. International Conference on Design Science Research in Information Systems and Technology (DESRIST-2023), May 31 - June 3, Pretoria, South Africa},
    publisher = {Springer Nature Switzerland},
    author = {Enrico Bunde and Daniel Eisenhardt and Daniel Sonntag and Hans-Jürgen Profitlich and Christian Meske},
    url = {https://scholar.google.de/citations?view_op=view_citation&hl=en&user=v7i6Uz4AAAAJ&sortby=pubdate&citation_for_view=v7i6Uz4AAAAJ:anf4URPfarAC https://www.dfki.de/fileadmin/user_upload/import/14706_"Giving_DIAnA_More_TIME__8211}
}

@article{pub14707,
    abstract = {o.A.},
    month = {5},
    year = {2023},
    title = {Avoid Predatory Journals},
    journal = {KI - Künstliche Intelligenz, German Journal on Artificial Intelligence - Organ des Fachbereiches "Künstliche Intelligenz" der Gesellschaft für Informatik e.V. (KI)},
    volume = {37},
    pages = {1-3},
    publisher = {Springer, Berlin Heidelberg},
    author = {Daniel Sonntag},
    url = {https://link.springer.com/article/10.1007/s13218-023-00805-w https://www.dfki.de/fileadmin/user_upload/import/14707_Avoid_Predatory_Journals.pdf}
}

@inproceedings{pub14708,
    abstract = {Within the past few years, the accuracy of deep learning and machine learning models has been improving significantly while less attention has been paid to their responsibility, explainability, and interpretability. eXplainable Artificial Intelligence (XAI) methods, guidelines, concepts, and strategies offer the possibility of models' evaluation for improving fidelity, faithfulness, and overall explainability. Due to the diversity of data and learning methodologies, there needs to be a clear definition for the validity, reliability, and evaluation metrics of explainability. This article reviews evaluation metrics used for XAI through the PRISMA systematic guideline for a comprehensive and systematic literature review. Based on the results, this study suggests two taxonomy for the evaluation metrics. One taxonomy is based on the applications, and one is based on the evaluation metrics.},
    year = {2023},
    title = {Evaluation Metrics for XAI: A Review, Taxonomy, and Practical Applications},
    booktitle = {2023 IEEE 27th International Conference on Intelligent Engineering Systems (INES). Conference on Intelligent Engineering Systems (INES-2023), July 26-28, Nairobi, Kenya},
    pages = {000111-000124},
    publisher = {IEEE},
    doi = {https://doi.org/10.1109/INES59282.2023.10297629},
    author = {Md Abdul Kadir and Amir Mosavi and Daniel Sonntag},
    keywords = {Measurement;Deep learning;Systematics;Bibliographies;Taxonomy;Psychology;Reliability;XAI;machine learning;deep learning;explainable artificial intelligence;explainable AI;explainable machine learning;metrics;evaluation},
    url = {https://ieeexplore.ieee.org/abstract/document/10297629 https://www.dfki.de/fileadmin/user_upload/import/14708_XAI_Evaluation_Metrics__Taxonomies__Concepts_and_Applications__INES_2023_-7.pdf}
}

@misc{pub14709,
    abstract = {In this paper, we propose a CNN fine-tuning method which enables users to give simultaneous feedback on two outputs: the classification itself and the visual explanation for the classification. We present the effect of this feedback strategy in a skin lesion classification task and measure how CNNs react to the two types of user feedback. To implement this approach, we propose a novel CNN architecture that integrates the Grad-CAM technique for explaining the model's decision in the training loop. Using simulated user feedback, we found that fine-tuning our model on both classification and explanation improves visual explanation while preserving classification accuracy, thus potentially increasing the trust of users in using CNN-based skin lesion classifiers.},
    year = {2023},
    title = {Fine-tuning of explainable CNNs for skin lesion classification based on dermatologists' feedback towards increasing trust},
    author = {Md Abdul Kadir and Fabrizio Nunnari and Daniel Sonntag},
    status_notes = {preprint},
    url = {https://arxiv.org/abs/2304.01399 https://www.dfki.de/fileadmin/user_upload/import/14709_2304.01399.pdf}
}

@techreport{pub14710,
    series = {DFKI Research Reports, RR},
    abstract = {Deep learning is ubiquitous, but its lack of transparency limits
its impact on several potential application areas. We demonstrate a
virtual reality tool for automating the process of assigning data inputs
to different categories. A dataset is represented as a cloud of points in
virtual space. The user explores the cloud through movement and uses
hand gestures to categorise portions of the cloud. This triggers gradual
movements in the cloud: points of the same category are attracted to
each other, different groups are pushed apart, while points are globally
distributed in a way that utilises the entire space. The space, time, and
forces observed in virtual reality can be mapped to well-defined machine
learning concepts, namely the latent space, the training epochs and the
backpropagation. Our tool illustrates how the inner workings of deep
neural networks can be made tangible and transparent. We expect this
approach to accelerate the autonomous development of deep learning
applications by end users in novel areas.},
    month = {5},
    year = {2023},
    title = {A Virtual Reality Tool for Representing, Visualizing and Updating Deep Learning Models},
    volume = {2305.15353v1},
    pages = {8},
    institution = {DFKI},
    author = {Hannes Kath and Bengt Lüers and Thiago Gouvea and Daniel Sonntag},
    keywords = {Virtual Reality · Annotation Tool · Latent Space · Representation Learning},
    url = {https://doi.org/10.48550/arXiv.2305.15353 https://www.dfki.de/fileadmin/user_upload/import/14710_2305.15353.pdf}
}

@techreport{pub14711,
    series = {DFKI Research Reports, RR},
    abstract = {The impact of machine learning (ML) in many fields of application
is constrained by lack of annotated data. Among existing tools
for ML-assisted data annotation, one little explored tool type relies on
an analogy between the coordinates of a graphical user interface and the
latent space of a neural network for interaction through direct manipulation.
In the present work, we 1) expand the paradigm by proposing
two new analogies: time and force as reflecting iterations and gradients
of network training; 2) propose a network model for learning a compact
graphical representation of the data that takes into account both its internal
structure and user provided annotations; and 3) investigate the
impact of model hyperparameters on the learned graphical representations
of the data, identifying candidate model variants for a future user
study.},
    month = {5},
    year = {2023},
    title = {A Deep Generative Model for Interactive Data Annotation through Direct Manipulation in Latent Space},
    volume = {2305.15337v1},
    pages = {7},
    institution = {DFKI},
    author = {Hannes Kath and Thiago Gouvea and Daniel Sonntag},
    keywords = {Deep Generative Model · Self-supervised Learning · Variational Autoencoder},
    url = {https://doi.org/10.48550/arXiv.2305.15337 https://www.dfki.de/fileadmin/user_upload/import/14711_2305.15337.pdf}
}

@inproceedings{pub14712,
    abstract = {This paper presents the semi-automatically created Corpus of Aligned Read Speech Including Annotations (CARInA), a speech corpus based on the German Spoken Wikipedia Corpus (GSWC). CARInA tokenizes, consolidates and organizes the vast, but rather unstructured material contained in GSWC. The contents are grouped by annotation completeness, and extended by canonic, morphosyntactic and prosodic annotations. The annotations are provided in BPF and TextGrid format. It contains 194 hours of speech material from 327 speakers, of which 124 hours are fully phonetically aligned and 30 hours are fully aligned at all annotation levels. CARInA is freely available 1 , designed to grow and improve over time, and suitable for large-scale speech analyses or machine learning tasks as illustrated by two examples shown in this paper.},
    month = {5},
    year = {2022},
    title = {Carina – A Corpus of Aligned German Read Speech Including Annotations},
    booktitle = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). International Conference on Acoustics, Speech and Signal Processing (ICASSP-2022), May 22-27, Singapore, Singapore},
    pages = {6157-6161},
    publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
    doi = {https://doi.org/10.1109/ICASSP43922.2022.9746160},
    author = {Hannes Kath and Simon Stone and Stefan Rapp and Peter Birkholz},
    keywords = {CARInA, speech data, prosodic annotation},
    url = {https://doi.org/10.1109/ICASSP43922.2022.9746160 https://www.dfki.de/fileadmin/user_upload/import/14712_Carina__A_Corpus_of_Aligned_German_Read_Speech_Including_Annotations.pdf}
}

@inproceedings{pub14629,
    abstract = {In industries loading and unloading of heavy loads manually is one of the most important task which turns out to be quite difficult, time-consuming and risky for humans. This paper illustrates the mechanical design of the industry based automated robot which include: Ackerman Steering Mechanism and Differential Mechanism. Ackerman Steering allows front two wheels to turn left and right in the track without going out of the track. Differential has been mounted with two back wheels and a DC motor has been used with its controller to start motion of the robot. The autonomous robot is designed to start its movement from a starting position where goods are loaded on it, then follow a path of white line drawn on black surface and unload goods by itself after reaching a destination place. Digital Line Following sensor has been mounted in front of the robot so that the sensor can detect path by emitting and receiving signals allowing it to move in the pre-defined track having left and right turns while carrying goods from starting position to the destination. The main objective is to load and unload heavy goods that has been achieved by two large linear actuators for producing required torque and force necessary to unload heavy loads up to 150kg sideways to the ground safely. Besides, the robot has been built up having the ability to avoid collision with any obstacles that come in its way. Building an industrial robot with moderate speed, good efficiency for loading and unloading purpose within a short time to ease human suffering has been the main focus of this paper.},
    year = {2015},
    title = {An autonomous industrial robot for loading and unloading goods},
    booktitle = {2015 International Conference on Informatics, Electronics & Vision (ICIEV). International Conference on Informatics, Electronics & Vision (ICIEV-2015), 4th International Conference, June 15-18, Fukuoka, Japan},
    pages = {1-6},
    publisher = {IEEE},
    author = {Md Abdul Kadir and Md Belayet Chowdhury and Jaber AL Rashid and Shifur Rahman Shakil and Md Khalilur Rhaman},
    organization = {IEEE},
    url = {https://www.dfki.de/fileadmin/user_upload/import/14629_An_Autonomous_Industrial_Robot_for_Loading_and.pdf https://dl.acm.org/doi/10.1109/ICIEV.2015.7333984#:~:text=We%20consider%20two%20mechanisms%20to%20procure}
}

