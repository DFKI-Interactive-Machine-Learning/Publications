@inproceedings{pub13402,
author = {Liang, Siting and Hartmann, Mareike and Sonntag, Daniel},
title = {Cross-domain German Medical Named Entity Recognition using a Pre-Trained Language Model and Unified Medical Semantic Types},
booktitle = {Association for Computational Linguistics. Clinical Natural Language Processing Workshop (ClinicalNLP-2023), July 9-14, Toronto, Canada},
year = {2023},
publisher = {ACL}
}

@inproceedings{pub13395,
author = {Kath, Hannes Berthold and Gouvea, Thiago and Sonntag, Daniel},
title = {A Human-in-the-Loop Tool for Annotating Passive Acoustic Monitoring Datasets},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. International Joint Conference on Artificial Intelligence (IJCAI-2023), located at IJCAI, August 19-25, Macao, Macao. International Joint Conference on Artificial Intelligence (IJCAI), befindet sich IJCAI, August 19-25, Macao, China},
year = {2023},
publisher = {International Joint Conferences on Artificial Intelligence}
}

@inproceedings{pub13356,
author = {Gouvea, Thiago and Kath, Hannes Berthold and Troshani, Ilira and Lüers, Bengt and Serafini, Patrícia P. and Campos, Ivan B. and Afonso, André S. and Leandro, Sérgio M. F. M. and Swanepoel, Lourens and Theron, Nicholas and Swemmer, Anthony M. and Sonntag, Daniel},
title = {Interactive Machine Learning Solutions for Acoustic Monitoring of Animal Wildlife in Biosphere Reserves},
booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. International Joint Conference on Artificial Intelligence (IJCAI-2023), befindet sich IJCAI, August 19-25, Macao, Macao SAR of China},
year = {2023},
publisher = {International Joint Conferences on Artificial Intelligence}
}

@inproceedings{pub13420,
author = {Anagnostopoulou, Aliki and Hartmann, Mareike and Sonntag, Daniel},
title = {Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory},
booktitle = {Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP). ACL Workshop on Simple and Efficient Natural Language Processing (SustaiNLP-2023), befindet sich Annual Meeting of the Association for Computational Linguistics 2023, July 13-13, Toronto, Canada},
year = {2023},
month = {7},
publisher = {Association for Computational Linguistics}
}

@inproceedings{13200,
title = {A User Interface for Explaining Machine Learning Model Explanations},
author = {Md Abdul Kadir and Abdulrahman Mohamed Selim and Michael Barz and Daniel Sonntag},
url = {https://doi.org/10.1145/3581754.3584131},
doi = {10.1145/3581754.3584131},
isbn = {9798400701078},
year = {2023},
date = {2023-01-01},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {59–63},
publisher = {Association for Computing Machinery},
address = {Sydney, NSW, Australia},
series = {IUI '23 Companion},
abstract = {Explainable Artificial Intelligence (XAI) is an emerging subdiscipline of Machine Learning (ML) and human-computer interaction. Discriminative models need to be understood. An explanation of such ML models is vital when an AI system makes decisions that have significant consequences, such as in healthcare or finance. By providing an input-specific explanation, users can gain confidence in an AI system’s decisions and be more willing to trust and rely on it. One problem is that interpreting example-based explanations for discriminative models, such as saliency maps, can be difficult because it is not always clear how the highlighted features contribute to the model’s overall prediction or decisions. Moreover, saliency maps, which are state-of-the-art visual explanation methods, do not provide concrete information on the influence of particular features. We propose an interactive visualisation tool called EMILE-UI that allows users to evaluate the provided explanations of an image-based classification task, specifically those provided by saliency maps. This tool allows users to evaluate the accuracy of a saliency map by reflecting the true attention or focus of the corresponding model. It visualises the relationship between the ML model and its explanation of input images, making it easier to interpret saliency maps and understand how the ML model actually predicts. Our tool supports a wide range of deep learning image classification models and image data as inputs.},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings}
}

@inproceedings{13201,
title = {IMETA: An Interactive Mobile Eye Tracking Annotation Method for Semi-Automatic Fixation-to-AOI Mapping},
author = {László Kopácsi and Michael Barz and Omair Shahzad Bhatti and Daniel Sonntag},
url = {https://www.dfki.de/fileadmin/user_upload/import/13201_3581754.3584125.pdf},
doi = {https://doi.org/10.1145/3581754.3584125},
year = {2023},
date = {2023-01-01},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {33-36},
publisher = {Association for Computing Machinery},
abstract = {Mobile eye tracking studies involve analyzing areas of interest (AOIs) and visual attention to these AOIs to understand how people process visual information. However, accurately annotating the data collected for user studies can be a challenging and time-consuming task. Current approaches for automatically or semi-automatically analyzing head-mounted eye tracking data in mobile eye tracking studies have limitations, including a lack of annotation flexibility or the inability to adapt to specific target domains. To address this problem, we present IMETA, an architecture for semi-automatic fixation-to-AOI mapping. When an annotator assigns an AOI label to a sequence of frames based on the respective fixation points, an interactive video object segmentation method is used to estimate the mask proposal of the AOI. Then, we use the 3D reconstruction of the visual scene created from the eye tracking video to map these AOI masks to 3D. The resulting 3D segmentation of the AOI can be used to suggest labels for the rest of the video, with the suggestions becoming increasingly accurate as more samples are provided by an annotator using interactive machine learning (IML). IMETA has the potential to reduce the annotation workload and speed up the evaluation of mobile eye tracking studies.},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings}
}

@inproceedings{13196,
title = {Interactive Fixation-to-AOI Mapping for Mobile Eye Tracking Data Based on Few-Shot Image Classification},
author = {Michael Barz and Omair Shahzad Bhatti and Hasan Md Tusfiqur Alam and Ho Minh Duy Nguyen and Daniel Sonntag},
url = {https://www.dfki.de/fileadmin/user_upload/import/13196_3581754.3584179.pdf},
doi = {https://doi.org/10.1145/3581754.3584179},
year = {2023},
date = {2023-01-01},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {175-178},
publisher = {Association for Computing Machinery},
abstract = {Mobile eye tracking is an important tool in psychology and human-centred interaction design for understanding how people process visual scenes and user interfaces. However, analysing recordings from mobile eye trackers, which typically include an egocentric video of the scene and a gaze signal, is a time-consuming and largely manual process. To address this challenge, we propose a web-based annotation tool that leverages few-shot image classification and interactive machine learning (IML) to accelerate the annotation process. The tool allows users to efficiently map fixations to areas of interest (AOI) in a video-editing-style interface. It includes an IML component that generates suggestions and learns from user feedback using a few-shot image classification model initialised with a small number of images per AOI. Our goal is to improve the efficiency and accuracy of fixation-to-AOI mapping in mobile eye tracking.},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings}
}


@article{13195,
title = {Digital ink and differentiated subjective ratings for cognitive load measurement in middle childhood},
author = {Kristin Altmeyer and Michael Barz and Luisa Lauer and Markus Peschel and Daniel Sonntag and Roland Brünken and Sarah Malone},
url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bjep.12595},
year = {2023},
date = {2023-01-01},
journal = {British Journal of Educational Psychology},
volume = {n/a},
pages = {18},
publisher = {John Wiley & Sons, Ltd},
abstract = {Abstract Background New methods are constantly being developed to adapt cognitive load measurement to different contexts. However, research on middle childhood students' cognitive load measurement is rare. Research indicates that the three cognitive load dimensions (intrinsic, extraneous, and germane) can be measured well in adults and teenagers using differentiated subjective rating instruments. Moreover, digital ink recorded by smartpens could serve as an indicator for cognitive load in adults. Aims With the present research, we aimed at investigating the relation between subjective cognitive load ratings, velocity and pressure measures recorded with a smartpen, and performance in standardized sketching tasks in middle childhood students. Sample Thirty-six children (age 7–12) participated at the university's laboratory. Methods The children performed two standardized sketching tasks, each in two versions. The induced intrinsic cognitive load or the extraneous cognitive load was varied between the versions. Digital ink was recorded while the children drew with a smartpen on real paper and after each task, they were asked to report their perceived intrinsic and extraneous cognitive load using a newly developed 5-item scale. Results Results indicated that cognitive load ratings as well as velocity and pressure measures were substantially related to the induced cognitive load and to performance in both sketching tasks. However, cognitive load ratings and smartpen measures were not substantially related. Conclusions Both subjective rating and digital ink hold potential for cognitive load and performance measurement. However, it is questionable whether they measure the exact same constructs.},
keywords = {},
pubstate = {published},
tppubtype = {article}
}

@inproceedings{pub12923,
    author = {Nguyen, Ho Minh Duy and Nguyen, Hoang and Truong, Mai T. N. and Cao, Tri and Nguyen, Binh T. and Ho, Nhat and Swoboda, Paul and Albarqouni, Shadiand Xie, Pengtao and Sonntag, Daniel},
    title = {Joint Self-Supervised Image-Volume Representation Learning with Intra-Inter Contrastive Clustering},
    booktitle = {Thirty-Seventh AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence (AAAI-2023)},
    year = {2023},
    month = {2},
    publisher = {AAAI Press}
}

