@inproceedings{13200,
title = {A User Interface for Explaining Machine Learning Model Explanations},
author = {Md Abdul Kadir and Abdulrahman Mohamed Selim and Michael Barz and Daniel Sonntag},
url = {https://doi.org/10.1145/3581754.3584131},
doi = {10.1145/3581754.3584131},
isbn = {9798400701078},
year = {2023},
date = {2023-01-01},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {59–63},
publisher = {Association for Computing Machinery},
address = {Sydney, NSW, Australia},
series = {IUI '23 Companion},
abstract = {Explainable Artificial Intelligence (XAI) is an emerging subdiscipline of Machine Learning (ML) and human-computer interaction. Discriminative models need to be understood. An explanation of such ML models is vital when an AI system makes decisions that have significant consequences, such as in healthcare or finance. By providing an input-specific explanation, users can gain confidence in an AI system’s decisions and be more willing to trust and rely on it. One problem is that interpreting example-based explanations for discriminative models, such as saliency maps, can be difficult because it is not always clear how the highlighted features contribute to the model’s overall prediction or decisions. Moreover, saliency maps, which are state-of-the-art visual explanation methods, do not provide concrete information on the influence of particular features. We propose an interactive visualisation tool called EMILE-UI that allows users to evaluate the provided explanations of an image-based classification task, specifically those provided by saliency maps. This tool allows users to evaluate the accuracy of a saliency map by reflecting the true attention or focus of the corresponding model. It visualises the relationship between the ML model and its explanation of input images, making it easier to interpret saliency maps and understand how the ML model actually predicts. Our tool supports a wide range of deep learning image classification models and image data as inputs.},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings}
}

@inproceedings{13201,
title = {IMETA: An Interactive Mobile Eye Tracking Annotation Method for Semi-Automatic Fixation-to-AOI Mapping},
author = {László Kopácsi and Michael Barz and Omair Shahzad Bhatti and Daniel Sonntag},
url = {https://www.dfki.de/fileadmin/user_upload/import/13201_3581754.3584125.pdf},
doi = {https://doi.org/10.1145/3581754.3584125},
year = {2023},
date = {2023-01-01},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {33-36},
publisher = {Association for Computing Machinery},
abstract = {Mobile eye tracking studies involve analyzing areas of interest (AOIs) and visual attention to these AOIs to understand how people process visual information. However, accurately annotating the data collected for user studies can be a challenging and time-consuming task. Current approaches for automatically or semi-automatically analyzing head-mounted eye tracking data in mobile eye tracking studies have limitations, including a lack of annotation flexibility or the inability to adapt to specific target domains. To address this problem, we present IMETA, an architecture for semi-automatic fixation-to-AOI mapping. When an annotator assigns an AOI label to a sequence of frames based on the respective fixation points, an interactive video object segmentation method is used to estimate the mask proposal of the AOI. Then, we use the 3D reconstruction of the visual scene created from the eye tracking video to map these AOI masks to 3D. The resulting 3D segmentation of the AOI can be used to suggest labels for the rest of the video, with the suggestions becoming increasingly accurate as more samples are provided by an annotator using interactive machine learning (IML). IMETA has the potential to reduce the annotation workload and speed up the evaluation of mobile eye tracking studies.},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings}
}

@inproceedings{13196,
title = {Interactive Fixation-to-AOI Mapping for Mobile Eye Tracking Data Based on Few-Shot Image Classification},
author = {Michael Barz and Omair Shahzad Bhatti and Hasan Md Tusfiqur Alam and Ho Minh Duy Nguyen and Daniel Sonntag},
url = {https://www.dfki.de/fileadmin/user_upload/import/13196_3581754.3584179.pdf},
doi = {https://doi.org/10.1145/3581754.3584179},
year = {2023},
date = {2023-01-01},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {175-178},
publisher = {Association for Computing Machinery},
abstract = {Mobile eye tracking is an important tool in psychology and human-centred interaction design for understanding how people process visual scenes and user interfaces. However, analysing recordings from mobile eye trackers, which typically include an egocentric video of the scene and a gaze signal, is a time-consuming and largely manual process. To address this challenge, we propose a web-based annotation tool that leverages few-shot image classification and interactive machine learning (IML) to accelerate the annotation process. The tool allows users to efficiently map fixations to areas of interest (AOI) in a video-editing-style interface. It includes an IML component that generates suggestions and learns from user feedback using a few-shot image classification model initialised with a small number of images per AOI. Our goal is to improve the efficiency and accuracy of fixation-to-AOI mapping in mobile eye tracking.},
keywords = {},
pubstate = {published},
tppubtype = {inproceedings}
}


@article{13195,
title = {Digital ink and differentiated subjective ratings for cognitive load measurement in middle childhood},
author = {Kristin Altmeyer and Michael Barz and Luisa Lauer and Markus Peschel and Daniel Sonntag and Roland Brünken and Sarah Malone},
url = {https://www.dfki.de/fileadmin/user_upload/import/13195_Brit_J_of_Edu_Psychol_-_2023_-_Altmeyer_-_Digital_ink_and_differentiated_subjective_ratings_for_cognitive_load_measurement.pdf
https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bjep.12595},
year = {2023},
date = {2023-01-01},
journal = {British Journal of Educational Psychology},
volume = {n/a},
pages = {18},
publisher = {John Wiley & Sons, Ltd},
abstract = {Abstract Background New methods are constantly being developed to adapt cognitive load measurement to different contexts. However, research on middle childhood students' cognitive load measurement is rare. Research indicates that the three cognitive load dimensions (intrinsic, extraneous, and germane) can be measured well in adults and teenagers using differentiated subjective rating instruments. Moreover, digital ink recorded by smartpens could serve as an indicator for cognitive load in adults. Aims With the present research, we aimed at investigating the relation between subjective cognitive load ratings, velocity and pressure measures recorded with a smartpen, and performance in standardized sketching tasks in middle childhood students. Sample Thirty-six children (age 7–12) participated at the university's laboratory. Methods The children performed two standardized sketching tasks, each in two versions. The induced intrinsic cognitive load or the extraneous cognitive load was varied between the versions. Digital ink was recorded while the children drew with a smartpen on real paper and after each task, they were asked to report their perceived intrinsic and extraneous cognitive load using a newly developed 5-item scale. Results Results indicated that cognitive load ratings as well as velocity and pressure measures were substantially related to the induced cognitive load and to performance in both sketching tasks. However, cognitive load ratings and smartpen measures were not substantially related. Conclusions Both subjective rating and digital ink hold potential for cognitive load and performance measurement. However, it is questionable whether they measure the exact same constructs.},
keywords = {},
pubstate = {published},
tppubtype = {article}
}

@inproceedings{pub12923,
    author = {Nguyen, Ho Minh Duy and Nguyen, Hoang and Truong, Mai T. N. and Cao, Tri and Nguyen, Binh T. and Ho, Nhat and Swoboda, Paul and Albarqouni, Shadiand Xie, Pengtao and Sonntag, Daniel},
    title = {Joint Self-Supervised Image-Volume Representation Learning with Intra-Inter Contrastive Clustering},
    booktitle = {Thirty-Seventh AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence (AAAI-2023)},
    year = {2023},
    month = {2},
    publisher = {AAAI Press}
}

@inproceedings{pub12633,
    author = {Bhatti, Omair Shahzad and Barz, Michael and Sonntag, Daniel},
    editor = {Bergmann, Ralph and Malburg, Lukas and Rodermund, Stephanie C. and Timm, Ingo J.},
    title = {Leveraging Implicit Gaze-Based User Feedback for Interactive Machine Learning},
    booktitle = {KI 2022: Advances in Artificial Intelligence. German Conference on Artificial Intelligence (KI), Cham},
    year = {2022},
    pages = {9--16},
    publisher = {Springer International Publishing},
    isbn = {978-3-031-15791-2}
}

@inproceedings{pub12622,
    author = {Céard-Falkenberg, Felix and Kuznetsov, Konstantin and Prange, Alexander and Barz, Michael and Sonntag, Daniel},
    title = {pEncode: A Tool for Visualizing Pen Signal Encodings in Real-time},
    booktitle = {Proceedings of the First International Conference on Hybrid Human-Machine Intelligence. International Conference on Hybrid Human-Artificial Intelligence (HHAI-2022), 1st International Conference on Hybrid Human-Artificial Intelligence, June 13-17, Amsterdam, Netherlands},
    series = {Frontiers in Artificial Intelligence and Applications},
    year = {2022},
    volume = {354},
    pages = {281--284},
    address = {De Boelelaan 1105, 1081 HV Amsterdam, Netherlands},
    organization = {Vrije Universiteit Amsterdam},
    publisher = {IOS Press},
    isbn = {978-1-64368-308-9}
}

@inproceedings{pub12519,
    author = {Hartmann, Mareike and Sonntag, Daniel},
    title = {A survey on improving NLP models with human explanations},
    booktitle = {Proceedings of the First Workshop on Learning with Natural Language Supervision. Workshop on Learning with Natural Language Supervision, befindet sich ACL 2022, May 26-26, Dublin, Ireland},
    year = {2022},
    publisher = {Association for Computational Linguistics}
}

@inproceedings{pub12621,
    author = {Kuznetsov, Konstantin and Barz, Michael and Sonntag, Daniel},
    title = {SpellInk: Interactive correction of spelling mistakes in handwritten text},
    booktitle = {Proceedings of the First International Conference on Hybrid Human-Machine Intelligence. International Conference on Hybrid Human-Artificial Intelligence (HHAI-2022), the 1st International Conference on Hybrid Human-Artificial Intelligence, June 13-17, Amsterdam, Netherlands},
    series = {Frontiers in Artificial Intelligence and Applications},
    year = {2022},
    volume = {354},
    pages = {278--280},
    address = {De Boelelaan 1105, 1081 HV Amsterdam, Netherlands},
    organization = {Vrije Universiteit Amsterdam},
    publisher = {IOS Press},
    isbn = {978-1-64368-308-9}
}

@misc{pub12900,
    author = {Szeier, Szilvia and Baffy, Benjámin and Baranyi, Gábor and Skaf, Joul and Kopácsi, László and Sonntag, Daniel and Sörös, Gábor and Lőrincz, András},
    title = {3D Semantic Label Transfer and Matching in Human-Robot Collaboration},
    year = {2022},
    month = {10},
    publisher = {Learning to Generate 3D Shapes and Scenes, ECCV 2022 Workshop}
}

@inproceedings{pub12839,
    author = {Liang, Siting and Hartmann, Mareike and Sonntag, Daniel},
    title = {Cross-lingual German Biomedical Information Extraction: from Zero-shot to Human-in-the-Loop},
    booktitle = {2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics. NAACL Workshop on Bridging Human-Computer Interaction and Natural Language Processing (HCI+NLP-2022), befindet sich NAACL 2022, October 15-July 15, Seattle, Washington, United States},
    year = {2022},
    month = {7},
    organization = {NAACL},
    publisher = {Association for Computational Linguistics}
}

@misc{pub12516,
    author = {Anagnostopoulou, Aliki and Hartmann, Mareike and Sonntag, Daniel},
    title = {Putting Humans in the Image Captioning Loop},
    year = {2022},
    month = {7}
}

@inproceedings{pub12428,
    author = {Gouvea, Thiago and Troshani, Ilira and Herrlich, Marc and Sonntag, Daniel},
    title = {Annotating sound events through interactive design of interpretable features},
    booktitle = {Proceedings of the First International Conference on Hybrid Human-Machine Intelligence. International Conference on Hybrid Human Artificial Intelligence (MMAI-2022), June 13-17, Amsterdam, Netherlands},
    year = {2022},
    publisher = {IOS Press}
}

@inproceedings{pub12429,
    author = {Gouvea, Thiago and Troshani, Ilira and Herrlich, Marc and Sonntag, Daniel},
    title = {Interactive design of interpretable features for marine soundscape data annotation},
    booktitle = {Workshop on Human-centered Design of Symbiotic Hybrid Intelligence. Workshop on Human-centered Design of Symbiotic Hybrid Intelligence (HCSHI-2022), befindet sich HHAI, June 14-14, Amsterdam, Netherlands},
    year = {2022},
    publisher = {HHAI}
}

@inproceedings{pub12287,
    author = {Valdunciel, Pablo and Bhatti, Omair Shahzad and Barz, Michael and Sonntag, Daniel},
    title = {Interactive Assessment Tool for Gaze-based Machine Learning Models in Information Retrieval},
    booktitle = {ACM SIGIR Conference on Human Information Interaction and Retrieval. ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR-2022), March 14-18, Regensburg, Germany},
    year = {2022},
    month = {3},
    publisher = {Association for Computing Machinery},
    isbn = {9781450391863}
}
@article{pub12167,
    author = {Hartmann, Mareike and Anagnostopoulou, Aliki and Sonntag, Daniel},
    title = {Interactive Machine Learning for Image Captioning},
    year = {2022},
    month = {2}
}

@article{pub12165,
    author = {Barz, Michael and Bhatti, Omair Shahzad and Sonntag, Daniel},
    title = {Implicit Estimation of Paragraph Relevance from Eye Movements},
    year = {2022},
    month = {1},
    volume = {3},
    pages = {13},
    journal = {Frontiers in Computer Science},
    publisher = {Frontiers Media S.A.}
}

@inproceedings{pub12121,
    author = {Lauer, Luisa and Javaheri, Hamraz and Altmeyer, Kristin and Malone, 
    Sarah and Grünerbl, Agnes and Barz, Michael and Peschel, Markus and  Brünken, Rolandand Lukowicz, Paul},
    title = {Encountering Students' Learning Difficulties in Electrics - Didactical Concept and Prototype of Augmented Reality-Toolkit},
    booktitle = {Fostering scientific citizenship in an uncertain world - ESERA 2021 e-Proceedings. European Science Education Research Association Conference (ESERA-2021), Workshop: Digital Resources for Science Teaching and Learning, August 30-September 3},
    year = {2022},
    organization = {University of Minho},
    publisher = {University of Minho}
}

@techreport{pub12211,
    author = {Nguyen, Ho Minh Duy and Henschel, Roberto and Rosenhahn, Bodo and Sonntag, Daniel and Swoboda, Paul},
    title = {LMGP: Lifted Multicut Meets Geometry Projections for Multi-Camera Multi-Object Tracking},
    series = {DFKI Documents},
    year = {2022},
    volume = {01},
    institution = {DFKI, MPI-INF}
}

@article{pub12216,
    author = {Nguyen, Ho Minh Duy and Nguyen, Thu T. and Vu, Huong and Pham, Quang and Nguyen, Manh-Duy and Nguyen, Binh T. and Sonntag, Daniel},
    title = {TATL: Task Agnostic Transfer Learning for Skin Attributes Detection},
    year = {2022},
    volume = {01},
    pages = {1--27},
    journal = {Medical Image Analysis},
    publisher = {Elsevier}
}
